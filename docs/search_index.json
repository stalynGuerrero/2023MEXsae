[["index.html", "Fortalecimiento de capacidades en Técnicas de Estimación en Áreas Pequeñas con enfoque Bayesiano para la desagregación de datos Agenda", " Fortalecimiento de capacidades en Técnicas de Estimación en Áreas Pequeñas con enfoque Bayesiano para la desagregación de datos Andrés Gutiérrez1, Stalyn Guerrero2 2023-11-16 Agenda Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["material-del-curso.html", "Material del curso", " Material del curso En el siguiente enlace encontrará material bibliográfico complementario (Libros, presentaciones, casos de estudio y manuales de instalación) Descargar En el siguiente enlace encontrará las rutinas de R desarrolladas para el taller. Descargar "],["día-1---sesión-1--no-dejar-a-nadie-atrás---ods-y-la-agenda-2030.html", "Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030", " Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030 Ver presentación "],["día-1---sesión-2--censo-e-información-satelital.html", "Capítulo 2 Día 1 - Sesión 2- Censo e información satelital ", " Capítulo 2 Día 1 - Sesión 2- Censo e información satelital "],["uso-de-imágenes-satelitales-y-sae.html", "2.1 Uso de imágenes satelitales y SAE", " 2.1 Uso de imágenes satelitales y SAE Uno de los artículo pioneros de estimación de áreas pequeñas fue el artículo de Singh, R, et. al. (2002) el cual abordó la estimación del rendimiento de cultivos para los tehsil (unidad subadministrativa) del distriyo Rohtak district en Haryana (India). Las imágenes raster representan el mundo mediante un conjunto de celdas contiguas igualmente espaciadas conocidas como pixeles, estas imágenes tienen información como un sistema de información geográfico, Un sistema de referencia de coordenadas. Las imágenes almacenan un identificador, un valor en cada pixel (o un vector con diferentes valores) y cada celda tiene asociada una escala de colores. Las imágenes pueden obtenerse crudas y procesadas, estas primeras contienen solamente las capas de colores, las segundas contienen también valores que han sido procesados en cada celda (índices de vegetación, intensidad lumínica, tipo de vegetación). La información cruda puede utilizarse para entrenar características que se desean entrenar (carreteras, tipo de cultivo, bosque / no bosque), afortunadamente en Google Earth Engine encontramos muchos indicadores procesadas asociadas a un pixel. Estos indicadores pueden agregarse a nivel de un área geográfica. 2.1.1 Fuentes de datos de imágenes satelitales Algunas de las principales fuentes de imágenes satelitales son: http://earthexplorer.usgs.gov/ https://lpdaacsvc.cr.usgs.gov/appeears/ https://search.earthdata.nasa.gov/search https://scihub.coGTMnicus.eu/ https://aws.amazon.com/public-data-sets/landsat/ Sin embargo la mayor parte de estas fuentes están centralizadas en Google Earth Engine que permite buscar fuentes de datos provenientes de imágenes satelitales. GEE se puede manejar por medio de APIS en diferentes lenguajes de programación: Javascript (por defecto), Python y R (paquete rgee). "],["google-earth-eninge.html", "2.2 Google Earth Eninge", " 2.2 Google Earth Eninge Crear una cuenta en link, una vez que se ingrese a la cuenta puede buscarse los conjuntos de datos de interés: Una vez se busque el conjunto de datos se puede abrir un editor de código brindado por google en Javascript. Copiar y pegar la sintaxis que brinda el buscador de conjunto de datos para visualizar la imagen raster y disponer de sentencias que GTMmitan la obtención del conjunto de datos de interés posteriormente en R "],["instalación-de-rgee.html", "2.3 Instalación de rgee", " 2.3 Instalación de rgee Descargar e instalar anaconda o conda. (https://www.anaconda.com/products/individual) Abrir Anaconda prompt y configurar ambiente de trabajo (ambiente python MEX2023) con las siguientes sentencias: conda env list conda create -n MEX2023 python=3.9 activate MEX2023 pip install google-api-python-client pip install earthengine-api pip install numpy Listar los ambientes de Python disponibles en anaconda prompt conda env list Una vez identificado la ruta del ambiente ambiente MEX2023 definirla en R (no se debe olvidar cambiar \\ por /). Instalar reticulate y rgee, cargar paquetes para procesamiento espacial y configurar el ambiente de trabajo como sigue: library(reticulate) # Conexión con Python library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(dplyr) # Paquete para procesamiento de datos library(magrittr) rgee_environment_dir = &quot;C:/Users/gnieto/Anaconda3/envs/MEX2023/python.exe&quot; # Configurar python (Algunas veces no es detectado y se debe reiniciar R) reticulate::use_python(rgee_environment_dir, required=T) rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;MEX2023&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) Una vez configurado el ambiente puede iniciarlizarse una sesión de Google Earth Engine como sigue: rgee::ee_Initialize(drive = T) Notas: Se debe inicializar cada sesión con el comando rgee::ee_Initialize(drive = T). Los comandos de javascript que invoquen métodos con “.” se sustituyen por signo peso ($), por ejemplo: ee.ImageCollection().filterDate() # Javascript ee$ImageCollection()$filterDate() # R 2.3.1 Descargar información satelital Paso 1: disponer de los shapefile shape &lt;- read_sf(&quot;Recursos/Día1/Sesion2/Shape/MEX_dam.shp&quot;) #shape &lt;- read_sf(&quot;Recursos/Día1/Sesion2/Shape/MEX_dam2.shp&quot;) plot(shape[&quot;geometry&quot;]) Paso 2: Seleccionar el archivo de imágenes que desea procesar, para nuestro ejemplo luces nocturnas. luces &lt;- ee$ImageCollection(&quot;NOAA/DMSP-OLS/NIGHTTIME_LIGHTS&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2013-01-01&quot;, &quot;2014-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;stable_lights&quot;)) %&gt;% ee$ImageCollection$toBands() Paso 3: Descargar la información ## Tiempo 10 minutos shape_luces &lt;- map(unique(shape$dam), ~tryCatch(ee_extract( x = luces, y = shape[&quot;dam&quot;] %&gt;% filter(dam == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(dam = .x), error = function(e)data.frame(dam = .x))) shape_luces %&lt;&gt;% bind_rows() tba(shape_luces, cap = &quot;Promedio de luces nocturnasa&quot;) Tabla 2.1: Promedio de luces nocturnas estandarizada dam luces_nocturnas 02 98.1895 03 88.4394 18 88.7521 14 117.8567 01 89.4560 11 116.0198 22 94.6416 13 101.5930 16 103.8342 15 126.0225 09 89.1780 06 87.1353 17 93.3744 31 95.1464 04 90.4240 21 110.1384 23 90.7170 29 90.3565 12 98.6019 20 100.0054 27 101.5690 07 107.1431 26 103.5783 08 104.0508 05 100.0359 25 100.8792 10 93.7401 32 96.9760 24 95.8973 19 103.5633 28 99.4590 30 123.2261 Repetir la rutina para: Tipo de suelo: crops-coverfraction (Porcentaje de cubrimiento cultivos) y urban-coverfraction (Porcentaje de cobertura urbana) disponibles en https://develoGTMs.google.com/earth-engine/datasets/catalog/COGTMNICUS_Landcover_100m_Proba-V-C3_Global#description Tiempo de viaje al hospital o clínica más cercana (accessibility) y tiempo de viaje al hospital o clínica más cercana utilizando transporte no motorizado (accessibility_walking_only) información disponible en https://develoGTMs.google.com/earth-engine/datasets/catalog/Oxford_MAP_accessibility_to_healthcare_2019 Modificación humana, donde se consideran los asentamiento humano, la agricultura, el transporte, la minería y producción de energía e infraestructura eléctrica. En el siguiente link encuentra la información satelital https://develoGTMs.google.com/earth-engine/datasets/catalog/CSP_HM_GlobalHumanModification#description Paso 4 consolidar la información. Tabla 2.2: Predictores satelitales estandarizados dam luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado 02 98.1895 93.6670 102.6645 90.7414 112.6403 113.6432 03 88.4394 91.1942 87.6937 90.3823 106.3704 108.0033 18 88.7521 94.3308 89.0983 93.4305 95.3447 95.1180 14 117.8567 109.1871 115.4318 111.4279 96.6370 96.8989 01 89.4560 91.4337 88.7284 87.5749 92.5297 92.8327 11 116.0198 101.9790 110.8788 98.1615 93.3039 93.7549 22 94.6416 91.8641 94.3658 89.7262 92.8276 93.1459 13 101.5930 94.5173 99.3851 94.5405 92.9706 93.3128 16 103.8342 99.8159 106.5804 104.7533 95.5144 96.1129 15 126.0225 96.7277 128.4589 96.6770 92.7404 93.0623 Los resultados se muestran en los siguientes mapas 2.3.2 Luces nocturnas 2.3.3 Cubrimiento cultivos 2.3.4 Cubrimiento urbanos 2.3.5 Modificación humana 2.3.6 Tiempo promedio al hospital 2.3.7 Tiempo promedio al hospital en vehiculo no motorizado "],["censos-de-población-y-vivienda.html", "2.4 Censos de población y vivienda", " 2.4 Censos de población y vivienda Es necesario definir las variables del país con los que se desea trabajar. De acuerdo a esto, como primer paso se debe tener acceso al censo del país, para ello puede acceder desde el siguiente enlace https://redatam.org/en/microdata en el cual dispondrá de un archivo .zip con los microdatos del país. Ahora bien, para leer el conjunto de datos, es necesario emplear la función redatam.open de la librería redatam, la cual depende directamente del diccionario censal del software REDATAM, este es un archivo con extensión dicx y que debe encontrarse en la carpeta sobre los datos que se están leyendo. Así, es como se crea un objeto dentro de R que hace la respectiva unión del diccionario con los microdatos de la base de datos censal. La siguiente sintaxis muestra la lectura del diccionario en R y los cálculos iniciales library(redatam) mexico &lt;- redatam.open(&quot;Recursos/Día1/Sesion2/Data/cpv2020mex-cde.dicX&quot;) CONTEOS &lt;- redatam.query( mexico, &quot;freq ENT.REDCODEN by VIVIENDA.area by PERSONA.sexo by PERSONA.edad by PERSONA.pbloper by PERSONA.ESCOACUM by PERSONA.disres&quot;, tot.omit = FALSE ) # Eliminando totales de la tabla CONTEOS2 &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)), all_vars(. != &quot;__tot__&quot;)) Después de realizar algunas validaciones se estandarizan las variables como muestra el siguiente código. censo_mrp &lt;- CONTEOS2 %&gt;% transmute( dam = str_pad( string = REDCODEN1_value, width = 2, pad = &quot;0&quot; ), area = case_when(area2_value == 1 ~ &quot;1&quot;, # 1 = Urbana TRUE ~ &quot;0&quot;), # 0 = Rural sexo = as.character(sexo3_value), edad = case_when( edad4_value %in% 0:14 ~ &quot;1&quot;, # 5 a 14 edad4_value %in% 15:29 ~ &quot;2&quot;, # 15 a 29 edad4_value %in% 30:44 ~ &quot;3&quot;, # 30 a 44 edad4_value %in% 45:64 ~ &quot;4&quot;, # 45 a 64 TRUE ~ &quot;5&quot; ), # 65 o mas anoest = case_when( edad4_value &lt; 2| is.na(ESCOACUM6_value) ~ &quot;98&quot;, # No aplica ESCOACUM6_value == 99 ~ &quot;99&quot;, #NS/NR ESCOACUM6_value %in% 0 ~ &quot;1&quot;, # Sin educacion ESCOACUM6_value %in% c(1:6) ~ &quot;2&quot;, # 1-6 ESCOACUM6_value %in% c(7:12) ~ &quot;3&quot;, # 7-12 ESCOACUM6_value &gt; 12 ~ &quot;4&quot; , # 12 o mas TRUE ~ &quot;Error&quot; ), etnia = case_when( pbloper5_value == 1 ~ &quot;1&quot;, # Indigena pbloper5_value %in% c(5, 2) ~ &quot;2&quot;, # Afro, TRUE ~ &quot;3&quot; # Otro ), discapacidad = case_when( disres7_value == 8 ~ &quot;0&quot;, # No discapacitado TRUE ~ &quot;1&quot; # Discapacitado ), value ) %&gt;% group_by(depto, area, etnia, sexo, edad, anoest, discapacidad) %&gt;% summarise(n = sum(value), .groups = &quot;drop&quot;) A partir de la base estandarizada es posible construir algunas covariables para el departamento. censo_mrp &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/censo_mrp_dam.rds&quot;) tasa_censo &lt;- model.matrix(dam ~ -1 +., data = censo_mrp %&gt;% select(-n)) %&gt;% data.frame() %&gt;% mutate(dam = censo_mrp$dam, n = censo_mrp$n) %&gt;% group_by(dam) %&gt;% summarise_all(~weighted.mean(x = .,w = n)) %&gt;% mutate(etnia1 = 1-etnia3-etnia2) %&gt;% select(-area0, -anoest98,-anoest99,-etnia3,-n) tba(tasa_censo) dam area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 01 0.8448 0.0125 0.5142 0.2645 0.2106 0.1854 0.0655 0.2341 0.4255 0.1886 0.0576 0.0547 02 0.9534 0.0229 0.4955 0.2616 0.2301 0.2047 0.0634 0.2357 0.4697 0.1531 0.0529 0.0696 03 0.9041 0.0287 0.4960 0.2486 0.2440 0.1943 0.0606 0.2363 0.4564 0.1661 0.0483 0.1025 04 0.7731 0.0323 0.5084 0.2421 0.2224 0.2017 0.0724 0.2451 0.3923 0.1686 0.0795 0.4275 05 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 06 0.9002 0.0130 0.5105 0.2451 0.2250 0.2092 0.0819 0.2509 0.4208 0.1762 0.0566 0.1211 07 0.4933 0.0078 0.5127 0.2574 0.1964 0.1597 0.0625 0.3446 0.3093 0.0882 0.0452 0.3416 08 0.8727 0.0097 0.5057 0.2494 0.2098 0.2059 0.0748 0.2632 0.4235 0.1646 0.0501 0.0971 09 0.9945 0.0201 0.5227 0.2292 0.2279 0.2476 0.1137 0.1828 0.4417 0.2730 0.0607 0.0842 10 0.7435 0.0101 0.5074 0.2521 0.1954 0.1927 0.0742 0.2730 0.4356 0.1333 0.0573 0.0810 11 0.7389 0.0105 0.5153 0.2603 0.2097 0.1848 0.0742 0.2889 0.4287 0.1030 0.0551 0.0574 12 0.6151 0.0887 0.5187 0.2439 0.1892 0.1812 0.0900 0.3101 0.3576 0.1062 0.0649 0.2727 13 0.5742 0.0261 0.5197 0.2411 0.2107 0.2031 0.0874 0.2653 0.4351 0.1348 0.0578 0.3354 14 0.8848 0.0128 0.5115 0.2515 0.2135 0.1962 0.0831 0.2704 0.4136 0.1648 0.0521 0.0632 15 0.8796 0.0180 0.5152 0.2505 0.2170 0.2135 0.0750 0.2404 0.4631 0.1525 0.0515 0.1441 16 0.7332 0.0119 0.5152 0.2461 0.1989 0.1919 0.0913 0.3274 0.3604 0.1206 0.0593 0.1913 17 0.8235 0.0218 0.5165 0.2454 0.2066 0.2144 0.0943 0.2453 0.4334 0.1698 0.0611 0.2250 18 0.7033 0.0097 0.5049 0.2389 0.2050 0.1964 0.0888 0.2677 0.4163 0.1513 0.0593 0.1475 19 0.9619 0.0188 0.4995 0.2521 0.2199 0.2094 0.0792 0.2053 0.4658 0.1895 0.0489 0.0574 20 0.5094 0.0502 0.5220 0.2357 0.2004 0.1899 0.0966 0.3442 0.3486 0.0993 0.0698 0.6241 21 0.7305 0.0209 0.5189 0.2575 0.2066 0.1830 0.0789 0.3115 0.3680 0.1395 0.0502 0.3031 22 0.8054 0.0171 0.5123 0.2584 0.2293 0.1930 0.0712 0.2328 0.4082 0.2056 0.0448 0.1192 23 0.8915 0.0269 0.4975 0.2673 0.2547 0.1780 0.0454 0.2236 0.4797 0.1419 0.0401 0.3023 24 0.6681 0.0131 0.5135 0.2501 0.2060 0.1955 0.0908 0.2611 0.4255 0.1500 0.0566 0.1881 25 0.7769 0.0099 0.5092 0.2489 0.2026 0.2094 0.0916 0.2618 0.3947 0.1984 0.0575 0.0861 26 0.8861 0.0185 0.5040 0.2459 0.2126 0.2122 0.0819 0.2243 0.4678 0.1729 0.0562 0.1230 27 0.6269 0.0112 0.5146 0.2390 0.2204 0.1958 0.0742 0.2665 0.4170 0.1516 0.0653 0.1975 28 0.8985 0.0081 0.5079 0.2424 0.2134 0.2143 0.0827 0.2482 0.4414 0.1699 0.0537 0.0613 29 0.8316 0.0160 0.5153 0.2548 0.2127 0.1930 0.0751 0.2570 0.4355 0.1445 0.0587 0.1502 30 0.6236 0.0241 0.5203 0.2312 0.2047 0.2184 0.1013 0.3181 0.3722 0.1319 0.0619 0.2459 31 0.8595 0.0352 0.5068 0.2517 0.2218 0.1987 0.0885 0.2727 0.3975 0.1705 0.0644 0.5956 32 0.6562 0.0083 0.5132 0.2383 0.2025 0.1882 0.0855 0.2944 0.4119 0.1261 0.0610 0.0436 El indicador también es posible definirlo a partir de una variable del censo, haciendo que el proceso sea más corto, para este caso es empleada la variable VIVIENDA.PISOS, agregada por dam En el primer bloque que código usando la función redatam.query() se realiza el conteo de viviendas por el material del piso. Seguido de esto, eliminamos los registros que no son de interés, por ejemplo, el total en el dam o total nacional, los cuales se identifican dentro de la base con la etiqueta __tot__. El siguiente paso es contar el número de viviendas por dam que tienen los pisos de tierra en el censo (Pobx) y el total de viviendas que respondieron a la pregunta (PobT), para finalmente realizar el cociente de estas dos preguntas. CONTEOS &lt;- redatam.query(mexico, &quot;freq ENT.REDCODEN by VIVIENDA.PISOS&quot;, tot.omit = FALSE) PISO &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)), all_vars(!. %in% c(&quot;__tot__&quot;, &quot;__mv__&quot;))) tasa_piso &lt;- PISO %&gt;% mutate(Pobx = ifelse(PISOS2_value %in% c(1), value, 0), PobT = value) %&gt;% group_by(depto = str_pad( string = REDCODEN1_value, width = 2, pad = &quot;0&quot; )) %&gt;% summarise(PobT = sum(PobT), Pobx = sum(Pobx)) %&gt;% transmute(depto, piso_tierra = Pobx / PobT) La tabla resultante se muestra a continuación. dam piso_tierra 01 0.0034 02 0.0151 03 0.0321 04 0.0229 05 0.0058 06 0.0169 07 0.1102 08 0.0171 09 0.0033 10 0.0328 11 0.0136 12 0.1218 13 0.0251 14 0.0107 15 0.0140 16 0.0388 17 0.0300 18 0.0303 19 0.0050 20 0.1174 21 0.0457 22 0.0108 23 0.0154 24 0.0430 25 0.0204 26 0.0200 27 0.0316 28 0.0113 29 0.0129 30 0.0546 31 0.0119 32 0.0098 El proceso se repite con otras preguntas del censo hasta consolidar la tabla siguiente. predictors_censo_dam &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/predictors_satelital_dam.rds&quot;) %&gt;% select(-c(luces_nocturnas:accesibilidad_hosp_caminado)) tba(predictors_censo_dam) dam area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 02 0.9534 0.0229 0.4955 0.2616 0.2301 0.2047 0.0634 0.2357 0.4697 0.1531 0.0529 0.0696 0.0065 0.5651 0.0661 0.0421 0.0412 0.3004 0.0151 0.1804 0.3994 0.2318 0.0456 0.2259 0.0228 03 0.9041 0.0287 0.4960 0.2486 0.2440 0.1943 0.0606 0.2363 0.4564 0.1661 0.0483 0.1025 0.0065 0.6135 0.1014 0.0840 0.0368 0.3336 0.0321 0.0824 0.1956 0.2478 0.0512 0.2404 0.0232 18 0.7033 0.0097 0.5049 0.2389 0.2050 0.1964 0.0888 0.2677 0.4163 0.1513 0.0593 0.1475 0.0352 0.7995 0.1430 0.1536 0.1522 0.5236 0.0303 0.0866 0.1657 0.2304 0.0753 0.2526 0.0204 14 0.8848 0.0128 0.5115 0.2515 0.2135 0.1962 0.0831 0.2704 0.4136 0.1648 0.0521 0.0632 0.0073 0.6678 0.0931 0.0624 0.0155 0.3628 0.0107 0.0487 0.0500 0.2490 0.0570 0.2130 0.0240 01 0.8448 0.0125 0.5142 0.2645 0.2106 0.1854 0.0655 0.2341 0.4255 0.1886 0.0576 0.0547 0.0056 0.5739 0.0761 0.0367 0.0110 0.3627 0.0034 0.0480 0.0239 0.2925 0.0458 0.2063 0.0289 11 0.7389 0.0105 0.5153 0.2603 0.2097 0.1848 0.0742 0.2889 0.4287 0.1030 0.0551 0.0574 0.0234 0.6324 0.2444 0.0840 0.0477 0.5050 0.0136 0.0357 0.0911 0.1598 0.0780 0.2699 0.0363 22 0.8054 0.0171 0.5123 0.2584 0.2293 0.1930 0.0712 0.2328 0.4082 0.2056 0.0448 0.1192 0.0198 0.7170 0.1123 0.0867 0.0173 0.3516 0.0108 0.0183 0.0784 0.3066 0.0570 0.2184 0.0320 13 0.5742 0.0261 0.5197 0.2411 0.2107 0.2031 0.0874 0.2653 0.4351 0.1348 0.0578 0.3354 0.0274 0.7899 0.2174 0.2145 0.1435 0.6082 0.0251 0.0456 0.1515 0.1998 0.0841 0.2612 0.0309 16 0.7332 0.0119 0.5152 0.2461 0.1989 0.1919 0.0913 0.3274 0.3604 0.1206 0.0593 0.1913 0.0172 0.6654 0.1993 0.1733 0.0890 0.5346 0.0388 0.1195 0.1977 0.1831 0.0976 0.2639 0.0283 15 0.8796 0.0180 0.5152 0.2505 0.2170 0.2135 0.0750 0.2404 0.4631 0.1525 0.0515 0.1441 0.0145 0.5008 0.1643 0.0741 0.0581 0.4246 0.0140 0.0347 0.0901 0.2242 0.0509 0.2802 0.0430 09 0.9945 0.0201 0.5227 0.2292 0.2279 0.2476 0.1137 0.1828 0.4417 0.2730 0.0607 0.0842 0.0021 0.1082 0.0392 0.0352 0.0051 0.2263 0.0033 0.0118 0.0552 0.3654 0.0312 0.1945 0.0358 06 0.9002 0.0130 0.5105 0.2451 0.2250 0.2092 0.0819 0.2509 0.4208 0.1762 0.0566 0.1211 0.0041 0.5624 0.0274 0.1100 0.0204 0.3942 0.0169 0.0267 0.1826 0.2598 0.0587 0.2182 0.0202 17 0.8235 0.0218 0.5165 0.2454 0.2066 0.2144 0.0943 0.2453 0.4334 0.1698 0.0611 0.2250 0.0120 0.4688 0.1835 0.1219 0.0416 0.4111 0.0300 0.0534 0.1661 0.2484 0.0660 0.2564 0.0332 31 0.8595 0.0352 0.5068 0.2517 0.2218 0.1987 0.0885 0.2727 0.3975 0.1705 0.0644 0.5956 0.0645 0.8017 0.0635 0.3560 0.1736 0.4484 0.0119 0.0319 0.0747 0.2498 0.0825 0.3299 0.0166 04 0.7731 0.0323 0.5084 0.2421 0.2224 0.2017 0.0724 0.2451 0.3923 0.1686 0.0795 0.4275 0.0475 0.3700 0.1682 0.3201 0.2263 0.5100 0.0229 0.1469 0.3977 0.2731 0.0850 0.3585 0.0241 21 0.7305 0.0209 0.5189 0.2575 0.2066 0.1830 0.0789 0.3115 0.3680 0.1395 0.0502 0.3031 0.0165 0.7313 0.2116 0.2361 0.1291 0.5776 0.0457 0.0696 0.1903 0.2148 0.0921 0.3378 0.0295 23 0.8915 0.0269 0.4975 0.2673 0.2547 0.1780 0.0454 0.2236 0.4797 0.1419 0.0401 0.3023 0.0153 0.6734 0.0873 0.1771 0.0842 0.4125 0.0154 0.0589 0.0871 0.2188 0.0556 0.3267 0.0187 29 0.8316 0.0160 0.5153 0.2548 0.2127 0.1930 0.0751 0.2570 0.4355 0.1445 0.0587 0.1502 0.0278 0.2229 0.1070 0.1055 0.0416 0.5676 0.0129 0.0621 0.0575 0.2265 0.0573 0.3033 0.0471 12 0.6151 0.0887 0.5187 0.2439 0.1892 0.1812 0.0900 0.3101 0.3576 0.1062 0.0649 0.2727 0.0819 0.7850 0.3264 0.4461 0.3372 0.6799 0.1218 0.2705 0.3482 0.1706 0.1423 0.3988 0.0347 20 0.5094 0.0502 0.5220 0.2357 0.2004 0.1899 0.0966 0.3442 0.3486 0.0993 0.0698 0.6241 0.0254 0.8382 0.3150 0.4952 0.3200 0.6950 0.1174 0.2275 0.4466 0.1531 0.1332 0.3634 0.0292 27 0.6269 0.0112 0.5146 0.2390 0.2204 0.1958 0.0742 0.2665 0.4170 0.1516 0.0653 0.1975 0.0231 0.6747 0.3255 0.3038 0.2462 0.6171 0.0316 0.0860 0.5613 0.2278 0.0752 0.3424 0.0591 07 0.4933 0.0078 0.5127 0.2574 0.1964 0.1597 0.0625 0.3446 0.3093 0.0882 0.0452 0.3416 0.0335 0.7851 0.3423 0.5200 0.3459 0.7706 0.1102 0.2195 0.6238 0.1495 0.1734 0.4423 0.0467 26 0.8861 0.0185 0.5040 0.2459 0.2126 0.2122 0.0819 0.2243 0.4678 0.1729 0.0562 0.1230 0.0106 0.6896 0.0597 0.0687 0.0778 0.3824 0.0200 0.0785 0.2462 0.2552 0.0516 0.2421 0.0341 08 0.8727 0.0097 0.5057 0.2494 0.2098 0.2059 0.0748 0.2632 0.4235 0.1646 0.0501 0.0971 0.0153 0.8879 0.0852 0.0772 0.0555 0.4189 0.0171 0.1695 0.3581 0.2461 0.0579 0.2010 0.0323 05 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 0.0056 0.5310 0.0474 0.0326 0.0523 0.4265 0.0058 0.0948 0.0993 0.2523 0.0472 0.2140 0.0421 25 0.7769 0.0099 0.5092 0.2489 0.2026 0.2094 0.0916 0.2618 0.3947 0.1984 0.0575 0.0861 0.0208 0.7730 0.0721 0.0885 0.1084 0.4526 0.0204 0.0328 0.0477 0.2889 0.0571 0.2744 0.0300 10 0.7435 0.0101 0.5074 0.2521 0.1954 0.1927 0.0742 0.2730 0.4356 0.1333 0.0573 0.0810 0.0282 0.8958 0.1638 0.1313 0.1463 0.5273 0.0328 0.2497 0.1717 0.2082 0.0591 0.2571 0.0444 32 0.6562 0.0083 0.5132 0.2383 0.2025 0.1882 0.0855 0.2944 0.4119 0.1261 0.0610 0.0436 0.0263 0.8465 0.1582 0.0632 0.1471 0.5192 0.0098 0.2558 0.1372 0.1944 0.0663 0.2408 0.0367 24 0.6681 0.0131 0.5135 0.2501 0.2060 0.1955 0.0908 0.2611 0.4255 0.1500 0.0566 0.1881 0.0212 0.8686 0.2583 0.2121 0.2065 0.5264 0.0430 0.1160 0.1841 0.2255 0.0740 0.2446 0.0311 19 0.9619 0.0188 0.4995 0.2521 0.2199 0.2094 0.0792 0.2053 0.4658 0.1895 0.0489 0.0574 0.0028 0.5360 0.0327 0.0394 0.0206 0.2899 0.0050 0.0215 0.0480 0.2778 0.0385 0.2144 0.0311 28 0.8985 0.0081 0.5079 0.2424 0.2134 0.2143 0.0827 0.2482 0.4414 0.1699 0.0537 0.0613 0.0050 0.6650 0.0693 0.0654 0.0724 0.4207 0.0113 0.0627 0.1211 0.2468 0.0497 0.2487 0.0407 30 0.6236 0.0241 0.5203 0.2312 0.2047 0.2184 0.1013 0.3181 0.3722 0.1319 0.0619 0.2459 0.0189 0.7740 0.2791 0.2865 0.2421 0.5909 0.0546 0.1136 0.4469 0.1920 0.1040 0.2914 0.0365 2.4.1 Mapas de las variables con información censal. temp2 &lt;- predictors_censo_dam %&gt;% select(-dam) %&gt;% names() temp2 &lt;- paste0(&quot;Recursos/Día1/Sesion2/0Recursos/&quot;, temp2, &quot; .png&quot;) knitr::include_graphics(temp2) "],["día-1---sesión-3--fundamentos-de-la-inferencia-bayesiana-en-r-y-stan.html", "Capítulo 3 Día 1 - Sesión 3- Fundamentos de la inferencia Bayesiana en R y STAN", " Capítulo 3 Día 1 - Sesión 3- Fundamentos de la inferencia Bayesiana en R y STAN El proyecto Manhattan y la estimación desagregada con encuestas de hogares "],["modelos-sintéticos-simples.html", "3.1 Modelos sintéticos simples", " 3.1 Modelos sintéticos simples 3.1.1 Regla de Bayes En términos de inferencia para \\(\\boldsymbol{\\theta}\\), es necesario encontrar la distribución de los parámetros condicionada a la observación de los datos. Para este fin, es necesario definir la distribución conjunta de la variable de interés con el vector de parámetros. \\[ p(\\boldsymbol{\\theta},\\mathbf{Y})=p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta}) \\] La distribución \\(p(\\boldsymbol{\\theta})\\) se le conoce con el nombre de distribución previa. El término \\(p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})\\) es la distribución de muestreo, verosimilitud o distribución de los datos. La distribución del vector de parámetros condicionada a los datos observados está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})=\\frac{p(\\boldsymbol{\\theta},\\mathbf{Y})}{p(\\mathbf{Y})}=\\frac{p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{Y})} \\] A la distribución \\(p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\) se le conoce con el nombre de distribución posterior. Nótese que el denominador no depende del vector de parámetros y considerando a los datos observados como fijos, corresponde a una constante y puede ser obviada. Por lo tanto, otra representación de la regla de Bayes está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\propto p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}) \\] 3.1.2 Inferencia Bayesiana. En términos de estimación, inferencia y predicción, el enfoque Bayesiano supone dos momentos o etapas: Antes de la recolección de las datos, en donde el investigador propone, basado en su conocimiento, experiencia o fuentes externas, una distribución de probabilidad previa para el parámetro de interés. Después de la recolección de los datos. Siguiendo el teorema de Bayes, el investigador actualiza su conocimiento acerca del comportamiento probabilístico del parámetro de interés mediante la distribución posterior de este. 3.1.3 Modelos uniparamétricos Los modelos que están definidos en términos de un solo parámetro que pertenece al conjunto de los números reales se definen como modelos uniparamétricos. 3.1.3.1 Modelo de unidad: Bernoulli Suponga que \\(Y\\) es una variable aleatoria con distribución Bernoulli dada por: \\[ p(Y \\mid \\theta)=\\theta^y(1-\\theta)^{1-y}I_{\\{0,1\\}}(y) \\] Como el parámetro \\(\\theta\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible formular varias opciones para la distribución previa del parámetro. En particular, la distribución uniforme restringida al intervalo \\([0,1]\\) o la distribución Beta parecen ser buenas opciones. Puesto que la distribución uniforme es un caso particular de la distribución Beta. Por lo tanto la distribución previa del parámetro \\(\\theta\\) estará dada por \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] y la distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid Y \\sim Beta(y+\\alpha,\\beta-y+1) \\end{equation*} \\] Cuando se tiene una muestra aleatoria \\(Y_1,\\ldots,Y_n\\) de variables con distribución Bernoulli de parámetro \\(\\theta\\), entonces la distribución posterior del parámetro de interés es \\[ \\begin{equation*} \\theta \\mid Y_1,\\ldots,Y_n \\sim Beta\\left(\\sum_{i=1}^ny_i+\\alpha,\\beta-\\sum_{i=1}^ny_i+n\\right) \\end{equation*} \\] Objetivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[ P_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) toma el valor de 1 cuando el ingreso de la persona es menor a la linea de pobreza 0 en caso contrario. Note que, \\[ \\begin{equation*} \\bar{Y}_d = P_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(P\\) esta dado por: \\[ \\hat{P} = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{P} = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 3.1.3.1.1 Práctica en R library(tidyverse) encuesta &lt;- readRDS(&quot;Recursos/Día1/Sesion3/Data/encuestaMEX20N1.Rds&quot;) Sea \\(Y\\) la variable aleatoria \\[ Y_{i}=\\begin{cases} 1 &amp; ingreso&lt;lp\\\\ 0 &amp; ingreso\\geq lp \\end{cases} \\] El tamaño de la muestra es de 19016 Indígena datay &lt;- encuesta %&gt;% filter(etnia_ee == 1, discapacidad == 1) %&gt;% transmute(y = ifelse(ingcorte &lt; lp, 1,0)) addmargins(table(datay$y)) %&gt;% tba() Var1 Freq 0 11770 1 7246 Sum 19016 Un grupo de estadístico experto decide utilizar una distribución previa Beta, definiendo los parámetros de la distribución previa como \\(Beta(\\alpha=1, \\beta=1)\\). La distribución posterior del parámetro de interés, que representa la probabilidad de estar por debajo de la linea de pobreza, es \\(Beta(7246 + 1, 1 - 7246 + 19016)=Beta(7247, 1.1771\\times 10^{4})\\) Figura 3.1: Distribución previa (línea roja) y distribución posterior (línea negra) La estimación del parámetro estaría dado por: \\[ E(X) = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{7247}{7247+ 1.1771\\times 10^{4}} = 0.38106 \\] luego, el intervalo de credibilidad para la distribución posterior es. n = length(datay$y) n1 = sum(datay$y) qbeta(c(0.025, 0.975), shape1 = 1 + n1, shape2 = 1 - n1 + n) ## [1] 0.3741700 0.3879738 3.1.3.1.2 Práctica en STAN En STAN es posible obtener el mismo tipo de inferencia creando cuatro cadenas cuya distribución de probabilidad coincide con la distribución posterior del ejemplo. data { // Entrada el modelo int&lt;lower=0&gt; n; // Numero de observaciones int y[n]; // Vector de longitud n real a; real b; } parameters { // Definir parámetro real&lt;lower=0, upper=1&gt; theta; } model { // Definir modelo y ~ bernoulli(theta); theta ~ beta(a, b); // Distribución previa } generated quantities { real ypred[n]; // vector de longitud n for (ii in 1:n){ ypred[ii] = bernoulli_rng(theta); } } Para compilar STAN debemos definir los parámetros de entrada sample_data &lt;- list(n = nrow(datay), y = datay$y, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan library(rstan) Bernoulli &lt;- &quot;Recursos/Día1/Sesion3/Data/modelosStan/1Bernoulli.stan&quot; options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_Bernoulli &lt;- stan( file = Bernoulli, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Bernoulli, file = &quot;Recursos/Día1/Sesion3/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) model_Bernoulli &lt;- readRDS(&quot;Recursos/Día1/Sesion3/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Ber1 &lt;- summary(model_Bernoulli, pars = &quot;theta&quot;)$summary tabla_Ber1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.3812 2e-04 0.0036 0.3741 0.3788 0.3811 0.3835 0.3885 572.8452 1.002 Para observar las cadenas compilamos las lineas de código library(posterior) library(ggplot2) temp &lt;- as_draws_df(as.array(model_Bernoulli,pars = &quot;theta&quot;)) p1 &lt;- ggplot(data = temp, aes(x = theta))+ geom_density(color = &quot;blue&quot;, size = 2) + stat_function(fun = posterior1, args = list(y = datay$y), size = 2) + theme_bw(base_size = 20) + labs(x = latex2exp::TeX(&quot;\\\\theta&quot;), y = latex2exp::TeX(&quot;f(\\\\theta)&quot;)) # ggsave(plot = p1, filename = &quot;Recursos/Día1/Sesion3/0Recursos/Bernoulli/Bernoulli2.png&quot;, scale = 2) p1 Figura 3.2: Resultado con STAN (línea azul) y posterior teórica (línea negra) Para validar las cadenas library(bayesplot) library(patchwork) posterior_theta &lt;- as.array(model_Bernoulli, pars = &quot;theta&quot;) p1 &lt;- (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / traceplot(model_Bernoulli,pars = &quot;theta&quot;, inc_warmup = TRUE) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Bernoulli/Bernoulli3.png&quot;, scale = 2) p1 Predicción de \\(Y\\) en cada una de las iteraciones de las cadenas. y_pred_B &lt;- as.array(model_Bernoulli, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, 1:n] p1 &lt;- ppc_dens_overlay(y = datay$y, y_pred2) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Bernoulli/Bernoulli4.png&quot;, scale = 2) p1 3.1.3.2 Modelo de área: Binomial Cuando se dispone de una muestra aleatoria de variables con distribución Bernoulli \\(Y_1,\\ldots,Y_n\\), la inferencia Bayesiana se puede llevar a cabo usando la distribución Binomial, puesto que es bien sabido que la suma de variables aleatorias Bernoulli \\[ \\begin{equation*} S=\\sum_{i=1}^nY_i \\end{equation*} \\] sigue una distribución Binomial. Es decir: \\[ \\begin{equation} p(S \\mid \\theta)=\\binom{n}{s}\\theta^s(1-\\theta)^{n-s}I_{\\{0,1,\\ldots,n\\}}(s), \\end{equation} \\] Nótese que, cuando \\(n=1\\) la distribución Binomial se convierte en una distribución Bernoulli. Puesto que, el parámetro \\(\\theta\\) es una proporción la distribución natural que modela este tipo de parámetros es la distribución beta la cual se define como: \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] La distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid S \\sim Beta(s+\\alpha,\\beta-s+n) \\end{equation*} \\] Ahora, cuando se tiene una sucesión de variables aleatorias \\(S_1,\\ldots,S_d, \\ldots,S_D\\) independientes y con distribución \\(Binomial(n_d,\\theta_d)\\) para \\(d=1,\\ldots,D\\). La distribución posterior del parámetro de interés \\(\\theta_d\\) es \\[ \\begin{equation*} \\theta_d \\mid s_d \\sim Beta\\left(s_d+\\alpha,\\ \\beta+ n_d- s_d\\right) \\end{equation*} \\] Objetivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[P_{d}=\\frac{\\sum_{U}y_{di}}{N_{d}}\\]. Un estimador aproximadamente insesgado para \\(P_{d}\\) basado en el diseño muestral es \\[ \\hat{P}^{DIR}_{d} = \\frac{\\sum_{s_{d}}w_{di}y_{di}}{\\sum_{s_{d}}w_{di}} \\] donde \\(w_{di}\\) es el factor de expansión de \\(i-\\)ésimo individuo en el \\(d-\\)ésimo dominio y \\(y_{di}\\) toma los valores de uno o cero. Ahora, dada la naturaleza de \\(P_d\\), es posible asumir que \\(P_{d}\\mid\\hat{P}^{DIR}_{d} \\sim Beta(\\alpha,\\beta)\\). Luego, el estimador bayesiano para \\(P_{d}\\) esta dado por \\(\\tilde{P}_{d}=E\\left(P_{d}\\mid\\hat{P}^{DIR}_{d}\\right)\\) y la varianza del estimador se obtiene como: \\[ Var\\left(\\tilde{P}_{d}\\right) = Var\\left(P_{d}\\mid\\hat{P}_{d}\\right)=E_{\\mathscr{M}}\\left(Var_{\\mathscr{P}}\\left(P_{d}\\mid\\hat{P}_{d}\\right)\\right)+Var_{\\mathscr{M}}\\left(E_{\\mathscr{P}}\\left(P_{d}\\mid\\hat{P}_{d}\\right)\\right) \\] 3.1.3.2.1 Práctica en STAN Sea \\(S_k\\) el conteo de personas en condición de pobreza en el \\(k-ésimo\\) departamento en la muestra. dataS &lt;- encuesta %&gt;% transmute( dam, y = ifelse(ingcorte &lt; lp, 1,0) ) %&gt;% group_by(dam) %&gt;% summarise(nd = n(), #Número de ensayos Sd = sum(y) #Número de éxito ) tba(dataS) dam nd Sd 01 10046 2506 02 13930 2284 03 8877 1661 04 7783 3318 05 13692 3730 06 10614 2280 07 8288 5540 08 15014 4014 09 9056 2430 10 10113 3826 11 11895 3943 12 9254 5438 13 7820 3167 14 9976 2763 15 13649 6112 16 7484 2747 17 8741 3531 18 6981 1706 19 12003 2777 20 9519 4981 21 8145 4603 22 13851 3492 23 7537 3347 24 9344 3768 25 11886 2774 26 8137 2116 27 7316 3204 28 7756 2741 29 8490 4508 30 9367 4927 31 10319 4693 32 8860 3576 Creando código de STAN data { int&lt;lower=0&gt; K; // Número de provincia int&lt;lower=0&gt; n[K]; // Número de ensayos int&lt;lower=0&gt; s[K]; // Número de éxitos real a; real b; } parameters { real&lt;lower=0, upper=1&gt; theta[K]; // theta_d|s_d } model { for(kk in 1:K) { s[kk] ~ binomial(n[kk], theta[kk]); } to_vector(theta) ~ beta(a, b); } generated quantities { real spred[K]; // vector de longitud K for(kk in 1:K){ spred[kk] = binomial_rng(n[kk],theta[kk]); } } Preparando el código de STAN Binomial2 &lt;- &quot;Recursos/Día1/Sesion3/Data/modelosStan/3Binomial.stan&quot; Organizando datos para STAN sample_data &lt;- list(K = nrow(dataS), s = dataS$Sd, n = dataS$nd, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_Binomial2 &lt;- stan( file = Binomial2, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Binomial2, &quot;Recursos/Día1/Sesion3/0Recursos/Binomial/model_Binomial2.rds&quot;) model_Binomial2 &lt;- readRDS(&quot;Recursos/Día1/Sesion3/0Recursos/Binomial/model_Binomial2.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Bin1 &lt;-summary(model_Binomial2, pars = &quot;theta&quot;)$summary tabla_Bin1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta[1] 0.2494 1e-04 0.0042 0.2412 0.2466 0.2495 0.2523 0.2573 4618.500 0.9992 theta[2] 0.1640 0e+00 0.0031 0.1581 0.1619 0.1641 0.1662 0.1701 4505.446 0.9988 theta[3] 0.1872 1e-04 0.0041 0.1793 0.1846 0.1872 0.1900 0.1950 5256.099 0.9997 theta[4] 0.4263 1e-04 0.0057 0.4149 0.4227 0.4263 0.4300 0.4375 4756.186 0.9990 theta[5] 0.2725 1e-04 0.0037 0.2654 0.2700 0.2725 0.2749 0.2801 4265.075 0.9987 theta[6] 0.2148 1e-04 0.0042 0.2067 0.2120 0.2148 0.2177 0.2229 4798.561 0.9993 theta[7] 0.6685 1e-04 0.0052 0.6581 0.6649 0.6686 0.6719 0.6786 4533.159 0.9988 theta[8] 0.2675 0e+00 0.0036 0.2604 0.2650 0.2674 0.2699 0.2747 5710.288 0.9985 theta[9] 0.2685 1e-04 0.0047 0.2592 0.2653 0.2685 0.2716 0.2778 5825.572 0.9990 theta[10] 0.3783 1e-04 0.0048 0.3691 0.3752 0.3784 0.3814 0.3879 4288.454 0.9987 theta[11] 0.3315 1e-04 0.0043 0.3228 0.3287 0.3315 0.3343 0.3402 5036.823 0.9989 theta[12] 0.5876 1e-04 0.0052 0.5771 0.5842 0.5877 0.5910 0.5979 5717.177 0.9991 theta[13] 0.4050 1e-04 0.0055 0.3940 0.4013 0.4049 0.4087 0.4157 4692.941 0.9986 theta[14] 0.2770 1e-04 0.0046 0.2683 0.2738 0.2769 0.2801 0.2861 4023.902 1.0000 theta[15] 0.4479 1e-04 0.0044 0.4394 0.4450 0.4478 0.4509 0.4564 3743.678 0.9987 theta[16] 0.3672 1e-04 0.0055 0.3566 0.3635 0.3670 0.3708 0.3779 4809.139 0.9988 theta[17] 0.4040 1e-04 0.0054 0.3939 0.4004 0.4041 0.4077 0.4145 4763.514 0.9987 theta[18] 0.2446 1e-04 0.0052 0.2346 0.2410 0.2446 0.2480 0.2548 5607.378 0.9992 theta[19] 0.2313 1e-04 0.0039 0.2236 0.2287 0.2314 0.2340 0.2390 5531.903 0.9986 theta[20] 0.5233 1e-04 0.0049 0.5135 0.5200 0.5233 0.5265 0.5329 4168.350 0.9995 theta[21] 0.5652 1e-04 0.0058 0.5545 0.5612 0.5652 0.5690 0.5765 4263.072 0.9989 theta[22] 0.2521 1e-04 0.0038 0.2446 0.2496 0.2520 0.2547 0.2594 5212.314 0.9987 theta[23] 0.4440 1e-04 0.0056 0.4330 0.4402 0.4440 0.4476 0.4551 5305.546 0.9982 theta[24] 0.4034 1e-04 0.0051 0.3933 0.3999 0.4034 0.4069 0.4135 4540.669 0.9985 theta[25] 0.2333 1e-04 0.0040 0.2251 0.2306 0.2334 0.2361 0.2413 5114.174 0.9991 theta[26] 0.2601 1e-04 0.0047 0.2512 0.2570 0.2600 0.2633 0.2694 3784.524 0.9992 theta[27] 0.4380 1e-04 0.0059 0.4261 0.4340 0.4380 0.4420 0.4496 4386.208 0.9987 theta[28] 0.3535 1e-04 0.0055 0.3429 0.3497 0.3534 0.3572 0.3643 4572.773 0.9990 theta[29] 0.5310 1e-04 0.0056 0.5205 0.5271 0.5309 0.5348 0.5420 4877.650 0.9983 theta[30] 0.5259 1e-04 0.0052 0.5162 0.5225 0.5258 0.5292 0.5359 4823.609 0.9986 theta[31] 0.4547 1e-04 0.0052 0.4447 0.4513 0.4547 0.4582 0.4649 4772.350 0.9985 theta[32] 0.4037 1e-04 0.0052 0.3940 0.4003 0.4034 0.4071 0.4142 3598.994 0.9991 Para validar las cadenas p1 &lt;- mcmc_areas(as.array(model_Binomial2, pars = &quot;theta&quot;)) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Binomial/Binomial1.png&quot;, scale = 2) p1 p1 &lt;- mcmc_trace(as.array(model_Binomial2, pars = &quot;theta&quot;)) # traceplot(model_Binomial2, pars = &quot;theta&quot;,inc_warmup = TRUE) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Binomial/Binomial2.png&quot;, # scale = 2) p1 y_pred_B &lt;- as.array(model_Binomial2, pars = &quot;spred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 200) y_pred2 &lt;- y_pred_B[rowsrandom, ] g1 &lt;- ggplot(data = dataS, aes(x = Sd))+ geom_histogram(aes(y = ..density..)) + geom_density(size = 2, color = &quot;blue&quot;) + labs(y = &quot;&quot;)+ theme_bw(20) g2 &lt;- ppc_dens_overlay(y = dataS$Sd, y_pred2) p1 &lt;- g1/g2 # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Binomial/Binomial3.png&quot;, # scale = 2) p1 3.1.3.3 Modelo de unidad: Normal con media desconocida Suponga que \\(Y_1,\\cdots,Y_n\\) son variables independientes e idénticamente distribuidos con distribución \\(Normal(\\theta,\\sigma^2)\\) con \\(\\theta\\) desconocido pero \\(\\sigma^2\\) conocido. De esta forma, la función de verosimilitud de los datos está dada por \\[ \\begin{align*} p(\\mathbf{Y} \\mid \\theta) &amp;=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(y_i-\\theta)^2\\right\\}I_\\mathbb{R}(y) \\\\ &amp;=(2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\theta)^2\\right\\} \\end{align*} \\] Como el parámetro \\(\\theta\\) puede tomar cualquier valor en los reales, es posible asignarle una distribución previa \\(\\theta \\sim Normal(\\mu,\\tau^2)\\). Bajo este marco de referencia se tienen los siguientes resultados La distribución posterior del parámetro de interés \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta|\\mathbf{Y} \\sim Normal(\\mu_n,\\tau^2_n) \\end{equation*} \\] En donde \\[ \\begin{equation} \\mu_n=\\frac{\\frac{n}{\\sigma^2}\\bar{Y}+\\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}} \\ \\ \\ \\ \\ \\ \\ \\text{y} \\ \\ \\ \\ \\ \\ \\ \\tau_n^2=\\left(\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}\\right)^{-1} \\end{equation} \\] Objetivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y}_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) es el ingreso de cada personas Note que, \\[ \\begin{equation*} \\bar{Y}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(\\bar{Y}\\) esta dado por: \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 3.1.3.3.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% filter(dam == &quot;01&quot;, ingcorte&gt;0) %&gt;% transmute( dam , logIngreso = log(ingcorte +1)) #3 media &lt;- mean(dataNormal$logIngreso) Sd &lt;- sd(dataNormal$logIngreso) g1 &lt;- ggplot(dataNormal,aes(x = logIngreso))+ geom_density(size =2, color = &quot;blue&quot;) + stat_function(fun =dnorm, args = list(mean = media, sd = Sd), size =2) + theme_bw(base_size = 20) + labs(y = &quot;&quot;, x = (&quot;Log(Ingreso)&quot;)) g2 &lt;- ggplot(dataNormal, aes(sample = logIngreso)) + stat_qq() + stat_qq_line() + theme_bw(base_size = 20) p1 &lt;- g1|g2 # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Normal/Normal1.png&quot;, # scale = 2) p1 Creando código de STAN data { int&lt;lower=0&gt; n; // Número de observaciones real y[n]; // LogIngreso real &lt;lower=0&gt; Sigma; // Desviación estándar } parameters { real theta; } model { y ~ normal(theta, Sigma); theta ~ normal(0, 1000); // Distribución previa } generated quantities { real ypred[n]; // Vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,Sigma); } } Preparando el código de STAN NormalMedia &lt;- &quot;Recursos/Día1/Sesion3/Data/modelosStan/4NormalMedia.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), Sigma = sd(dataNormal$logIngreso), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_NormalMedia &lt;- stan( file = NormalMedia, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia, &quot;Recursos/Día1/Sesion3/0Recursos/Normal/model_NormalMedia.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día1/Sesion3/0Recursos/Normal/model_NormalMedia.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Nor1 &lt;- summary(model_NormalMedia, pars = &quot;theta&quot;)$summary tabla_Nor1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 8.2466 3e-04 0.0073 8.2323 8.2417 8.2465 8.2515 8.2605 705.1854 1.0025 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) p1 &lt;- (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) # ggsave(plot = p1, # filename =&quot;Recursos/Día1/Sesion3/0Recursos/Normal/Normal2.png&quot;, # scale = 2) p1 y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(dataNormal$logIngreso), y_pred2)/ ppc_dens_overlay(y = exp(as.numeric(dataNormal$logIngreso))-1, exp(y_pred2)-1) + xlim(0,20000) ggsave(plot = p1, filename =&quot;Recursos/Día1/Sesion3/0Recursos/Normal/Normal3.png&quot;, scale = 2) p1 3.1.4 Modelos multiparamétricos La distribución normal univariada que tiene dos parámetros: la media \\(\\theta\\) y la varianza \\(\\sigma^2\\). La distribución multinomial cuyo parámetro es un vector de probabilidades \\(\\boldsymbol{\\theta}\\). 3.1.4.1 Modelo de unidad: Normal con media y varianza desconocida Supongamos que se dispone de realizaciones de un conjunto de variables independientes e idénticamente distribuidas \\(Y_1,\\cdots,Y_n\\sim N(\\theta,\\sigma^2)\\). Cuando se desconoce tanto la media como la varianza de la distribución es necesario plantear diversos enfoques y situarse en el más conveniente, según el contexto del problema. En términos de la asignación de las distribuciones previas para \\(\\theta\\) y \\(\\sigma^2\\) es posible: Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son informativas. Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son no informativas. Suponer que la distribución previa para \\(\\theta\\) depende de \\(\\sigma^2\\) y escribirla como \\(p(\\theta \\mid \\sigma^2)\\), mientras que la distribución previa de \\(\\sigma^2\\) no depende de \\(\\theta\\) y se puede escribir como \\(p(\\sigma^2)\\). La distribución previa para el parámetro \\(\\theta\\) será \\[ \\begin{equation*} \\theta \\sim Normal(0,10000) \\end{equation*} \\] Y la distribución previa para el parámetro \\(\\sigma^2\\) será \\[ \\begin{equation*} \\sigma^2 \\sim IG(0.0001,0.0001) \\end{equation*} \\] La distribución posterior condicional de \\(\\theta\\) es \\[ \\begin{equation} \\theta \\mid \\sigma^2,\\mathbf{Y} \\sim Normal(\\mu_n,\\tau_n^2) \\end{equation} \\] En donde las expresiones para \\(\\mu_n\\) y \\(\\tau_n^2\\) están dados previamente. En el siguiente enlace enconará el libro: Modelos Bayesianos con R y STAN donde puede profundizar en el desarrollo matemático de los resultados anteriores. Objetivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y}_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) es el ingreso de cada personas Note que, \\[ \\begin{equation*} \\bar{Y}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(\\bar{Y}\\) esta dado por: \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] 3.1.4.1.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% filter(dam == &quot;01&quot;, ingcorte&gt;0) %&gt;% transmute(dam, logIngreso = log(ingcorte +1)) Creando código de STAN data { int&lt;lower=0&gt; n; real y[n]; } parameters { real sigma; real theta; } transformed parameters { real sigma2; sigma2 = pow(sigma, 2); } model { y ~ normal(theta, sigma); theta ~ normal(0, 1000); sigma2 ~ inv_gamma(0.001, 0.001); } generated quantities { real ypred[n]; // vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,sigma); } } Preparando el código de STAN NormalMeanVar &lt;- &quot;Recursos/Día1/Sesion3/Data/modelosStan/5NormalMeanVar.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_NormalMedia &lt;- stan( file = NormalMeanVar, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia,&quot;Recursos/Día1/Sesion3/0Recursos/Normal/model_NormalMedia2.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día1/Sesion3/0Recursos/Normal/model_NormalMedia2.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\sigma^2\\) es: tabla_Nor2 &lt;- summary(model_NormalMedia, pars = c(&quot;theta&quot;, &quot;sigma2&quot;, &quot;sigma&quot;))$summary tabla_Nor2 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 8.2467 2e-04 0.0073 8.2325 8.2418 8.2467 8.2515 8.2611 1145.486 1.0005 sigma2 0.5225 2e-04 0.0072 0.5086 0.5176 0.5223 0.5276 0.5368 2093.372 0.9990 sigma 0.7228 1e-04 0.0050 0.7132 0.7194 0.7227 0.7264 0.7327 2093.723 0.9989 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) p1 &lt;- (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Normal/Normal4.png&quot;, # scale = 2) p1 posterior_sigma2 &lt;- as.array(model_NormalMedia, pars = &quot;sigma2&quot;) p1 &lt;- (mcmc_dens_chains(posterior_sigma2) + mcmc_areas(posterior_sigma2) ) / mcmc_trace(posterior_sigma2) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Normal/Normal5.png&quot;, # scale = 2) p1 posterior_sigma &lt;- as.array(model_NormalMedia, pars = &quot;sigma&quot;) p1 &lt;- (mcmc_dens_chains(posterior_sigma) + mcmc_areas(posterior_sigma) ) / mcmc_trace(posterior_sigma) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Normal/Normal6.png&quot;, # scale = 2) p1 y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(exp(dataNormal$logIngreso)-1), y_pred2) + xlim(0,20000) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Normal/Normal7.png&quot;, # scale = 2) p1 3.1.4.2 Modelo de área: Multinomial En esta sección discutimos el modelamiento bayesiano de datos provenientes de una distribución multinomial que corresponde a una extensión multivariada de la distribución binomial. Suponga que \\(\\textbf{Y}=(Y_1,\\ldots,Y_K)^{T}\\) es un vector aleatorio con distribución multinomial, así, su distribución está parametrizada por el vector \\(\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_K)^{T}\\) y está dada por la siguiente expresión \\[ \\begin{equation} p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})=\\binom{n}{y_1,\\ldots,y_K}\\prod_{k=1}^K\\theta_k^{y_k} \\ \\ \\ \\ \\ \\theta_k&gt;0 \\texttt{ , } \\sum_{k=1}^{K}y_k=n \\texttt{ y } \\sum_{k=1}^K\\theta_k=1 \\end{equation} \\] Donde \\[ \\begin{equation*} \\binom{n}{y_1,\\ldots,y_K}=\\frac{n!}{y_1!\\cdots y_K!}. \\end{equation*} \\] Como cada parámetro \\(\\theta_k\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible asignar a la distribución de Dirichlet como la distribución previa del vector de parámetros. Por lo tanto la distribución previa del vector de parámetros \\(\\boldsymbol{\\theta}\\), parametrizada por el vector de hiperparámetros \\(\\boldsymbol{\\alpha}=(\\alpha_1,\\ldots,\\alpha_K)^{T}\\), está dada por \\[ \\begin{equation} p(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha})=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_K)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_K)} \\prod_{k=1}^K\\theta_k^{\\alpha_k-1} \\ \\ \\ \\ \\ \\alpha_k&gt;0 \\texttt{ y } \\sum_{k=1}^K\\theta_k=1 \\end{equation} \\] La distribución posterior del parámetro \\(\\boldsymbol{\\theta}\\) sigue una distribución \\(Dirichlet(y_1+\\alpha_1,\\ldots,y_K+\\alpha_K)\\) 3.1.4.2.1 Práctica en STAN Sea \\(Y\\) condición de actividad laboral dataMult &lt;- encuesta %&gt;% filter(condact3 %in% 1:3) %&gt;% transmute( empleo = as_factor(condact3)) %&gt;% group_by(empleo) %&gt;% tally() %&gt;% mutate(theta = n/sum(n)) tba(dataMult) empleo n theta Ocupado 149387 0.5876 Desocupado 6969 0.0274 Inactivo 97867 0.3850 donde 1 corresponde a Ocupado, 2 son los Desocupado y 3 son Inactivo Creando código de STAN data { int&lt;lower=0&gt; k; // Número de cátegoria int y[k]; // Número de exitos vector[k] alpha; // Parámetro de las distribción previa } parameters { simplex[k] theta; } transformed parameters { real delta; // Tasa de desocupación delta = theta[2]/ (theta[2] + theta[1]); // (Desocupado)/(Desocupado + Ocupado) } model { y ~ multinomial(theta); theta ~ dirichlet(alpha); } generated quantities { int ypred[k]; ypred = multinomial_rng(theta, sum(y)); } Preparando el código de STAN Multinom &lt;- &quot;Recursos/Día1/Sesion3/Data/modelosStan/6Multinom.stan&quot; Organizando datos para STAN sample_data &lt;- list(k = nrow(dataMult), y = dataMult$n, alpha = c(0.5, 0.5, 0.5)) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_Multinom &lt;- stan( file = Multinom, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_Multinom, &quot;Recursos/Día1/Sesion3/0Recursos/Multinomial/model_Multinom.rds&quot;) model_Multinom &lt;- readRDS(&quot;Recursos/Día1/Sesion3/0Recursos/Multinomial/model_Multinom.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\delta\\) es: tabla_Mul1 &lt;- summary(model_Multinom, pars = c(&quot;delta&quot;, &quot;theta&quot;))$summary tabla_Mul1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat delta 0.0446 0 5e-04 0.0436 0.0442 0.0446 0.0449 0.0456 830.6843 1.0017 theta[1] 0.5876 0 9e-04 0.5858 0.5870 0.5876 0.5882 0.5894 1905.1503 0.9986 theta[2] 0.0274 0 3e-04 0.0268 0.0272 0.0274 0.0276 0.0281 811.4177 1.0018 theta[3] 0.3850 0 9e-04 0.3832 0.3843 0.3850 0.3856 0.3868 1564.5875 0.9987 posterior_theta1 &lt;- as.array(model_Multinom, pars = &quot;theta[1]&quot;) p1 &lt;- (mcmc_dens_chains(posterior_theta1) + mcmc_areas(posterior_theta1) ) / mcmc_trace(posterior_theta1) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Multinomial/Multinomial1.png&quot;, # scale = 2) p1 posterior_theta2 &lt;- as.array(model_Multinom, pars = &quot;theta[2]&quot;) p1 &lt;- (mcmc_dens_chains(posterior_theta2) + mcmc_areas(posterior_theta2) ) / mcmc_trace(posterior_theta2) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Multinomial/Multinomial2.png&quot;, # scale = 2) p1 posterior_theta3 &lt;- as.array(model_Multinom, pars = &quot;theta[3]&quot;) p1 &lt;- (mcmc_dens_chains(posterior_theta3) + mcmc_areas(posterior_theta3) ) / mcmc_trace(posterior_theta3) ggsave(plot = p1, filename = &quot;Recursos/Día1/Sesion3/0Recursos/Multinomial/Multinomial3.png&quot;, scale = 2) p1 posterior_delta &lt;- as.array(model_Multinom, pars = &quot;delta&quot;) p1 &lt;- (mcmc_dens_chains(posterior_delta) + mcmc_areas(posterior_delta) ) / mcmc_trace(posterior_delta) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Multinomial/Multinomial4.png&quot;, # scale = 2) p1 La imagen es muy pesada no se carga al repositorio. n &lt;- nrow(dataMult) y_pred_B &lt;- as.array(model_Multinom, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 200) y_pred2 &lt;- y_pred_B[, 1:n] p1 &lt;- ppc_dens_overlay(y = as.numeric(dataMult$n), y_pred2) # ggsave(plot = p1, # filename = &quot;Recursos/Día1/Sesion3/0Recursos/Multinomial/ppc_multinomial.PNG&quot;, # scale = 2) p1 "],["día-1---sesión-4--función-generalizada-de-varianza.html", "Capítulo 4 Día 1 - Sesión 4- Función Generalizada de Varianza", " Capítulo 4 Día 1 - Sesión 4- Función Generalizada de Varianza Uno de los insumos más importantes en el modelo de áreas es la varianza del estimador directo, a nivel de dominio, la cual no puede calcularse de ningún modo. En correspondencia, este valor debe estimarse desde los datos recolectados en cada dominio. Sin embargo, en dominios en las que se cuenta con un tamaño de muestra muy pequeño, estas estimaciones no tendrán un buen comportamiento. Por ende, es muy útil utilizar un modelo de suavizamiento de las varianzas para eliminar el ruido y la volatilidad de estas estimaciones y extraer la verdadera señal del proceso Hidiroglou (2019) afirma que \\(E_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta}\\) y \\(V_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\sigma_{u}^2+\\tilde{\\sigma}^2_{d}\\), en donde el subíndice \\(\\mathscr{MP}\\) hace referencia a la inferencia doble que se debe tener en cuenta en este tipo de ajustes y define la medida de probabilidad conjunta entre el modelo y el diseño de muestreo. \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento y la inclusión de las covariables auxiliares (\\(\\boldsymbol{x}_{d}\\)). \\(\\mathscr{MP}\\) hace referencia a la medida de probabilidad inducida por el diseño de muestreo complejo que induce las estimaciones directas. La solución que acá se plantea se conoce con el nombre de Función Generalizada de Varianza, la cual consiste en ajustar un modelo log-lineal a la varianza directa estimada. Partiendo del hecho de que se tiene acceso a un estimador insesgado de \\(\\sigma^2\\), denotado por \\(\\hat{\\sigma}^2\\) se tiene que: \\[ E_{\\mathscr{MP}}\\left(\\hat{\\sigma}_{d}^{2}\\right)=E_{\\mathscr{M}}\\left(E_{\\mathscr{P}}\\left(\\hat{\\sigma}_{d}^{2}\\right)\\right)=E_{\\mathscr{M}}\\left(\\sigma_{d}^{2}\\right)=\\tilde{\\sigma}_{d}^{2} \\] La anterior igualdad puede interpretarse como que un estimador insesgado y simple de \\(\\tilde{\\sigma}_{d}^{2}\\) puede ser \\(\\hat{\\sigma}_{d}^{2}\\). Sin embargo, este estimador de muestreo es inestable cuando el tamaño de muestra es pequeño, que es justo el paradigma dominante en la estimación de áreas pequeñas. Rivest and Belmonte (2000) consideran modelos de suavizamiento para la estimación de las varianzas directas definidos de la siguiente manera: \\[ \\log\\left(\\hat{\\sigma}_{d}^{2}\\right)=\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}+\\boldsymbol{\\varepsilon}_{d} \\] En donde \\(\\boldsymbol{z}_{d}\\) es un vector de covariables explicativas que son funciones de \\(\\boldsymbol{x}_{d}\\), \\(\\boldsymbol{\\alpha}\\) es un vector de parámetros que deben ser estimados, \\(\\boldsymbol{\\varepsilon}_{d}\\) son errores aleatorios con media cero y varianza constante, que se asumen idénticamente distribuidos condicionalmente sobre \\(\\boldsymbol{z}_{d}\\). Del anterior modelo, la estimación suavizada de la varianza de muestreo está dada por: \\[ \\tilde{\\sigma}_{d}^{2}=E_{\\mathscr{MP}}\\left(\\sigma_{d}^{2}\\right)=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)\\times\\Delta \\] En donde, \\(E_{\\mathscr{MP}}\\left(\\varepsilon_{d}\\right)=\\Delta\\). No hay necesidad de especificar una distribución paramétrica para los errores de este modelo. Al utilizar el método de los momentos, se tiene el siguiente estimador insesgado para \\(\\Delta\\): \\[ \\hat{\\Delta}=\\frac{\\sum_{d=1}^{D}\\hat{\\sigma}_{d}^{2}}{\\sum_{d=1}^{D}\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)} \\] De la misma forma, al utilizar los procedimientos estándar en una regresión lineal, la estimación del coeficiente de parámetros de regresión está dada por la siguiente expresión: \\[ \\hat{\\boldsymbol{\\alpha}}=\\left(\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\boldsymbol{z}_{d}^{T}\\right)^{-1}\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\log\\left(\\hat{\\sigma}_{d}^{2}\\right) \\] Por último, el estimador suavizado de la varianza muestral está definido por: \\[ \\hat{\\tilde{\\sigma}}_{d}^{2}=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\hat{\\boldsymbol{\\alpha}}\\right)\\hat{\\Delta} \\] "],["datos-de-la-encuesta.html", "4.1 Datos de la encuesta", " 4.1 Datos de la encuesta El siguiente bloque de código utiliza varias librerías en R (tidyverse y magrittr), así como también utiliza una función definida en otro archivo (source(“0Recursos/0Source_FH.R”)). Luego, el código carga la encuesta que esta almacenada en un archivo de datos en formato RDS y utiliza la función %&gt;% para encadenar una serie de transformaciones en los datos: transmute() se utiliza para seleccionar y renombrar columnas. Se crea una nueva variable llamada pobreza que se establece en 1 si la variable ingcorte(ingreso percapital) es menor que la variable lp, y en 0 en caso contrario. La función ifelse() se utiliza para asignar valores a la variable “pobreza” en función de si el ingreso de un individuo es menor o mayor que el umbral de pobreza. library(tidyverse) library(magrittr) source(&quot;Recursos/Día1/Sesion4/0Recursos/0Source_FH.R&quot;) encuesta &lt;- readRDS(&quot;Recursos/Día1/Sesion4/Data/encuestaMEX20N1.rds&quot;) %&gt;% transmute( dam , dam2, wkx = `fep`, upm , estrato , pobreza = ifelse(ingcorte &lt; lp, 1 , 0)) dam: Corresponde al código asignado a la división administrativa mayor del país. dam2: Corresponde al código asignado a la segunda división administrativa del país. lp Linea de pobreza definida por CEPAL. wkx Factor de expansión por persona dam dam2 wkx upm estrato pobreza 01 01001 190 0000001 011 1 01 01001 190 0000001 011 1 01 01001 190 0000001 011 1 01 01001 190 0000001 011 1 01 01001 190 0000001 011 1 01 01001 190 0000001 011 1 01 01001 190 0000001 011 1 01 01001 189 0000002 011 0 01 01001 189 0000002 011 0 01 01001 189 0000002 011 0 En el siguiente bloque de código utiliza las librerías survey y srvyr para crear un diseño de muestreo a partir de una base de datos de encuestas. El diseño de muestreo incluye información sobre las unidades primarias de muestreo (UPM), los pesos de muestreo (wkx), y las estratas (estrato) utilizadas en el muestreo. Además, se utiliza la opción “survey.lonely.psu” para ajustar los tamaños de muestra en los grupos de unidades primarias de muestreo que no tienen otras unidades primarias de muestreo en el mismo grupo. library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- as_survey_design( ids = upm, weights = wkx, strata = estrato, nest = TRUE, .data = encuesta ) #summary(diseno) Para la estimación directa de la proporción se emplea la función direct.supr, disponible en el archivo 0Source_FH.R. Está función realiza las estimaciones y criterios de calidad en una encuesta de muestreo complejo con diseño estratificado y por conglomerados. Toma cinco argumentos: design.base, variable, group, upm y estrato. La función comienza cargando varios paquetes, como rlang, tidyverse, dplyr, survey y srvyr. Luego, los argumentos group, variable, upm y estrato se convierten en argumentos utilizando la función enquo. La función utiliza la encuesta de muestreo complejo design.base para calcular las estimaciones de los parámetros y los criterios de calidad. Utiliza la función survey_mean() de la librería survey para calcular la media y los intervalos de confianza de la variable de interés. La función también calcula otros indicadores de calidad, como el coeficiente de variación, el tamaño de muestra efectivo y el efecto del diseño. Luego, utiliza la función as.data.frame() para convertir los resultados en un objeto de marco de datos. Además, la función calcula otros criterios de calidad para determinar si las estimaciones son confiables. En particular, evalúa si se cumple un umbral mínimo para el número de grados de libertad, si la muestra es suficientemente grande y si el efecto del diseño es razonable. La función también tiene la opción de incluir o excluir ciertos grupos de muestreo basados en sus características. directodam2 &lt;- direct.supr(design.base = diseno, variable = pobreza, group = dam2, upm = upm, estrato = estrato) directodam2 %&gt;% group_by(Flag) %&gt;% summarise(n = n()) %&gt;% arrange(n) %&gt;% tba() Flag n Incluir 268 Excluir 822 Para los dominios que no son excluidos se hace la transformación arcoseno, calculo del DEFF y varianza base_sae &lt;- directodam2 %&gt;% filter(Flag != &quot;Excluir&quot;) %&gt;% transmute( dam2 = dam2, # Id para los dominios nd = n, # Número de observaciones por dominios n_effec = n.eff, # n efectivo. pobreza = p, # Estimación de la variable vardir = ee ^ 2, # Estimación de la varianza directa cv = CV, deff_dam2 = deff # Deff por dominio ) # View(base_sae) tba(head(base_sae)) dam2 nd n_effec pobreza vardir cv deff_dam2 01001 5305 1701.8655 0.2387 0.0002 6.4717 3.1172 01003 572 304.7587 0.3220 0.0042 20.1055 1.8769 01005 1202 380.1773 0.2653 0.0016 15.2573 3.1617 01006 619 446.3468 0.2348 0.0014 15.6690 1.3868 01007 748 814.1301 0.2809 0.0009 10.9058 0.9188 01011 691 218.5612 0.2593 0.0036 23.1166 3.1616 seguidamente se realiza la transformación \\(\\log(\\hat{\\sigma}^2_d)\\), además se realiza la selección de las columnas identificador del municipio (dam2), la estimación directa (pobreza), El número de personas en el dominio (nd) y la varianza estimada del para la estimación directa vardir,siendo esta la que transforma mediante la función log(). baseFGV &lt;- base_sae %&gt;% select(dam2, pobreza, nd, vardir) %&gt;% mutate(ln_sigma2 = log(vardir)) "],["análisis-gráfico.html", "4.2 Análisis gráfico", " 4.2 Análisis gráfico El primer gráfico, p1, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la variable pobreza, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como pobreza. El segundo gráfico, p2, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la variable nd, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Tamaño de muestra. El tercer gráfico, p3, muestra una gráfica de dispersión de la variable ln_sigma2 en función del producto de pobreza y nd, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Número de pobres. El cuarto gráfico, p4, muestra una gráfica de dispersión de la variable ln_sigma2 en función de la raíz cuadrada de la variable pobreza, con una línea suave que representa una estimación de la tendencia. El eje x está etiquetado como Raiz cuadrada de pobreza. En general, los gráficos estan diseñados para explorar la relación entre ln_sigma2 y diferentes variables independientes, como pobreza, nd, y la raíz cuadrada de la pobreza. La elección de utilizar la función “loess” para suavizar las líneas en lugar de una línea recta puede ayudar a visualizar mejor las tendencias generales en los datos. theme_set(theme_bw()) # pobreza vs Ln_sigma2 # p1 &lt;- ggplot(baseFGV, aes(x = pobreza, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;pobreza&quot;) # Tamaño de muestra vs Ln_sigma2 # p2 &lt;- ggplot(baseFGV, aes(x = nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Tamaño de muestra&quot;) # Número de pobres vs Ln_sigma2 # p3 &lt;- ggplot(baseFGV, aes(x = pobreza * nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Número de pobres&quot;) # Raiz_pobreza vs Ln_sigma2 # p4 &lt;- ggplot(baseFGV, aes(x = sqrt(pobreza), y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Raiz cuadrada de pobreza&quot;) library(patchwork) (p1 | p2) / (p3 | p4) "],["modelo-para-la-varianza.html", "4.3 Modelo para la varianza", " 4.3 Modelo para la varianza El código ajusta un modelo de regresión lineal múltiple (utilizando la función lm()), donde ln_sigma2 es la variable respuesta y las variables predictoras son pobreza, nd, y varias transformaciones de éstas. El objetivo de este modelo es estimar la función generalizada de varianza (FGV) para los dominios observados. library(gtsummary) FGV1 &lt;- lm(ln_sigma2 ~ 1+ I(sqrt(pobreza)) + I(pobreza*nd), data = baseFGV) tbl_regression(FGV1) %&gt;% add_glance_table(include = c(r.squared, adj.r.squared)) #ucllmwtyac table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #ucllmwtyac thead, #ucllmwtyac tbody, #ucllmwtyac tfoot, #ucllmwtyac tr, #ucllmwtyac td, #ucllmwtyac th { border-style: none; } #ucllmwtyac p { margin: 0; padding: 0; } #ucllmwtyac .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ucllmwtyac .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ucllmwtyac .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ucllmwtyac .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ucllmwtyac .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ucllmwtyac .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ucllmwtyac .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ucllmwtyac .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ucllmwtyac .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ucllmwtyac .gt_column_spanner_outer:first-child { padding-left: 0; } #ucllmwtyac .gt_column_spanner_outer:last-child { padding-right: 0; } #ucllmwtyac .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ucllmwtyac .gt_spanner_row { border-bottom-style: hidden; } #ucllmwtyac .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ucllmwtyac .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ucllmwtyac .gt_from_md > :first-child { margin-top: 0; } #ucllmwtyac .gt_from_md > :last-child { margin-bottom: 0; } #ucllmwtyac .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ucllmwtyac .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ucllmwtyac .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ucllmwtyac .gt_row_group_first td { border-top-width: 2px; } #ucllmwtyac .gt_row_group_first th { border-top-width: 2px; } #ucllmwtyac .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ucllmwtyac .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ucllmwtyac .gt_first_summary_row.thick { border-top-width: 2px; } #ucllmwtyac .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ucllmwtyac .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ucllmwtyac .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ucllmwtyac .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #ucllmwtyac .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ucllmwtyac .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ucllmwtyac .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ucllmwtyac .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ucllmwtyac .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ucllmwtyac .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ucllmwtyac .gt_left { text-align: left; } #ucllmwtyac .gt_center { text-align: center; } #ucllmwtyac .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ucllmwtyac .gt_font_normal { font-weight: normal; } #ucllmwtyac .gt_font_bold { font-weight: bold; } #ucllmwtyac .gt_font_italic { font-style: italic; } #ucllmwtyac .gt_super { font-size: 65%; } #ucllmwtyac .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #ucllmwtyac .gt_asterisk { font-size: 100%; vertical-align: 0; } #ucllmwtyac .gt_indent_1 { text-indent: 5px; } #ucllmwtyac .gt_indent_2 { text-indent: 10px; } #ucllmwtyac .gt_indent_3 { text-indent: 15px; } #ucllmwtyac .gt_indent_4 { text-indent: 20px; } #ucllmwtyac .gt_indent_5 { text-indent: 25px; } Characteristic Beta 95% CI1 p-value I(sqrt(pobreza)) 4.6 4.0, 5.2 I(pobreza * nd) 0.00 0.00, 0.00 R² 0.621 Adjusted R² 0.618 1 CI = Confidence Interval Después de tener la estimación del modelo se debe obtener el valor de la constante \\(\\Delta\\) para lo cual se usa el siguiente código. delta.hat = sum(baseFGV$vardir) / sum(exp(fitted.values(FGV1))) De donde se obtiene que \\(\\Delta = 1.2484511\\). Final es posible obtener la varianza suavizada ejecutando el siguiente comando. hat.sigma &lt;- data.frame(dam2 = baseFGV$dam2, hat_var = delta.hat * exp(fitted.values(FGV1))) baseFGV &lt;- left_join(baseFGV, hat.sigma) tba(head(baseFGV, 10)) dam2 pobreza nd vardir ln_sigma2 hat_var 01001 0.2387 5305 0.0002 -8.3405 0.0001 01003 0.3220 572 0.0042 -5.4746 0.0036 01005 0.2653 1202 0.0016 -6.4142 0.0020 01006 0.2348 619 0.0014 -6.6051 0.0027 01007 0.2809 748 0.0009 -6.9716 0.0028 01011 0.2593 691 0.0036 -5.6290 0.0028 02001 0.1537 3058 0.0008 -7.1694 0.0007 02002 0.1987 4591 0.0005 -7.6960 0.0003 02003 0.1253 1139 0.0011 -6.7787 0.0015 02004 0.1764 4610 0.0003 -8.2937 0.0003 Validación del modelo para la FGV par(mfrow = c(2, 2)) plot(FGV1) Comparación entre la varianza estimada versus la pronosticada por la FGV ggplot(baseFGV , aes(y = vardir, x = hat_var)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(x = &quot;FGV&quot;, y = &quot;VarDirEst&quot;) + ylab(&quot;Varianza del Estimador Directo&quot;) Predicción de la varianza suavizada base_sae &lt;- base_sae %&gt;% left_join(hat.sigma, by = &quot;dam2&quot;) Ahora, realizamos un gráfico de linea para ver la volatilidad es la estimaciones de las varianzas. ggplot(base_sae %&gt;% arrange(nd), aes(x = 1:nrow(base_sae))) + geom_line(aes(y = vardir, color = &quot;VarDirEst&quot;)) + geom_line(aes(y = hat_var, color = &quot;FGV&quot;)) + labs(y = &quot;Varianzas&quot;, x = &quot;Tamaño muestral&quot;, color = &quot; &quot;) + scale_x_continuous(breaks = seq(1, nrow(base_sae), by = 10), labels = base_sae$nd[order(base_sae$nd)][seq(1, nrow(base_sae), by = 10)]) + scale_color_manual(values = c(&quot;FGV&quot; = &quot;Blue&quot;, &quot;VarDirEst&quot; = &quot;Red&quot;)) El siguiente código utiliza la función mutate() del paquete dplyr para crear nuevas variables de la base de datos base_sae y luego guarda el resultado en un archivo RDS llamado base_FH_2017.rds. En concreto, el código realiza las siguientes operaciones: La variable deff_dam2 se ajusta a 1 cuando es NaN. La variable deff_FGV se calcula a partir de otras dos variables hat_var y vardir. Si vardir es 0, entonces deff_FGV se ajusta a 1. En caso contrario, se divide hat_var por vardir / deff_dam2 para obtener deff_FGV. La variable deff_FGV se regulariza utilizando el criterio MDS: si deff_FGV es menor que 1, se ajusta a 1. Finalmente, se calcula la variable n_eff_FGV dividiendo nd (el tamaño de la muestra) por deff_FGV. base_FH &lt;- base_sae %&gt;% mutate( deff_dam2 = ifelse(is.nan(deff_dam2), 1, deff_dam2), deff_FGV = ifelse( vardir == 0 , 1, hat_var / (vardir / deff_dam2) ), # Criterio MDS para regularizar el DeffFGV deff_FGV = ifelse(deff_FGV &lt; 1, 1, deff_FGV), n_eff_FGV = nd / deff_FGV ) saveRDS(object = base_FH, &quot;Recursos/Día1/Sesion4/Data/base_FH_2020.rds&quot;) "],["día-2---sesión-1--modelo-de-fay-herriot---estimación-de-la-pobreza.html", "Capítulo 5 Día 2 - Sesión 1- Modelo de Fay Herriot - Estimación de la pobreza", " Capítulo 5 Día 2 - Sesión 1- Modelo de Fay Herriot - Estimación de la pobreza El modelo de Fay Herriot, propuesto por Fay y Herriot (1979), es un modelo estadístico de área y es el más comúnmente utilizado, cabe tener en cuenta, que dentro de la metodología de estimación en áreas pequeñas, los modelos de área son los de mayor aplicación, ya que lo más factible es no contar con la información a nivel de individuo, pero si encontrar no solo los datos a nivel de área, sino también información auxiliar asociada a estos datos. Este modelo lineal mixto, fue el primero en incluir efectos aleatorios a nivel de área, lo que implica que la mayoría de la información que se introduce al modelo corresponde a agregaciaciones usualmente, departamentos, regiones, provincias, municipios entre otros, donde las estimaciones que se logran con el modelo se obtienen sobre estas agregaciones o subpoblaciones. Ahora, el modelo Fay Herriot es un modelo que relaciona los indicadores de las áreas \\(\\theta_d\\), donde \\(d\\) varía de 1 a \\(D\\), asumiendo que varían con respecto a un vector de \\(p\\) covariables \\(\\boldsymbol{x}_d\\). El modelo se define mediante la ecuación \\(\\theta_d = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d\\), donde \\(u_d\\) es el término de error o efecto aleatorio, diferente para cada área y se distribuye como \\(u_{d} \\stackrel{ind}{\\sim}\\left(0,\\sigma_{u}^{2}\\right)\\). Sin embargo, los verdaderos valores de los indicadores \\(\\theta_d\\) no son observables. Entonces, se utiliza el estimador directo \\(\\hat{\\theta}^{DIR}_d\\) para estimarlos, lo que conduce a un error de muestreo. Este estimador todavía se considera insesgado bajo el diseño muestral, es decir, \\[ \\hat{\\theta}_d^{DIR} = \\theta + e_d \\] El modelo se ajusta entonces utilizando el término de error debido al muestreo \\(e_d\\), donde \\(e_{d} \\stackrel{ind}{\\sim} \\left(0,\\sigma^2_{e_d}\\right)\\) y las varianzas \\(\\sigma^2_{e_d}\\) se estiman utilizando los microdatos de la encuesta. El modelo FH se reescribe como \\[ \\hat{\\theta}^{DIR}_{d} = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d + e_d \\]. El mejor predictor lineal insesgado (BLUP) bajo el modelo FH viene dado por \\[ \\tilde{\\theta}_{d}^{FH} = \\boldsymbol{x}^{T}{d}\\tilde{\\boldsymbol{\\beta}}+\\tilde{u}_{d} \\], donde \\(\\tilde{u}_d = \\gamma_d\\left(\\hat{\\theta}^{DIR}_{d} - \\boldsymbol{x}^{T}_{d}\\tilde{\\boldsymbol{\\beta}} \\right)\\) y \\(\\gamma_d=\\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_{e_d}}\\). Modelo de área para la estimación de la pobreza Sea \\(P_d\\) la probabilidad de encontrar una persona en condición de pobreza en el \\(d-\\)ésimo dominio de la población. Entonces, el estimador directo de \\(P_d\\) se puede escribir como: \\[ \\hat{P}^{DIR}_{d} = P_d + e_d \\] Ahora bien, \\(P_d\\) se puede modelar de la siguiente manera, \\[ P_d = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d \\] Luego, reescribiendo \\(\\hat{P}^{DIR}_{d}\\) en términos de las dos ecuaciones anteriores tenemos: \\[ \\hat{P}^{DIR}_{d} = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d + e_d \\] Ahora, es posible suponer que \\(\\hat{P}^{DIR}_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta, \\sigma_u^2 +\\sigma_{e_d}^2)\\), \\(\\hat{P}^{DIR}_d \\mid u_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + u_d,\\sigma_{e_d}^2)\\) y \\(u_d \\sim N(0, \\sigma^2_u)\\) Luego, se asumen distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma^2_u\\) \\[ \\begin{eqnarray*} \\beta_p &amp; \\sim &amp; N(0, 10000)\\\\ \\sigma^2_u &amp;\\sim &amp; IG(0.0001, 0.0001) \\end{eqnarray*} \\] por tanto, el estimador bayesiano para \\(P_d\\) esta dado como \\(\\tilde{P}_d = E\\left(P_d\\mid\\hat{P}_d^{DIR}\\right)\\) Predictor óptimo de \\(P_d\\) El predictor óptimo de \\(P_d\\) es \\[E(P_d | \\hat{P}^{DIR}_d) = \\gamma_d\\hat{P}^{DIR}_d + (1-\\gamma_d)\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta\\] con \\(\\gamma_d = \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\). sabemos que \\(\\hat{P}^{DIR}_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta, \\sigma_u^2 +\\sigma_{e_d}^2)\\), \\(\\hat{P}^{DIR}_d \\mid u_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + u_d,\\sigma_{e_d}^2)\\) y \\(u_d \\sim N(0, \\sigma^2_u)\\) Por tanto \\[ \\begin{align*} f(u_d| \\hat{P}^{DIR}_d) \\propto f(\\hat{P}^{DIR}_d | u_d)f(u_d) &amp; = \\frac{1}{\\sigma^2_{e_d}\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{1}{2\\sigma^2_{e_d}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta - u_d)^2}\\right\\} \\frac{1}{\\sigma^2_u\\sqrt{2\\pi}}\\exp\\left\\{- \\frac{1}{2\\sigma^2_u}u_d^2\\right\\}\\\\ &amp; \\propto \\exp\\left\\{-\\frac{u_d^2 - 2u_d(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)}{2\\sigma^2_{e_d}} - \\frac{u_d^2}{2\\sigma^2_u}\\right\\} \\\\ &amp; = \\exp\\left\\{-\\frac{1}{2}\\left[(\\frac{1}{\\sigma^2_{e_d}} + \\frac{1}{\\sigma^2_u})u_d^2 - 2\\frac{\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta}{\\sigma_{e_d}^2}u_d\\right] \\right\\} \\\\ &amp; = \\exp \\left\\{ -\\frac{1}{2\\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}}\\left[u_d^2 - 2\\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)u_d \\right] \\right\\} \\\\ &amp; \\propto \\exp \\left\\{ -\\frac{1}{2\\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}}\\left[u_d - \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)\\right]^2 \\right\\} \\\\ &amp; \\propto N(E(u_d|\\hat{P}^{DIR}_d), \\text{Var}(u_d|P^{DIR})) \\end{align*} \\] con \\(E(u_d|\\hat{P}^{DIR}_d) = \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)\\) y \\(\\text{Var}(u_d|P^{DIR}) = \\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\). Por lo tanto se tiene, \\[ \\begin{align*} E(P_d | \\hat{P}^{DIR}_d) = \\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + E(u_d|\\hat{P}^{DIR}_d) &amp; = \\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta) \\\\ &amp; = \\frac{\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\hat{P}^{DIR}_d + \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta \\\\ &amp; = \\gamma_d\\hat{P}^{DIR}_d + (1-\\gamma_d)\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta \\end{align*} \\] "],["procedimiento-de-estimación.html", "5.1 Procedimiento de estimación", " 5.1 Procedimiento de estimación Este código utiliza las librerías tidyverse y magrittr para procesamiento y analizar datos. La función readRDS() es utilizada para cargar un archivo de datos en formato RDS, que contiene las estimaciones directas y la varianza suvizada para la proporción de personas en condición de pobreza correspondientes al año 2018. Luego, se utiliza el operador %&gt;% de la librería magrittr para encadenar la selección de las columnas de interés, que corresponden a los nombres dam2, nd, pobreza, vardir y hat_var. library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion1/Data/base_FH_2020.rds&quot;) %&gt;% select(dam2, nd, pobreza, vardir, hat_var) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion1/Data/predictors_satelital_dam2.rds&quot;) Ahora, se realiza una unión completa (full_join) entre el conjunto de datos base_FH y los predictores statelevel_predictors_df utilizando la variable dam2 como clave de unión. Se utiliza la función tba() para imprimir las primeras 10 filas y 8 columnas del conjunto de datos resultante de la unión anterior. La unión completa (full_join) combina los datos de ambos conjuntos, manteniendo todas las filas de ambos, y llenando con valores faltantes (NA) en caso de no encontrar coincidencias en la variable de unión (dam2 en este caso). La función tba() imprime una tabla en formato HTML en la consola de R que muestra las primeras 10 filas y 8 columnas del conjunto de datos resultante de la unión. base_FH &lt;- full_join(base_FH, statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[1:10,1:8]) dam2 nd pobreza vardir hat_var dam luces_nocturnas suelo_cultivo 01001 5305 0.2387 0.0002 0.0001 01 127.0742 105.4925 01003 572 0.3220 0.0042 0.0036 01 102.5703 100.2905 01005 1202 0.2653 0.0016 0.0020 01 107.6633 100.5877 01006 619 0.2348 0.0014 0.0027 01 101.2918 99.5395 01007 748 0.2809 0.0009 0.0028 01 102.6725 100.9469 01011 691 0.2593 0.0036 0.0028 01 102.5554 98.6332 02001 3058 0.1537 0.0008 0.0007 02 134.1608 111.9291 02002 4591 0.1987 0.0005 0.0003 02 177.6958 151.0492 02003 1139 0.1253 0.0011 0.0015 02 109.6961 97.8580 02004 4610 0.1764 0.0003 0.0003 02 140.2065 97.8905 # View(base_FH) "],["preparando-los-insumos-para-stan.html", "5.2 Preparando los insumos para STAN", " 5.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados. Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[1:10,1:8]) dam2 nd pobreza vardir hat_var dam luces_nocturnas suelo_cultivo 01002 NA NA NA NA 01 103.2847 101.3865 01004 NA NA NA NA 01 99.2927 98.8613 01008 NA NA NA NA 01 99.7144 100.2844 01009 NA NA NA NA 01 100.4092 99.1593 01010 NA NA NA NA 01 101.4046 102.5622 03009 NA NA NA NA 03 99.8661 98.1378 04005 NA NA NA NA 04 102.8203 101.5725 04007 NA NA NA NA 04 99.7119 102.8452 04008 NA NA NA NA 04 99.6681 99.9763 16100 NA NA NA NA 16 98.9878 98.7013 Definir matriz de efectos fijos. Define un modelo lineal utilizando la función formula(), que incluye varias variables predictoras, como la edad, la etnia, la tasa de desocupación, entre otras. Utiliza la función model.matrix() para generar matrices de diseño (Xdat y Xs) a partir de los datos observados (data_dir) y no observados (data_syn) para utilizar en la construcción de modelos de regresión. La función model.matrix() convierte las variables categóricas en variables binarias (dummy), de manera que puedan ser utilizadas. formula_mod &lt;- formula(~ sexo2 + anoest2 + anoest3 + anoest4 + edad2 + edad3 + edad4 + edad5 + etnia1 + etnia2 + tasa_desocupacion + suelo_urbano + suelo_cultivo + modificacion_humana + alfabeta ) ## Dominios observados Xdat &lt;- model.matrix(formula_mod, data = data_dir) ## Dominios no observados Xs &lt;- model.matrix(formula_mod, data = data_syn) dim(Xs) ## [1] 2199 16 dim(data_syn) ## [1] 2199 37 Ahora, se utiliza la función setdiff() para identificar las columnas de Xdat que no están presentes en \\(X_s\\), es decir, las variables que no se encuentran en los datos no observados. A continuación, se crea una matriz temporal (temp) con ceros para las columnas faltantes de \\(X_s\\), y se agregan estas columnas a \\(X_s\\) utilizando cbind(). El resultado final es una matriz Xs con las mismas variables que Xdat, lo que asegura que se puedan realizar comparaciones adecuadas entre los datos observados y no observados en la construcción de modelos de regresión. En general, este código es útil para preparar los datos para su posterior análisis y asegurar que los modelos de regresión sean adecuados para su uso. temp &lt;- setdiff(colnames(Xdat),colnames(Xs)) temp &lt;- matrix( 0, nrow = nrow(Xs), ncol = length(temp), dimnames = list(1:nrow(Xs), temp) ) Xs &lt;- cbind(Xs,temp)[,colnames(Xdat)] Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados y = as.numeric(data_dir$pobreza), # Estimación directa sigma_e = sqrt(data_dir$hat_var) # Error de estimación ) Rutina implementada en STAN data { int&lt;lower=0&gt; N1; // number of data items int&lt;lower=0&gt; N2; // number of data items for prediction int&lt;lower=0&gt; p; // number of predictors matrix[N1, p] X; // predictor matrix matrix[N2, p] Xs; // predictor matrix vector[N1] y; // predictor matrix vector[N1] sigma_e; // known variances } parameters { vector[p] beta; // coefficients for predictors real&lt;lower=0&gt; sigma2_u; vector[N1] u; } transformed parameters{ vector[N1] theta; vector[N1] thetaSyn; vector[N1] thetaFH; vector[N1] gammaj; real&lt;lower=0&gt; sigma_u; thetaSyn = X * beta; theta = thetaSyn + u; sigma_u = sqrt(sigma2_u); gammaj = to_vector(sigma_u ./ (sigma_u + sigma_e)); thetaFH = (gammaj) .* y + (1-gammaj).*thetaSyn; } model { // likelihood y ~ normal(theta, sigma_e); // priors beta ~ normal(0, 100); u ~ normal(0, sigma_u); sigma2_u ~ inv_gamma(0.0001, 0.0001); } generated quantities{ vector[N2] y_pred; for(j in 1:N2) { y_pred[j] = normal_rng(Xs[j] * beta, sigma_u); } } Compilando el modelo en STAN. A continuación mostramos la forma de compilar el código de STAN desde R. En este código se utiliza la librería rstan para ajustar un modelo bayesiano utilizando el archivo 17FH_normal.stan que contiene el modelo escrito en el lenguaje de modelado probabilístico Stan. En primer lugar, se utiliza la función stan() para ajustar el modelo a los datos de sample_data. Los argumentos que se pasan a stan() incluyen el archivo que contiene el modelo (fit_FH_normal), los datos (sample_data), y los argumentos para controlar el proceso de ajuste del modelo, como el número de iteraciones para el período de calentamiento (warmup) y el período de muestreo (iter), y el número de núcleos de la CPU para utilizar en el proceso de ajuste (cores). Además, se utiliza la función parallel::detectCores() para detectar automáticamente el número de núcleos disponibles en la CPU, y se establece la opción mc.cores para aprovechar el número máximo de núcleos disponibles para el ajuste del modelo. El resultado del ajuste del modelo es almacenado en model_FH_normal, que contiene una muestra de la distribución posterior del modelo, la cual puede ser utilizada para realizar inferencias sobre los parámetros del modelo y las predicciones. En general, este código es útil para ajustar modelos bayesianos utilizando Stan y realizar inferencias posteriores. library(rstan) fit_FH_normal &lt;- &quot;Recursos/Día2/Sesion1/Data/modelosStan/17FH_normal.stan&quot; options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_FH_normal &lt;- stan( file = fit_FH_normal, data = sample_data, verbose = FALSE, warmup = 2500, iter = 3000, cores = 4 ) saveRDS(object = model_FH_normal, file = &quot;Recursos/Día2/Sesion1/Data/model_FH_normal.rds&quot;) Leer el modelo model_FH_normal&lt;- readRDS(&quot;Recursos/Día2/Sesion1/Data/model_FH_normal.rds&quot;) 5.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(posterior) library(patchwork) y_pred_B &lt;- as.array(model_FH_normal, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion1/0Recursos/FH1.png&quot;, # scale = 2) p1 Análisis gráfico de la convergencia de las cadenas de \\(\\sigma^2_u\\). posterior_sigma2_u &lt;- as.array(model_FH_normal, pars = &quot;sigma2_u&quot;) p1 &lt;- (mcmc_dens_chains(posterior_sigma2_u) + mcmc_areas(posterior_sigma2_u) ) / mcmc_trace(posterior_sigma2_u) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion1/0Recursos/FH2.png&quot;, # scale = 2) p1 Para validar la convergencia de todas las cadenas se hace uso del R-hat. parametros &lt;- summary(model_FH_normal)$summary %&gt;% data.frame() parametros &lt;- parametros[grepl(pattern = &quot;^theta|beta|gamma&quot;, rownames(parametros)), ] p1 &lt;- mcmc_rhat(parametros$Rhat) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion1/0Recursos/rhat.png&quot;, # scale = 2) p1 Como método de validación se comparan las diferentes elementos de la estimación del modelo de FH obtenidos en STAN theta &lt;- summary(model_FH_normal,pars = &quot;theta&quot;)$summary %&gt;% data.frame() thetaSyn &lt;- summary(model_FH_normal,pars = &quot;thetaSyn&quot;)$summary %&gt;% data.frame() theta_FH &lt;- summary(model_FH_normal,pars = &quot;thetaFH&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate( thetadir = pobreza, theta_pred = theta$mean, thetaSyn = thetaSyn$mean, thetaFH = theta_FH$mean, theta_pred_EE = theta$sd, Cv_theta_pred = theta_pred_EE/theta_pred ) # Estimación predicción del modelo vs ecuación ponderada de FH p11 &lt;- ggplot(data_dir, aes(x = theta_pred, y = thetaFH)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) # Estimación con la ecuación ponderada de FH Vs estimación sintética p12 &lt;- ggplot(data_dir, aes(x = thetaSyn, y = thetaFH)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) # Estimación con la ecuación ponderada de FH Vs estimación directa p21 &lt;- ggplot(data_dir, aes(x = thetadir, y = thetaFH)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) # Estimación directa Vs estimación sintética p22 &lt;- ggplot(data_dir, aes(x = thetadir, y = thetaSyn)) + geom_point() + geom_abline(slope = 1,intercept = 0, colour = &quot;red&quot;) + theme_bw(10) p1 &lt;- (p11+p12)/(p21+p22) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion1/0Recursos/FH3.png&quot;, # scale = 2) p1 Estimación del FH de la pobreza en los dominios NO observados. theta_syn_pred &lt;- summary(model_FH_normal,pars = &quot;y_pred&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate( theta_pred = theta_syn_pred$mean, thetaSyn = theta_pred, thetaFH = theta_pred, theta_pred_EE = theta_syn_pred$sd, Cv_theta_pred = theta_pred_EE/theta_pred) tba(data_syn %&gt;% slice(1:10) %&gt;% select(dam2:hat_var,theta_pred:Cv_theta_pred)) dam2 nd pobreza vardir hat_var theta_pred thetaSyn thetaFH theta_pred_EE Cv_theta_pred 01002 NA NA NA NA 0.3584 0.3584 0.3584 0.0735 0.2052 01004 NA NA NA NA 0.3679 0.3679 0.3679 0.0746 0.2029 01008 NA NA NA NA 0.4922 0.4922 0.4922 0.0956 0.1943 01009 NA NA NA NA 0.4378 0.4378 0.4378 0.0757 0.1729 01010 NA NA NA NA 0.3509 0.3509 0.3509 0.0747 0.2130 03009 NA NA NA NA 0.1983 0.1983 0.1983 0.0770 0.3883 04005 NA NA NA NA 0.5228 0.5228 0.5228 0.0809 0.1547 04007 NA NA NA NA 0.3056 0.3056 0.3056 0.0807 0.2640 04008 NA NA NA NA 0.4999 0.4999 0.4999 0.0818 0.1636 16100 NA NA NA NA 0.5807 0.5807 0.5807 0.0794 0.1368 consolidando las bases de estimaciones para dominios observados y NO observados. estimacionesPre &lt;- bind_rows(data_dir, data_syn) %&gt;% select(dam2, theta_pred) %&gt;% mutate(dam = substr(dam2,1,2)) "],["proceso-de-benchmark.html", "5.3 Proceso de Benchmark", " 5.3 Proceso de Benchmark Del censo extraer el total de personas por DAM2 total_pp &lt;- readRDS(file = &quot;Recursos/Día2/Sesion1/Data/total_personas_dam2.rds&quot;) N_dam_pp &lt;- total_pp %&gt;% group_by(dam) %&gt;% mutate(dam_pp = sum(total_pp) ) tba(N_dam_pp %&gt;% data.frame() %&gt;% slice(1:10)) dam dam2 total_pp dam_pp 01 01001 945506 1421198 01 01002 51474 1421198 01 01003 58155 1421198 01 01004 16997 1421198 01 01005 129859 1421198 01 01006 47557 1421198 01 01007 57269 1421198 01 01008 9552 1421198 01 01009 22461 1421198 01 01010 20382 1421198 Obtener las estimaciones directa por DAM o el nivel de agregación en el cual la encuesta es representativa. En este código, se lee un archivo RDS de una encuesta (encuestaMEX20N1.rds) y se utilizan las funciones transmute() para seleccionar y transformar las variables de interés. En primer lugar, se crean las variables dam y dam2 que corresponde al identificador de la división administrativa mayor y menor de segundo nivel de la encuesta. Luego, se crea una variable wkx que corresponde al peso de la observación en la encuesta, y una variable upm que corresponde al identificador de la Unidad Prrimaria de Muestreo. Finalmente, se crea una variable pobreza que toma el valor 1 si el ingreso de la vivienda es menor que un umbral lp, y 0 en caso contrario. encuesta &lt;- readRDS(&quot;Recursos/Día2/Sesion1/Data/encuestaMEX20N1.rds&quot;)%&gt;% transmute( dam, dam2, wkx = `fep`, upm = `upm`, estrato , pobreza = ifelse(ingcorte &lt; lp, 1 , 0)) El código está realizando un análisis de datos de encuestas utilizando el paquete survey de R. Primero, se crea un objeto diseno de diseño de encuestas usando la función as_survey_design() del paquete srvyr, que incluye los identificadores de la unidad primaria de muestreo (upm), los pesos (wkx), las estratos (estrato) y los datos de la encuesta (encuesta). Posteriormente, se agrupa el objeto diseno por la variable “Agregado” y se calcula la media de la variable pobreza con un intervalo de confianza para toda la población utilizando la función survey_mean(). El resultado se guarda en el objeto directoDam y se muestra en una tabla. library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) diseno &lt;- as_survey_design( ids = upm, weights = wkx, strata = estrato, nest = TRUE, .data = encuesta ) directoDam &lt;- diseno %&gt;% group_by(dam) %&gt;% summarise( theta_dir = survey_mean(pobreza, vartype = c(&quot;ci&quot;)) ) tba(directoDam %&gt;% slice(1:10)) dam theta_dir theta_dir_low theta_dir_upp 01 0.2482 0.2247 0.2717 02 0.1778 0.1557 0.2000 03 0.1951 0.1703 0.2198 04 0.4216 0.3831 0.4602 05 0.2666 0.2471 0.2861 06 0.2145 0.1906 0.2384 07 0.6745 0.6357 0.7133 08 0.2488 0.2268 0.2707 09 0.2510 0.2231 0.2789 10 0.3722 0.3425 0.4019 Realizar el consolidando información obtenida en 1 y 2. temp &lt;- estimacionesPre %&gt;% inner_join(N_dam_pp ) %&gt;% inner_join(directoDam ) tba(temp %&gt;% slice(1:10)) dam2 theta_pred dam total_pp dam_pp theta_dir theta_dir_low theta_dir_upp 01001 0.2397 01 945506 1421198 0.2482 0.2247 0.2717 01003 0.3315 01 58155 1421198 0.2482 0.2247 0.2717 01005 0.2599 01 129859 1421198 0.2482 0.2247 0.2717 01006 0.2535 01 47557 1421198 0.2482 0.2247 0.2717 01007 0.3073 01 57269 1421198 0.2482 0.2247 0.2717 01011 0.2823 01 61986 1421198 0.2482 0.2247 0.2717 02001 0.1592 02 440624 3739196 0.1778 0.1557 0.2000 02002 0.1993 02 1042395 3739196 0.1778 0.1557 0.2000 02003 0.1595 02 102896 3739196 0.1778 0.1557 0.2000 02004 0.1787 02 1909967 3739196 0.1778 0.1557 0.2000 Con la información organizada realizar el calculo de los pesos para el Benchmark R_dam2 &lt;- temp %&gt;% group_by(dam) %&gt;% summarise( R_dam_RB = unique(theta_dir) / sum((total_pp / dam_pp) * theta_pred) ) %&gt;% left_join(directoDam, by = &quot;dam&quot;) tba(R_dam2 %&gt;% arrange(desc(R_dam_RB))) dam R_dam_RB theta_dir theta_dir_low theta_dir_upp 29 1.3136 0.5301 0.4985 0.5617 32 1.1529 0.4047 0.3749 0.4346 21 1.1515 0.5480 0.5124 0.5836 30 1.1083 0.5043 0.4633 0.5453 23 1.1016 0.4036 0.3724 0.4348 15 1.0970 0.4160 0.3851 0.4470 28 1.0957 0.3323 0.3056 0.3591 07 1.0811 0.6745 0.6357 0.7133 17 1.0802 0.4044 0.3706 0.4383 04 1.0734 0.4216 0.3831 0.4602 10 1.0572 0.3722 0.3425 0.4019 24 1.0476 0.3722 0.3371 0.4073 14 1.0359 0.2804 0.2493 0.3115 27 1.0326 0.4311 0.3957 0.4665 05 1.0026 0.2666 0.2471 0.2861 09 1.0026 0.2510 0.2231 0.2789 31 0.9993 0.4294 0.3961 0.4627 03 0.9780 0.1951 0.1703 0.2198 25 0.9771 0.2333 0.2068 0.2597 26 0.9698 0.2591 0.2272 0.2909 12 0.9532 0.5857 0.5480 0.6235 01 0.9451 0.2482 0.2247 0.2717 02 0.9411 0.1778 0.1557 0.2000 06 0.9404 0.2145 0.1906 0.2384 19 0.9378 0.2039 0.1803 0.2275 16 0.9313 0.3541 0.3155 0.3927 08 0.9266 0.2488 0.2268 0.2707 11 0.9116 0.3330 0.3079 0.3580 22 0.8992 0.2318 0.2093 0.2542 13 0.8899 0.3963 0.3550 0.4377 18 0.8640 0.2446 0.2148 0.2743 20 0.8218 0.5014 0.4637 0.5391 calculando los pesos para cada dominio. pesos &lt;- temp %&gt;% mutate(W_i = total_pp / dam_pp) %&gt;% select(dam2, W_i) tba(pesos %&gt;% slice(1:10)) dam2 W_i 01001 0.6653 01003 0.0409 01005 0.0914 01006 0.0335 01007 0.0403 01011 0.0436 02001 0.1178 02002 0.2788 02003 0.0275 02004 0.5108 Realizar la estimación FH Benchmark En este proceso, se realiza la adición de una nueva columna denominada R_dam_RB, que es obtenida a partir de un objeto denominado R_dam2. Posteriormente, se agrega una nueva columna denominada theta_pred_RBench, la cual es igual a la multiplicación de R_dam_RB y theta_pred. Finalmente, se hace un left_join con el dataframe pesos, y se seleccionan únicamente las columnas dam, dam2, W_i, theta_pred y theta_pred_RBench para ser presentadas en una tabla (tba) que muestra únicamente las primeras 10 filas. estimacionesBench &lt;- estimacionesPre %&gt;% left_join(R_dam2, by = c(&quot;dam&quot;)) %&gt;% mutate(theta_pred_RBench = R_dam_RB * theta_pred) %&gt;% left_join(pesos) %&gt;% select(dam, dam2, W_i, theta_pred, theta_pred_RBench) tba(estimacionesBench %&gt;% slice(1:10)) dam dam2 W_i theta_pred theta_pred_RBench 01 01001 0.6653 0.2397 0.2266 01 01003 0.0409 0.3315 0.3133 01 01005 0.0914 0.2599 0.2456 01 01006 0.0335 0.2535 0.2396 01 01007 0.0403 0.3073 0.2905 01 01011 0.0436 0.2823 0.2668 02 02001 0.1178 0.1592 0.1498 02 02002 0.2788 0.1993 0.1876 02 02003 0.0275 0.1595 0.1501 02 02004 0.5108 0.1787 0.1682 Validación: Estimación FH con Benchmark estimacionesBench %&gt;% group_by(dam) %&gt;% summarise(theta_reg_RB = sum(W_i * theta_pred_RBench)) %&gt;% left_join(directoDam, by = &quot;dam&quot;) %&gt;% tba() dam theta_reg_RB theta_dir theta_dir_low theta_dir_upp 01 0.2482 0.2482 0.2247 0.2717 02 0.1778 0.1778 0.1557 0.2000 03 0.1951 0.1951 0.1703 0.2198 04 0.4216 0.4216 0.3831 0.4602 05 0.2666 0.2666 0.2471 0.2861 06 0.2145 0.2145 0.1906 0.2384 07 0.6745 0.6745 0.6357 0.7133 08 0.2488 0.2488 0.2268 0.2707 09 0.2510 0.2510 0.2231 0.2789 10 0.3722 0.3722 0.3425 0.4019 11 0.3330 0.3330 0.3079 0.3580 12 0.5857 0.5857 0.5480 0.6235 13 0.3963 0.3963 0.3550 0.4377 14 0.2804 0.2804 0.2493 0.3115 15 0.4160 0.4160 0.3851 0.4470 16 0.3541 0.3541 0.3155 0.3927 17 0.4044 0.4044 0.3706 0.4383 18 0.2446 0.2446 0.2148 0.2743 19 0.2039 0.2039 0.1803 0.2275 20 0.5014 0.5014 0.4637 0.5391 21 0.5480 0.5480 0.5124 0.5836 22 0.2318 0.2318 0.2093 0.2542 23 0.4036 0.4036 0.3724 0.4348 24 0.3722 0.3722 0.3371 0.4073 25 0.2333 0.2333 0.2068 0.2597 26 0.2591 0.2591 0.2272 0.2909 27 0.4311 0.4311 0.3957 0.4665 28 0.3323 0.3323 0.3056 0.3591 29 0.5301 0.5301 0.4985 0.5617 30 0.5043 0.5043 0.4633 0.5453 31 0.4294 0.4294 0.3961 0.4627 32 0.4047 0.4047 0.3749 0.4346 "],["validación-de-los-resultados..html", "5.4 Validación de los resultados.", " 5.4 Validación de los resultados. Este código realiza un análisis de datos y visualización mediante el uso de la librería ggplot2. En particular, el código une dos data frames mediante la función left_join(), agrupa los datos por la variable dam y realiza algunas operaciones para transformar las variables thetaSyn\", thetaFH y theta_pred_RBench. Luego, utiliza la función gather() para organizar los datos en formato largo y los visualiza mediante ggplot(). La visualización resultante muestra puntos de diferentes formas y colores para representar los diferentes métodos de estimación, y dos líneas punteadas que representan los intervalos de confianza superior e inferior para los valores observados en la variable theta_dir. temp &lt;- estimacionesBench %&gt;% left_join( bind_rows( data_dir %&gt;% select(dam2, thetaSyn, thetaFH), data_syn %&gt;% select(dam2, thetaSyn, thetaFH))) %&gt;% group_by(dam) %&gt;% summarise(thetaSyn = sum(W_i * thetaSyn), thetaFH = sum(W_i * theta_pred), theta_RBench = sum(W_i * theta_pred_RBench) ) %&gt;% left_join(directoDam, by = &quot;dam&quot;) %&gt;% mutate(id = 1:n()) temp %&lt;&gt;% gather(key = &quot;Metodo&quot;,value = &quot;Estimacion&quot;, -id, -dam, -theta_dir_upp, -theta_dir_low) p1 &lt;- ggplot(data = temp, aes(x = id, y = Estimacion, shape = Metodo)) + geom_point(aes(color = Metodo), size = 2) + geom_line(aes(y = theta_dir_low), linetype = 2) + geom_line(aes(y = theta_dir_upp), linetype = 2) + theme_bw(20) + scale_x_continuous(breaks = temp$id, labels = temp$dam) + labs(y = &quot;&quot;, x = &quot;&quot;) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion1/0Recursos/validar_bench.png&quot;,width = 16,height = 12) p1 "],["mapa-de-pobreza.html", "5.5 Mapa de pobreza", " 5.5 Mapa de pobreza Este es un bloque de código se cargan varios paquetes (sp, sf, tmap) y realiza algunas operaciones. Primero, realiza una unión (left_join) entre las estimaciones de ajustadas por el Benchmarking (estimacionesBench) y las estimaciones del modelo (data_dir, data_syn), utilizando la variable dam2 como clave para la unión. Luego, lee un archivo Shapefile que contiene información geoespacial del país. A continuación, crea un mapa temático (tmap) utilizando la función tm_shape() y agregando capas con la función tm_polygons(). El mapa representa una variable theta_pred_RBench utilizando una paleta de colores llamada “YlOrRd” y establece los cortes de los intervalos de la variable con la variable brks_lp. Finalmente, la función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). library(sf) library(tmap) estimacionesBench %&lt;&gt;% left_join( bind_rows( data_dir %&gt;% select(dam2, theta_pred_EE , Cv_theta_pred), data_syn %&gt;% select(dam2, theta_pred_EE , Cv_theta_pred))) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Shape/MEX_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(estimacionesBench, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.1,0.15, 0.2, 0.3, 0.4, 0.6, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;theta_pred_RBench&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) tmap_save(Mapa_lp, filename = &quot;Recursos/Día2/Sesion1/0Recursos/Mapa_MEX_pobreza_normal.PNG&quot;, width = 2500, height = 2000, asp = 0) Mapa_lp "],["día-2---sesión-2--modelos-de-área---estimación-de-la-pobreza-y-la-transformación-arcoseno..html", "Capítulo 6 Día 2 - Sesión 2- Modelos de área - Estimación de la pobreza y la transformación ArcoSeno.", " Capítulo 6 Día 2 - Sesión 2- Modelos de área - Estimación de la pobreza y la transformación ArcoSeno. En su concepción más básica, el modelo de Fay-Herriot es una combinación lineal de covariables. Sin embargo, el resultado de esta combinación pueden tomar valores que se salen del rango aceptable en el que puede estar una proporción; es decir, en general el estimador de Fay-Herriot \\(\\theta \\in R\\), mientras que el estimador directo \\(\\theta \\in (0,1)\\). La transformación arcoseno esta dada por: \\[ \\hat{z}_d = arcsin\\left( \\sqrt{ \\hat{\\theta}_d} \\right) \\] donde \\[ Var\\left( \\hat{z}_d \\right) = \\frac{\\widehat{DEFF}_d}{4\\times n_d} = \\frac{1}{4\\times n_{d,efectivo} } \\] El modelo de Fay-Herriot estaría definido de la siguiente forma: \\[ \\begin{eqnarray*} Z_d \\mid \\mu_d,\\sigma^2_d &amp; \\sim &amp; N(\\mu_d, \\sigma^2_d)\\\\ \\mu_d &amp; = &amp; \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d \\\\ \\theta_d &amp; = &amp; \\left(sin(\\mu_d)\\right)^2 \\end{eqnarray*} \\] donde \\(u_d \\sim N(0 , \\sigma^2)\\). Suponga de las distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma_{u}^{2}\\) son dadas por \\[ \\begin{eqnarray*} \\boldsymbol{\\beta} \\sim N\\left(0,1000 \\right)\\\\ \\sigma_{u}^{2} \\sim IG\\left(0.0001,0.0001\\right) \\end{eqnarray*} \\] "],["procedimiento-de-estimación-1.html", "6.1 Procedimiento de estimación", " 6.1 Procedimiento de estimación Lectura de la base de datos que resultó en el paso anterior y selección de las columnas de interés library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion2/Data/base_FH_2020.rds&quot;) %&gt;% transmute(dam2, ## id dominios pobreza, T_pobreza = asin(sqrt(pobreza)), ## creando zd n_effec = n_eff_FGV, ## n efectivo varhat = 1/(4*n_effec) ## varianza para zd ) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion2/Data/predictors_satelital_dam2.rds&quot;) Uniendo las dos bases de datos. base_FH &lt;- full_join(base_FH, statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[,1:8] %&gt;% head(10)) dam2 pobreza T_pobreza n_effec varhat dam luces_nocturnas suelo_cultivo 01001 0.2387 0.5105 3058.5618 0.0001 01 127.0742 105.4925 01003 0.3220 0.6034 357.3147 0.0007 01 102.5703 100.2905 01005 0.2653 0.5411 318.9757 0.0008 01 107.6633 100.5877 01006 0.2348 0.5059 222.3658 0.0011 01 101.2918 99.5395 01007 0.2809 0.5585 272.1059 0.0009 01 102.6725 100.9469 01011 0.2593 0.5342 282.9142 0.0009 01 102.5554 98.6332 02001 0.1537 0.4028 478.2311 0.0005 02 134.1608 111.9291 02002 0.1987 0.4621 865.4328 0.0003 02 177.6958 151.0492 02003 0.1253 0.3618 176.1543 0.0014 02 109.6961 97.8580 02004 0.1764 0.4335 426.4288 0.0006 02 140.2065 97.8905 Seleccionando las covariables para el modelo. names_cov &lt;- c( &quot;sexo2&quot; , &quot;anoest2&quot; , &quot;anoest3&quot;, &quot;anoest4&quot;, &quot;edad2&quot; , &quot;edad3&quot; , &quot;edad4&quot; , &quot;edad5&quot; , &quot;etnia1&quot;, &quot;etnia2&quot; , &quot;tasa_desocupacion&quot; , &quot;suelo_urbano&quot; , &quot;suelo_cultivo&quot; , &quot;modificacion_humana&quot;, &quot;alfabeta&quot; ) "],["preparando-los-insumos-para-stan-1.html", "6.2 Preparando los insumos para STAN", " 6.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(T_pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[,1:8] %&gt;% slice(1:10)) dam2 pobreza T_pobreza n_effec varhat dam luces_nocturnas suelo_cultivo 01002 NA NA NA NA 01 103.2847 101.3865 01004 NA NA NA NA 01 99.2927 98.8613 01008 NA NA NA NA 01 99.7144 100.2844 01009 NA NA NA NA 01 100.4092 99.1593 01010 NA NA NA NA 01 101.4046 102.5622 03009 NA NA NA NA 03 99.8661 98.1378 04005 NA NA NA NA 04 102.8203 101.5725 04007 NA NA NA NA 04 99.7119 102.8452 04008 NA NA NA NA 04 99.6681 99.9763 16100 NA NA NA NA 16 98.9878 98.7013 Definir matriz de efectos fijos. ## Dominios observados Xdat &lt;- cbind(inter = 1,data_dir[,names_cov]) ## Dominios no observados Xs &lt;- cbind(inter = 1,data_syn[,names_cov]) Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados y = as.numeric(data_dir$T_pobreza), sigma_e = sqrt(data_dir$varhat) ) Compilando el modelo en STAN library(rstan) fit_FH_arcoseno &lt;- &quot;Recursos/Día2/Sesion2/Data/modelosStan/15FH_arcsin_normal.stan&quot; options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_FH_arcoseno &lt;- stan( file = fit_FH_arcoseno, data = sample_data, verbose = FALSE, warmup = 2500, iter = 3000, cores = 4 ) saveRDS(model_FH_arcoseno, &quot;Recursos/Día2/Sesion2/Data/model_FH_arcoseno.rds&quot;) model_FH_arcoseno &lt;- readRDS(&quot;Recursos/Día2/Sesion2/Data/model_FH_arcoseno.rds&quot;) 6.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(patchwork) library(posterior) y_pred_B &lt;- as.array(model_FH_arcoseno, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) ggsave(plot = p1, filename = &quot;Recursos/Día2/Sesion2/0Recursos/FH_Asin.png&quot;, scale = 2) p1 Análisis gráfico de la convergencia de las cadenas de \\(\\sigma^2_u\\). posterior_sigma2_u &lt;- as.array(model_FH_arcoseno, pars = &quot;sigma2_u&quot;) p1 &lt;- (mcmc_dens_chains(posterior_sigma2_u) + mcmc_areas(posterior_sigma2_u)) / mcmc_trace(posterior_sigma2_u) ggsave(plot = p1, filename = &quot;Recursos/Día2/Sesion2/0Recursos/FH_Asin2.png&quot;, scale = 2) p1 Estimación del FH de la pobreza en los dominios observados. theta_FH &lt;- summary(model_FH_arcoseno,pars = &quot;theta&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate(pred_arcoseno = theta_FH$mean, pred_arcoseno_EE = theta_FH$sd, Cv_pred = pred_arcoseno_EE/pred_arcoseno) Estimación del FH de la pobreza en los dominios NO observados. theta_FH_pred &lt;- summary(model_FH_arcoseno,pars = &quot;theta_pred&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate(pred_arcoseno = theta_FH_pred$mean, pred_arcoseno_EE = theta_FH_pred$sd, Cv_pred = pred_arcoseno_EE/pred_arcoseno) "],["mapa-de-pobreza-1.html", "6.3 Mapa de pobreza", " 6.3 Mapa de pobreza El siguiente bloque de código carga los paquetes sp, sf y tmap, y realiza algunas operaciones. Primero, une (rbind) las estimaciones de los dominios observados y los no observados (data_dir, data_syn) y selecciona las variables dam2, pobreza, pred_arcoseno, pred_arcoseno_EE y Cv_pred utilizando la función select(). Luego, lee un archivo Shapefile que contiene información geoespacial del país. A continuación, crea un mapa temático (tmap) utilizando la función tm_shape() y agregando capas con la función tm_polygons(). El mapa representa dos variables llamadas pobreza y pred_arcoseno, utilizando una paleta de colores llamada “YlOrRd” y establece los cortes de los intervalos de las variables con la variable brks_lp. Finalmente, la función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). library(sf) library(tmap) data_map &lt;- rbind(data_dir, data_syn) %&gt;% select(dam2, pobreza, pred_arcoseno, pred_arcoseno_EE,Cv_pred ) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Shape/MEX_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(data_map, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.15, 0.3, 0.45, 0.6, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;pobreza&quot;, &quot;pred_arcoseno&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) tmap_save( Mapa_lp, filename = &quot;Recursos/Día2/Sesion2/0Recursos/Mapa_arcoseno.PNG&quot;, width = 2500, height = 2000, asp = 0 ) Mapa_lp "],["mapa-del-coeficiente-de-variación..html", "6.4 Mapa del coeficiente de variación.", " 6.4 Mapa del coeficiente de variación. Ahora, se crea un segundo mapa temático (tmap) llamado Mapa_cv. Utiliza la misma estructura del primer mapa (mapa) creado anteriormente y agrega una capa utilizando la función tm_polygons(). El mapa representa la variable Cv_pred, utilizando una paleta de colores llamada “YlOrRd” y establece el título del mapa con el parámetro title. La función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). Finalmente, el mapa Mapa_cv se muestra en la consola de R. Mapa_cv &lt;- mapa + tm_polygons( c(&quot;Cv_pred&quot;), title = &quot;Mapa de pobreza(cv)&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 2.5) tmap_save( Mapa_cv, filename = &quot;Recursos/Día2/Sesion2/0Recursos/Mapa_arcoseno_cv.PNG&quot;, width = 2500, height = 2000, asp = 0 ) Mapa_cv NOTA: Dado que la estimación del modelo y el error de estimación son pequeño, entonces, el coeficiente de variación no es una buena medida de la calidad de la estimación. "],["día-2---sesión-3--modelos-de-área---estimación-de-la-pobreza-en-familia-beta-y-binomial.html", "Capítulo 7 Día 2 - Sesión 3- Modelos de área - Estimación de la pobreza en familia beta y binomial ", " Capítulo 7 Día 2 - Sesión 3- Modelos de área - Estimación de la pobreza en familia beta y binomial "],["modelos-de-área-con-variable-respuesta-beta..html", "7.1 Modelos de área con variable respuesta Beta.", " 7.1 Modelos de área con variable respuesta Beta. El modelo beta-logístico fue inicialmente considerado por Jiang y Lahiri (2006b) para un enfoque EBP en uno de sus ejemplos ilustrativos para estimar medias de dominio de población finita. El modelo Fay Herriot beta-logístico estaría dado por las siguientes expresiones \\[ \\begin{eqnarray*} \\hat{P}_{d} \\mid P_d &amp; \\sim &amp; beta(a_d, b_d)\\\\ \\end{eqnarray*} \\] La función del enlace es \\[ \\begin{eqnarray*} logit(P_{d}) \\mid \\boldsymbol{\\beta}, \\sigma^2_u &amp; \\sim &amp; N(\\boldsymbol{x}_d^T\\boldsymbol{\\beta},\\sigma^2_u)\\\\ \\end{eqnarray*} \\] Los parámetros \\(a_d\\) y \\(b_d\\) son estimados así: \\[ \\begin{eqnarray*} a_d &amp;=&amp; P_d \\times \\phi_d\\\\ b_d &amp;=&amp; (1 - P_d) \\times \\phi_d\\\\ \\end{eqnarray*} \\] donde \\[\\phi_d = \\frac{n_d}{\\widehat{DEFF}_d} -1 = n_{d,efecctivo} -1\\] #### Nota {#Nota} La distribución Beta tiene la siguiente forma: \\[ \\begin{equation} p(\\theta \\mid a, b)= \\frac{1}{Beta(a,b)}\\theta^{a-1}(1-\\theta)^{b-1}I_{[0,1]}(\\theta). \\end{equation} \\] donde \\(E(\\theta) = \\frac{a}{a+b}\\) y \\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)}\\) Las distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma^2_u\\) \\[ \\begin{eqnarray*} \\beta_k &amp;\\sim&amp; N(0, 10000)\\\\ \\sigma^2_u &amp;\\sim&amp; IG(0.0001,0.0001) \\end{eqnarray*} \\] 7.1.1 Procedimiento de estimación Lectura de la base de datos que resultó en el paso anterior y selección de las columnas de interés library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/base_FH_2020.rds&quot;) %&gt;% select(dam2, pobreza, n_eff_FGV) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/predictors_satelital_dam2.rds&quot;) Uniendo las dos bases de datos. base_FH &lt;- full_join(base_FH,statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[,1:8] %&gt;% head(10)) dam2 pobreza n_eff_FGV dam luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana 01001 0.2387 3058.5618 01 127.0742 105.4925 128.6893 106.7313 01003 0.3220 357.3147 01 102.5703 100.2905 100.6374 101.8864 01005 0.2653 318.9757 01 107.6633 100.5877 104.8903 100.5953 01006 0.2348 222.3658 01 101.2918 99.5395 99.7443 98.6083 01007 0.2809 272.1059 01 102.6725 100.9469 100.5439 99.5256 01011 0.2593 282.9142 01 102.5554 98.6332 100.3694 98.0801 02001 0.1537 478.2311 02 134.1608 111.9291 137.4761 128.0117 02002 0.1987 865.4328 02 177.6958 151.0492 166.6009 137.4847 02003 0.1253 176.1543 02 109.6961 97.8580 106.8797 104.3917 02004 0.1764 426.4288 02 140.2065 97.8905 167.4086 107.1455 Seleccionando las covariables para el modelo. names_cov &lt;- c( &quot;sexo2&quot; , &quot;anoest2&quot; , &quot;anoest3&quot;, &quot;anoest4&quot;, &quot;edad2&quot; , &quot;edad3&quot; , &quot;edad4&quot; , &quot;edad5&quot; , &quot;etnia1&quot;, &quot;etnia2&quot; , &quot;tasa_desocupacion&quot; , &quot;suelo_urbano&quot; , &quot;suelo_cultivo&quot; , &quot;modificacion_humana&quot;, &quot;alfabeta&quot; ) 7.1.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[,1:8] %&gt;% slice(1:10)) dam2 pobreza n_eff_FGV dam luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana 01002 NA NA 01 103.2847 101.3865 101.7692 100.7418 01004 NA NA 01 99.2927 98.8613 98.8818 97.8499 01008 NA NA 01 99.7144 100.2844 98.6047 99.5866 01009 NA NA 01 100.4092 99.1593 99.8478 98.6020 01010 NA NA 01 101.4046 102.5622 99.7574 99.7674 03009 NA NA 03 99.8661 98.1378 100.0069 101.0845 04005 NA NA 04 102.8203 101.5725 99.8653 103.0741 04007 NA NA 04 99.7119 102.8452 98.2200 108.2876 04008 NA NA 04 99.6681 99.9763 98.6357 99.8922 16100 NA NA 16 98.9878 98.7013 98.4484 97.8097 Definir matriz de efectos fijos. ## Dominios observados Xdat &lt;- data_dir[,names_cov] ## Dominios no observados Xs &lt;- data_syn[,names_cov] Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados y = as.numeric(data_dir$pobreza), phi = data_dir$n_eff_FGV - 1 ) Compilando el modelo en STAN library(rstan) fit_FH_beta_logitic &lt;- &quot;Recursos/Día2/Sesion3/Data/modelosStan/16FH_beta_logitc.stan&quot; rstan::rstan_options(auto_write = TRUE) # speed up running time options(mc.cores = parallel::detectCores()) model_FH_beta_logitic &lt;- stan( file = fit_FH_beta_logitic, data = sample_data, verbose = FALSE, warmup = 2500, iter = 3000, cores = 4 ) saveRDS(model_FH_beta_logitic, file = &quot;Recursos/Día2/Sesion3/Data/model_FH_beta_logitic.rds&quot;) model_FH_beta_logitic &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/model_FH_beta_logitic.rds&quot;) 7.1.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(patchwork) library(posterior) y_pred_B &lt;- as.array(model_FH_beta_logitic, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) ggsave(plot = p1, filename = &quot;Recursos/Día2/Sesion3/0Recursos/Beta1.PNG&quot;, scale = 2) p1 Análisis gráfico de la convergencia de las cadenas de \\(\\sigma^2_u\\). posterior_sigma2_u &lt;- as.array(model_FH_beta_logitic, pars = &quot;sigma2_u&quot;) p1 &lt;- (mcmc_dens_chains(posterior_sigma2_u) + mcmc_areas(posterior_sigma2_u) ) / mcmc_trace(posterior_sigma2_u) ggsave(plot = p1, filename = &quot;Recursos/Día2/Sesion3/0Recursos/Beta2.PNG&quot;, scale = 2) p1 Estimación del FH de la pobreza en los dominios observados. theta_FH &lt;- summary(model_FH_beta_logitic,pars = &quot;theta&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate(pred_beta_logit = theta_FH$mean, pred_beta_logit_EE = theta_FH$sd, Cv_pred = pred_beta_logit_EE/pred_beta_logit) Estimación del FH de la pobreza en los dominios NO observados. theta_FH_pred &lt;- summary(model_FH_beta_logitic,pars = &quot;thetapred&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate(pred_beta_logit = theta_FH_pred$mean, pred_beta_logit_EE = theta_FH_pred$sd, Cv_pred = pred_beta_logit_EE/pred_beta_logit) 7.1.2.2 Mapa de pobreza El mapa muestra el nivel de pobreza en diferentes áreas de Colombia, basado en dos variables, pobreza y pred_beta_logit. Primero, se cargan los paquetes necesarios sp, sf y tmap. Luego, se lee la información de los datos en R y se combinan utilizando la función rbind(). library(sf) library(tmap) data_map &lt;- rbind(data_dir, data_syn) %&gt;% select(dam2, pobreza, pred_beta_logit, pred_beta_logit_EE,Cv_pred ) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Shape/MEX_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(data_map, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.15, 0.3, 0.45, 0.6, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;pobreza&quot;, &quot;pred_beta_logit&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) tmap_save( Mapa_lp, filename = &quot;Recursos/Día2/Sesion3/0Recursos/Beta.PNG&quot;, width = 2500, height = 2000, asp = 0 ) Mapa_lp 7.1.2.3 Mapa del coeficiente de variación. Ahora, se crea un segundo mapa temático (tmap) llamado Mapa_cv. Utiliza la misma estructura del primer mapa (mapa) creado anteriormente y agrega una capa utilizando la función tm_polygons(). El mapa representa la variable Cv_pred, utilizando una paleta de colores llamada “YlOrRd” y establece el título del mapa con el parámetro title. La función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). Finalmente, el mapa Mapa_cv se muestra en la consola de R. Mapa_cv &lt;- mapa + tm_polygons( c(&quot;Cv_pred&quot;), title = &quot;Mapa de pobreza(cv)&quot;, palette = &quot;Greens&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 2.5) tmap_save( Mapa_cv, filename = &quot;Recursos/Día2/Sesion3/0Recursos/Beta_cv.PNG&quot;, width = 2500, height = 2000, asp = 0 ) Mapa_cv NOTA: Dado que la estimación del modelo y el error de estimación son pequeño, entonces, el coeficiente de variación no es una buena medida de la calidad de la estimación. "],["modelos-de-área-con-variable-respuesta-binomial..html", "7.2 Modelos de área con variable respuesta Binomial.", " 7.2 Modelos de área con variable respuesta Binomial. El modelo lineal de Fay-Herriot puede ser reemplazado por un modelo mixto lineal generalizado (GLMM). Esto se puede hacer cuando los datos observados \\(Y_d\\) son inherentemente discretos, como cuando son recuentos (no ponderados) de personas u hogares muestreados con ciertas características. Uno de estos modelos supone una distribución binomial para \\(Y_d\\) con probabilidad de éxito \\(\\theta_d\\), y una logística modelo de regresión para \\(\\theta_d\\) con errores normales en la escala logit. El modelo resultante es \\[ \\begin{eqnarray*} Y_{d}\\mid \\theta_{d},n_{d} &amp; \\sim &amp; Bin\\left(n_{d},\\theta_{d}\\right) \\end{eqnarray*} \\] para \\(d=1,\\dots,D\\) y \\[ \\begin{eqnarray*} logit\\left(\\theta_{d}\\right)=\\log\\left(\\frac{\\theta_{d}}{1-\\theta_{d}}\\right) &amp; = &amp; \\boldsymbol{x}_{d}^{T}\\boldsymbol{\\beta}+u_{d} \\end{eqnarray*} \\] donde \\(u_{d}\\sim N\\left(0,\\sigma_{u}^{2}\\right)\\) y \\(n_{d}\\) es el tamaño de la muestra para el área \\(d\\). El modelo anterior se puede aplicar fácilmente a recuentos de muestras no ponderadas \\(Y_d\\), pero esto ignora cualquier aspecto complejo del diseño de la encuesta. En muestras complejas donde las \\(Y_d\\) son estimaciones ponderadas, surgen dos problemas. En primer lugar, los posibles valores de el \\(Y_d\\) no serán los números enteros \\(0, 1, \\dots , n_d\\) para cualquier definición directa de tamaño de muestra \\(n_d\\). En su lugar, \\(Y_d\\) tomará un valor de un conjunto finito de números desigualmente espaciados determinados por las ponderaciones de la encuesta que se aplican a los casos de muestra en el dominio \\(d\\). En segundo lugar, la varianza muestral de \\(Y_d\\) implícito en la distribución Binomial, es decir, \\(n_d \\times \\theta_d (1-\\theta_d)\\), será incorrecto. Abordamos estos dos problemas al definir un tamaño de muestra efectivo \\(\\tilde{n}_d\\), y un número de muestra efectivo de éxitos \\(\\tilde{Y_d}\\) determinó mantener: (i) la estimación directa \\(\\hat{\\theta}_i\\), de la pobreza y (ii) una estimación de la varianza de muestreo correspondiente,\\(\\widehat{Var}(\\hat{\\theta}_d)\\). Es posible suponer que \\[ \\begin{eqnarray*} \\tilde{n}_{d} &amp; \\sim &amp; \\frac{\\check{\\theta}_{d}\\left(1-\\check{\\theta}_{d}\\right)}{\\widehat{Var}\\left(\\hat{\\theta}_{d}\\right)} \\end{eqnarray*} \\] donde \\(\\check{\\theta}_{d}\\) es una preliminar predicción basada en el modelo para la proporción poblacional \\(\\theta_d\\) y \\(\\widehat{Var}\\left(\\hat{\\theta}_{d}\\right)\\) depende de\\(\\check{\\theta}_{d}\\) a través de una función de varianza generalizada ajustada (FGV). Note que \\(\\tilde{Y}_{d}=\\tilde{n}_{d}\\times\\hat{\\theta}_{d}\\). Suponga de las distribuciones previas para \\(\\boldsymbol{\\beta}\\) y \\(\\sigma_{u}^{2}\\) son dadas por \\[ \\begin{eqnarray*} \\boldsymbol{\\beta} \\sim N\\left(0,10000\\right)\\\\ \\sigma_{u}^{2} \\sim IG\\left(0.0001,0.0001\\right) \\end{eqnarray*} \\] 7.2.1 Procedimiento de estimación Lectura de la base de datos que resultó en el paso anterior y selección de las columnas de interés library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/base_FH_2020.rds&quot;) %&gt;% select(dam2, pobreza, n_eff_FGV) Lectura de las covariables, las cuales son obtenidas previamente. Dado la diferencia entre las escalas de las variables es necesario hacer un ajuste a estas. statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/predictors_satelital_dam2.rds&quot;) Uniendo las dos bases de datos. base_FH &lt;- full_join(base_FH,statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[,1:8] %&gt;% head(10)) dam2 pobreza n_eff_FGV dam luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana 01001 0.2387 3058.5618 01 127.0742 105.4925 128.6893 106.7313 01003 0.3220 357.3147 01 102.5703 100.2905 100.6374 101.8864 01005 0.2653 318.9757 01 107.6633 100.5877 104.8903 100.5953 01006 0.2348 222.3658 01 101.2918 99.5395 99.7443 98.6083 01007 0.2809 272.1059 01 102.6725 100.9469 100.5439 99.5256 01011 0.2593 282.9142 01 102.5554 98.6332 100.3694 98.0801 02001 0.1537 478.2311 02 134.1608 111.9291 137.4761 128.0117 02002 0.1987 865.4328 02 177.6958 151.0492 166.6009 137.4847 02003 0.1253 176.1543 02 109.6961 97.8580 106.8797 104.3917 02004 0.1764 426.4288 02 140.2065 97.8905 167.4086 107.1455 Seleccionando las covariables para el modelo. names_cov &lt;- c( &quot;sexo2&quot; , &quot;anoest2&quot; , &quot;anoest3&quot;, &quot;anoest4&quot;, &quot;edad2&quot; , &quot;edad3&quot; , &quot;edad4&quot; , &quot;edad5&quot; , &quot;etnia1&quot;, &quot;etnia2&quot; , &quot;tasa_desocupacion&quot; , &quot;suelo_urbano&quot; , &quot;suelo_cultivo&quot; , &quot;modificacion_humana&quot;, &quot;alfabeta&quot; ) 7.2.2 Preparando los insumos para STAN Dividir la base de datos en dominios observados y no observados Dominios observados. data_dir &lt;- base_FH %&gt;% filter(!is.na(pobreza)) Dominios NO observados. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[1:10,1:8]) dam2 pobreza n_eff_FGV dam luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana 01002 NA NA 01 103.2847 101.3865 101.7692 100.7418 01004 NA NA 01 99.2927 98.8613 98.8818 97.8499 01008 NA NA 01 99.7144 100.2844 98.6047 99.5866 01009 NA NA 01 100.4092 99.1593 99.8478 98.6020 01010 NA NA 01 101.4046 102.5622 99.7574 99.7674 03009 NA NA 03 99.8661 98.1378 100.0069 101.0845 04005 NA NA 04 102.8203 101.5725 99.8653 103.0741 04007 NA NA 04 99.7119 102.8452 98.2200 108.2876 04008 NA NA 04 99.6681 99.9763 98.6357 99.8922 16100 NA NA 16 98.9878 98.7013 98.4484 97.8097 Definir matriz de efectos fijos. ## Dominios observados Xdat &lt;- data_dir[,names_cov] ## Dominios no observados Xs &lt;- data_syn[,names_cov] Obteniendo el tamaño de muestra efectivo \\(\\tilde{n}_d\\), y el número de muestra efectivo de éxitos \\(\\tilde{Y_d}\\) n_effec = ceiling(data_dir$n_eff_FGV) y_effect = ceiling((data_dir$pobreza)*n_effec) Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observados. N2 = nrow(Xs), # NO Observados. p = ncol(Xdat), # Número de regresores. X = as.matrix(Xdat), # Covariables Observados. Xs = as.matrix(Xs), # Covariables NO Observados n_effec = n_effec, y_effect = y_effect # Estimación directa. ) Compilando el modelo en STAN library(rstan) fit_FH_binomial &lt;- &quot;Recursos/Día2/Sesion3/Data/modelosStan/14FH_binomial.stan&quot; rstan::rstan_options(auto_write = TRUE) # speed up running time options(mc.cores = parallel::detectCores()) model_FH_Binomial &lt;- stan( file = fit_FH_binomial, data = sample_data, verbose = FALSE, warmup = 10000, iter = 11000, cores = 4 ) saveRDS(model_FH_Binomial, file = &quot;Recursos/Día2/Sesion3/Data/model_FH_Binomial.rds&quot;) Leer el modelo model_FH_Binomial &lt;- readRDS(&quot;Recursos/Día2/Sesion3/Data/model_FH_Binomial.rds&quot;) 7.2.2.1 Resultados del modelo para los dominios observados. En este código, se cargan las librerías bayesplot, posterior y patchwork, que se utilizan para realizar gráficos y visualizaciones de los resultados del modelo. A continuación, se utiliza la función as.array() y as_draws_matrix() para extraer las muestras de la distribución posterior del parámetro theta del modelo, y se seleccionan aleatoriamente 100 filas de estas muestras utilizando la función sample(), lo que resulta en la matriz y_pred2. Finalmente, se utiliza la función ppc_dens_overlay() de bayesplot para graficar una comparación entre la distribución empírica de la variable observada pobreza en los datos (data_dir$pobreza) y las distribuciones predictivas posteriores simuladas para la misma variable (y_pred2). La función ppc_dens_overlay() produce un gráfico de densidad para ambas distribuciones, lo que permite visualizar cómo se comparan. library(bayesplot) library(patchwork) library(posterior) y_pred_B &lt;- as.array(model_FH_Binomial, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(c(1:2000,3001:4000), 200) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion3/0Recursos/Binomial1.PNG&quot;, # scale = 2) p1 Análisis gráfico de la convergencia de las cadenas de \\(\\sigma_u\\). posterior_sigma_u &lt;- as.array(model_FH_Binomial, pars = &quot;sigma_u&quot;) p1 &lt;- (mcmc_dens_chains(posterior_sigma_u) + mcmc_areas(posterior_sigma_u) ) / mcmc_trace(posterior_sigma_u) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion3/0Recursos/Binomial2.PNG&quot;, # scale = 2) p1 Estimación del FH de la pobreza en los dominios observados. theta_FH &lt;- summary(model_FH_Binomial,pars = &quot;theta&quot;)$summary %&gt;% data.frame() data_dir %&lt;&gt;% mutate(pred_binomial = theta_FH$mean, pred_binomial_EE = theta_FH$sd, Cv_pred = pred_binomial_EE/pred_binomial) Estimación del FH de la pobreza en los dominios NO observados. theta_FH_pred &lt;- summary(model_FH_Binomial,pars = &quot;thetaLP&quot;)$summary %&gt;% data.frame() data_syn &lt;- data_syn %&gt;% mutate(pred_binomial = theta_FH_pred$mean, pred_binomial_EE = theta_FH_pred$sd, Cv_pred = pred_binomial_EE/pred_binomial) 7.2.2.2 Mapa de pobreza El mapa muestra el nivel de pobreza en diferentes áreas de Colombia, basado en dos variables, pobreza y pred_binomial. Primero, se cargan los paquetes necesarios sp, sf y tmap. Luego, se lee la información de los datos en R y se combinan utilizando la función rbind(). library(sf) library(tmap) data_map &lt;- rbind(data_dir, data_syn) %&gt;% select(dam2, pobreza, pred_binomial, pred_binomial_EE,Cv_pred ) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Recursos/Día2/Sesion3/Shape/MEX_dam2.shp&quot;) mapa &lt;- tm_shape(ShapeSAE %&gt;% left_join(data_map, by = &quot;dam2&quot;)) brks_lp &lt;- c(0,0.025,0.05, 0.1, 0.15, 0.2,0.4, 1) tmap_options(check.and.fix = TRUE) Mapa_lp &lt;- mapa + tm_polygons( c(&quot;pobreza&quot;, &quot;pred_binomial&quot;), breaks = brks_lp, title = &quot;Mapa de pobreza&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 1.5) tmap_save(Mapa_lp, filename = &quot;Recursos/Día2/Sesion3/0Recursos/Binomial.PNG&quot;, width = 2500, height = 2000, asp = 0) Mapa_lp 7.2.2.3 Mapa del coeficiente de variación. Ahora, se crea un segundo mapa temático (tmap) llamado Mapa_cv. Utiliza la misma estructura del primer mapa (mapa) creado anteriormente y agrega una capa utilizando la función tm_polygons(). El mapa representa la variable Cv_pred, utilizando una paleta de colores llamada “YlOrRd” y establece el título del mapa con el parámetro title. La función tm_layout() establece algunos parámetros de diseño del mapa, como la relación de aspecto (asp). Finalmente, el mapa Mapa_cv se muestra en la consola de R. Mapa_cv &lt;- mapa + tm_polygons( c(&quot;Cv_pred&quot;), title = &quot;Mapa de pobreza(cv)&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tm_layout(asp = 2.5) tmap_save(Mapa_cv, filename = &quot;Recursos/Día2/Sesion3/0Recursos/Binomial_cv.PNG&quot;, width = 2500, height = 2000, asp = 0) Mapa_cv NOTA: Dado que la estimación del modelo y el error de estimación son pequeño, entonces, el coeficiente de variación no es una buena medida de la calidad de la estimación. "],["día-2---sesión-4--modelo-de-unidad-para-la-estimación-del-ingreso-medio.html", "Capítulo 8 Día 2 - Sesión 4- Modelo de unidad para la estimación del ingreso medio", " Capítulo 8 Día 2 - Sesión 4- Modelo de unidad para la estimación del ingreso medio Uno de los primeros problemas a los que debemos enfrentarnos es la estimación del ingreso medio, la cual en una variable no simétrica que toma valores en los positivos. Sin embargo, empleando los métodos Bayesiano es posible obtener estimaciones de esta sin realizar una transformación Figura 8.1: Distribución del ingreso medio por dam2 Obejtivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y}_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) es el ingreso de cada personas Note que, \\[ \\begin{equation*} \\bar{Y}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(\\bar{Y}\\) esta dado por: \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{\\bar{Y}}_d = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] "],["modelo-bayesiano..html", "8.1 Modelo bayesiano.", " 8.1 Modelo bayesiano. Para realizar la predicción del ingreso medio en dam2 no observadas se asume que: \\[ \\begin{eqnarray*} Y_{di} &amp;\\sim &amp; N\\left(\\mu_{di},\\sigma_e^{2}\\right)\\\\ \\mu_{di}&amp;=&amp;\\boldsymbol{x}_{di}^{T}\\boldsymbol{\\beta}+u_{d}+e_{di} \\end{eqnarray*} \\] Donde \\(Y_{di}\\) representa el ingreso medio de la \\(i-ésima\\) persona en el \\(d-ésimo\\) domino, \\(\\boldsymbol{X}\\) es la información disponible para la \\(i-ésima\\) persona del \\(d-ésimo\\) domino, \\(\\boldsymbol{\\beta}\\) es el vector de parámetros \\(u_d\\) es el efecto introducido por el \\(d-ésimo\\) dominio y \\(e_{di}\\) es el error de estimación para la \\(i-ésima\\) personas del \\(d-ésimo\\) dominio. Note, que \\(u_{d}\\sim N\\left(0,\\sigma^2_{u}\\right)\\) y \\(e_{di}\\sim N\\left(0,\\sigma_{e}^{2}\\right)\\). Para este caso se asumen las distribuciones previas \\[ \\begin{eqnarray*} \\beta_k &amp; \\sim &amp; N(0, 1000)\\\\ \\sigma^2_y &amp;\\sim &amp; IG(0.0001,0.0001) \\end{eqnarray*} \\] las cuales se toman no informativas. A continuación se muestra el proceso realizado para la obtención de la predicción del ingreso medio en dominios no observados. "],["proceso-de-estimación-en-r.html", "8.2 Proceso de estimación en R", " 8.2 Proceso de estimación en R Para desarrollar la metodología se hace uso de las siguientes librerías. # Interprete de STAN en R library(rstan) library(rstanarm) # Manejo de bases de datos. library(tidyverse) # Gráficas de los modelos. library(bayesplot) library(patchwork) # Organizar la presentación de las tablas library(kableExtra) library(printr) Un conjunto de funciones desarrolladas para realizar de forma simplificada los procesos están consignadas en la siguiente rutina. source(&quot;Recursos/Día2/Sesion4/0Recursos/funciones_mrp.R&quot;) Entre las funciones incluidas en el archivo encuentra plot_interaction: Esta crea un diagrama de lineas donde se estudia la interacción entre las variables, en el caso de presentar un traslape de las lineas se recomienda incluir el interacción en el modelo. Plot_Compare Puesto que es necesario realizar una homologar la información del censo y la encuesta es conveniente llevar a cabo una validación de las variables que han sido homologadas, por tanto, se espera que las proporciones resultantes del censo y la encuesta estén cercanas entre sí. Aux_Agregado: Esta es función permite obtener estimaciones a diferentes niveles de agregación, toma mucha relevancia cuando se realiza un proceso repetitivo. Las funciones están diseñada específicamente para este proceso 8.2.1 Encuesta de hogares Los datos empleados en esta ocasión corresponden a la ultima encuesta de hogares, la cual ha sido estandarizada por CEPAL y se encuentra disponible en BADEHOG encuesta &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/encuestaMEX20N1.rds&quot;) encuesta_mrp &lt;- encuesta %&gt;% transmute( dam, dam2, ingreso = ingcorte,lp,li, logingreso = log(ingcorte + 1), area = case_when(haven::as_factor(areageo2,&quot;value&quot;) == 1 ~ &quot;1&quot;, TRUE ~ &quot;0&quot;), sexo = case_when(sexo == &quot;Mujer&quot; ~ &quot;1&quot;, TRUE ~ &quot;2&quot;), anoest = case_when( edad &lt; 2 | anoest == -1 ~ &quot;98&quot; , #No aplica anoest == 99 ~ &quot;99&quot;, #NS/NR anoest == 0 ~ &quot;1&quot;, # Sin educacion anoest %in% c(1:6) ~ &quot;2&quot;, # 1 - 6 anoest %in% c(7:12) ~ &quot;3&quot;, # 7 - 12 anoest &gt; 12 ~ &quot;4&quot;, # mas de 12 TRUE ~ &quot;Error&quot; ), edad = case_when( edad &lt; 15 ~ &quot;1&quot;, edad &lt; 30 ~ &quot;2&quot;, edad &lt; 45 ~ &quot;3&quot;, edad &lt; 65 ~ &quot;4&quot;, TRUE ~ &quot;5&quot;), discapacidad, etnia = etnia_ee, fep ) tba(encuesta_mrp %&gt;% head(10)) dam dam2 ingreso lp li logingreso area sexo anoest edad discapacidad etnia fep 01 01001 1803.277 2787 1342 7.4979 1 1 4 4 0 3 190 01 01001 1803.277 2787 1342 7.4979 1 1 3 2 0 3 190 01 01001 1803.277 2787 1342 7.4979 1 2 3 1 0 3 190 01 01001 2618.807 2787 1342 7.8709 1 2 3 4 0 3 190 01 01001 2618.807 2787 1342 7.8709 1 1 3 3 0 3 190 01 01001 2618.807 2787 1342 7.8709 1 2 3 2 0 3 190 01 01001 2618.807 2787 1342 7.8709 1 2 2 1 0 3 190 01 01001 5663.193 2787 1342 8.6419 1 2 4 2 0 3 189 01 01001 5663.193 2787 1342 8.6419 1 1 4 2 0 3 189 01 01001 11926.229 2787 1342 9.3866 1 2 4 2 0 3 189 La base de datos de la encuesta tiene la siguientes columnas: dam: Corresponde al código asignado a la división administrativa mayor del país. dam2: Corresponde al código asignado a la segunda división administrativa del país. lp y li lineas de pobreza y pobreza extrema definidas por CEPAL. área división geográfica (Urbano y Rural). sexo Hombre y Mujer. etnia En estas variable se definen tres grupos: afrodescendientes, indígenas y Otros. anoest Años de escolaridad edad Rangos de edad fep Factor de expansión por persona "],["validación-de-encuesta-frente-al-censo..html", "8.3 Validación de encuesta frente al censo.", " 8.3 Validación de encuesta frente al censo. library(survey) library(srvyr) library(patchwork) censo_dam2 &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/censo_mrp_dam2.rds&quot;) p1_dam &lt;- Plot_Compare(dat_encuesta = encuesta_mrp, dat_censo = censo_dam2, by = &quot;dam&quot;) p1_anotes &lt;- Plot_Compare(dat_encuesta = encuesta_mrp, dat_censo = censo_dam2, by = &quot;anoest&quot;) p1_edad &lt;- Plot_Compare(dat_encuesta = encuesta_mrp, dat_censo = censo_dam2, by = &quot;edad&quot;) p1 &lt;- (p1_dam)/(p1_anotes + p1_edad) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion4/0Recursos/plot_comp.png&quot;, # scale = 2) Evaluando interacciones en la encuesta encuesta_mrp$pobreza &lt;- encuesta_mrp$logingreso (plot_interaction(dat_encuesta = encuesta_mrp, by = &quot;sexo&quot;,by2 = &quot;area&quot;)/ plot_interaction(dat_encuesta = encuesta_mrp, by = &quot;sexo&quot;,by2 = &quot;anoest&quot;)) (plot_interaction(dat_encuesta = encuesta_mrp, by = &quot;sexo&quot;,by2 = &quot;edad&quot;)/ plot_interaction(dat_encuesta = encuesta_mrp, by = &quot;anoest&quot;,by2 = &quot;edad&quot;) ) Ahora, inspeccionamos el comportamiento de la variable de interés: media &lt;- mean(encuesta_mrp$logingreso) Sd &lt;- sd(encuesta_mrp$logingreso) ggplot(data = encuesta_mrp, aes(x = logingreso)) + geom_density(size =2, color = &quot;blue&quot;) + labs(y = &quot;&quot;) + stat_function(fun = dnorm, args = list(mean = media, sd = Sd), size =2) + theme_bw(base_size = 20) + theme(axis.text.y = element_blank(), axis.ticks = element_blank()) Figura 8.2: Distribuición del ingreso de las personas encuestadas La información auxiliar disponible ha sido extraída del censo e imágenes satelitales statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/predictors_satelital_dam2.rds&quot;) tba(statelevel_predictors_df %&gt;% head(10)) dam dam2 luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 01 01001 127.0742 105.4925 128.6893 106.7313 99.2730 99.4086 0.9453 0.0116 0.5180 0.2656 0.2129 0.1962 0.0671 0.2101 0.4280 0.2258 0.0574 0.0595 0.0020 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.1690 0.0284 01 01002 103.2847 101.3865 101.7692 100.7418 99.2012 99.2972 0.4106 0.0030 0.5082 0.2690 0.1853 0.1703 0.0665 0.3188 0.4404 0.0666 0.0706 0.0318 0.0448 0.5895 0.4913 0.0650 0.1084 0.6548 0.0118 0.2598 0.1648 0.1003 0.0648 0.3137 0.0436 01 01003 102.5703 100.2905 100.6374 101.8864 99.4002 99.4182 0.5802 0.0039 0.5117 0.2438 0.1828 0.1920 0.0990 0.3647 0.4001 0.0665 0.0865 0.0387 0.0116 1.0000 0.0776 0.0909 0.0035 0.5350 0.0096 0.1028 0.0070 0.0996 0.0630 0.2553 0.0647 01 01004 99.2927 98.8613 98.8818 97.8499 99.1040 99.1727 0.5490 0.0042 0.5139 0.2730 0.2032 0.1587 0.0610 0.2666 0.4727 0.0785 0.0603 0.0585 0.0258 0.5185 0.1752 0.0544 0.0123 0.7144 0.0079 0.2572 0.0861 0.1261 0.0592 0.3578 0.0303 01 01005 107.6633 100.5877 104.8903 100.5953 99.1894 99.2677 0.7330 0.0255 0.5014 0.2524 0.2309 0.1629 0.0569 0.2359 0.3886 0.1761 0.0511 0.0525 0.0050 0.9308 0.2182 0.0302 0.0013 0.3709 0.0039 0.0329 0.0200 0.2858 0.0538 0.2565 0.0162 01 01006 101.2918 99.5395 99.7443 98.6083 99.0990 99.1562 0.7530 0.0111 0.5086 0.2595 0.1996 0.1806 0.0706 0.2497 0.4258 0.1650 0.0576 0.0210 0.0043 0.6154 0.0544 0.0303 0.0183 0.4093 0.0032 0.0464 0.0215 0.2584 0.0582 0.2585 0.0197 01 01007 102.6725 100.9469 100.5439 99.5256 99.1553 99.2049 0.7223 0.0028 0.5125 0.2738 0.1898 0.1566 0.0620 0.2593 0.4439 0.1062 0.0500 0.0373 0.0107 0.6809 0.0585 0.0444 0.0432 0.5551 0.0057 0.1440 0.0729 0.1765 0.0728 0.3470 0.0192 01 01008 99.7144 100.2844 98.6047 99.5866 99.6891 99.6227 0.4434 0.1588 0.5210 0.2681 0.1871 0.1591 0.0641 0.2909 0.4598 0.0736 0.0479 0.0382 0.0136 0.6316 0.1688 0.0853 0.0085 0.7140 0.0072 0.3095 0.0818 0.1224 0.0563 0.3412 0.0217 01 01009 100.4092 99.1593 99.8478 98.6020 99.1182 99.1724 0.4175 0.0026 0.5112 0.2794 0.1917 0.1578 0.0689 0.3231 0.4423 0.0661 0.0544 0.0264 0.0306 0.7288 0.3110 0.0362 0.0308 0.5720 0.0101 0.1213 0.0847 0.1039 0.0731 0.3582 0.0841 01 01010 101.4046 102.5622 99.7574 99.7674 99.1833 99.2439 0.2279 0.0128 0.5002 0.2671 0.1919 0.1627 0.0680 0.3292 0.4268 0.0533 0.0480 0.0353 0.0389 0.9180 0.1160 0.0563 0.0315 0.7420 0.0136 0.4060 0.3097 0.0851 0.0743 0.3758 0.0399 8.3.1 Niveles de agregación para colapsar la encuesta Después de realizar una investigación en la literatura especializada y realizar estudios de simulación fue posible evidenciar que las predicciones obtenidas con la muestra sin agregar y la muestra agregada convergen a la media del dominio. Sin embargo, el realizar estas estimaciones con la muestra agregada reduce el tiempo computacional necesario para la convergencia de las cadenas MCMC. Con esto en mente se se realiza la identificación de las variables por las cuales se agregará la encuesta. byAgrega &lt;- c(&quot;dam2&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;anoest&quot;, &quot;edad&quot;) 8.3.2 Creando base con la encuesta agregada El resultado de agregar la base de dato se muestra a continuación: encuesta_df_agg &lt;- encuesta_mrp %&gt;% # Encuesta group_by_at(all_of(byAgrega)) %&gt;% # Agrupar por el listado de variables summarise(n = n(), # Número de observaciones # Ingreso medio de las personas con características similares. logingreso = mean(logingreso), .groups = &quot;drop&quot;) %&gt;% arrange(desc(n)) # Ordenar la base. La tabla obtenida es la siguiente: dam2 area sexo anoest edad n logingreso 08037 1 1 3 2 491 8.2663 08037 1 2 3 2 416 8.3813 01001 1 1 3 2 379 8.2301 01001 1 2 3 2 379 8.3168 02004 1 2 3 2 326 8.5024 23005 1 2 3 2 308 8.1430 02004 1 1 3 2 306 8.4478 08037 1 1 3 3 299 8.2545 25006 1 2 3 2 294 8.3635 25006 1 1 3 2 289 8.3257 El paso a seguir es unificar las tablas creadas. encuesta_df_agg &lt;- inner_join(encuesta_df_agg, statelevel_predictors_df) "],["definiendo-el-modelo-multinivel..html", "8.4 Definiendo el modelo multinivel.", " 8.4 Definiendo el modelo multinivel. Después de haber ordenado la encuesta, podemos pasar a la definición del modelo. options(MC.cores=parallel::detectCores()) # Permite procesar en paralelo. fit &lt;- stan_lmer( logingreso ~ # Ingreso medio (Y) (1 | dam2) + # Efecto aleatorio (ud) edad + # Efecto fijo (Variables X) sexo + tasa_desocupacion + luces_nocturnas + suelo_cultivo + suelo_urbano + modificacion_humana , weights = n, # Número de observaciones. data = encuesta_df_agg, # Encuesta agregada verbose = TRUE, # Muestre el avance del proceso chains = 4, # Número de cadenas. iter = 1000 # Número de realizaciones de la cadena ) saveRDS(fit, file = &quot;Recursos/Día2/Sesion4/Data/fit_ingresos.rds&quot;) Después de esperar un tiempo prudente se obtiene el siguiente modelo. fit &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/fit_ingresos.rds&quot;) tba(coef(fit)$dam2 %&gt;% head(10)) (Intercept) edad2 edad3 edad4 edad5 sexo2 tasa_desocupacion luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana 01001 5.7629 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01002 5.9340 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01003 6.0320 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01004 6.0625 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01005 6.0895 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01006 6.0810 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01007 6.0218 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01008 6.3261 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01009 6.2116 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 01010 5.8172 0.2342 0.231 0.3991 0.4775 0.0351 -1.98 0.0091 0.0071 0.0132 -0.0109 Validación del modelo library(posterior) library(bayesplot) encuesta_mrp2 &lt;- inner_join(encuesta_mrp, statelevel_predictors_df) y_pred_B &lt;- posterior_epred(fit, newdata = encuesta_mrp2) rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(encuesta_mrp2$logingreso), y_pred2) / ppc_dens_overlay(y = exp(as.numeric(encuesta_mrp2$logingreso)) - 1, (exp(y_pred2) - 1)) + xlim(0, 10000) ggsave(plot = p1, filename = &quot;Recursos/Día2/Sesion4/0Recursos/Ingreso.PNG&quot;, scale = 2) p1 &lt;- (mcmc_dens_chains(fit,pars = &quot;sigma&quot;) + mcmc_areas(fit,pars = &quot;sigma&quot;))/ mcmc_trace(fit,pars = &quot;sigma&quot;) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion4/0Recursos/Ingreso1.PNG&quot;, # scale = 2) var_names &lt;- c( &quot;edad2&quot;, &quot;edad3&quot;, &quot;edad4&quot;, &quot;edad5&quot;, &quot;sexo2&quot;, &quot;luces_nocturnas&quot;, &quot;suelo_urbano&quot;, &quot;suelo_cultivo&quot;, &quot;modificacion_humana&quot; ) p1 &lt;- mcmc_areas(fit, pars = var_names) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion4/0Recursos/Ingreso2.PNG&quot;, # scale = 2) p1 &lt;- mcmc_trace(fit,pars = var_names) ggsave(plot = p1, filename = &quot;Recursos/Día2/Sesion4/0Recursos/Ingreso3.PNG&quot;, scale = 2) ## Proceso de estimación y predicción Obtener el modelo es solo un paso más, ahora se debe realizar la predicción en el censo, el cual a sido previamente estandarizado y homologado con la encuesta. poststrat_df &lt;- readRDS(&quot;Recursos/Día2/Sesion4/Data/censo_mrp_dam2.rds&quot;) %&gt;% inner_join(statelevel_predictors_df) tba( poststrat_df %&gt;% arrange(desc(n)) %&gt;% head(10)) dam dam2 area etnia sexo edad anoest discapacidad n luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 02 02004 1 3 1 2 3 0 171484 140.2065 97.8905 167.4086 107.1455 99.3947 99.4554 0.9966 0.0268 0.4902 0.2696 0.2385 0.1970 0.0468 0.2458 0.4823 0.1274 0.0512 0.0597 0.0068 0.5761 0.0145 0.0386 0.0213 0.2865 0.0096 0.1836 0.2989 0.2037 0.0429 0.2514 0.0219 02 02004 1 3 2 2 3 0 157532 140.2065 97.8905 167.4086 107.1455 99.3947 99.4554 0.9966 0.0268 0.4902 0.2696 0.2385 0.1970 0.0468 0.2458 0.4823 0.1274 0.0512 0.0597 0.0068 0.5761 0.0145 0.0386 0.0213 0.2865 0.0096 0.1836 0.2989 0.2037 0.0429 0.2514 0.0219 11 11020 1 3 2 2 3 0 144757 140.4057 105.1877 142.3702 107.8847 99.2487 99.3674 0.9686 0.0139 0.5121 0.2730 0.2171 0.1779 0.0539 0.2817 0.4281 0.1188 0.0621 0.0724 0.0043 0.4528 0.1018 0.0544 0.0132 0.3999 0.0082 0.0146 0.0336 0.1899 0.0661 0.2872 0.0301 09 09007 1 3 1 2 3 0 143600 105.8486 97.8787 119.8566 98.3316 99.0787 99.1309 1.0000 0.0156 0.5185 0.2449 0.2134 0.2398 0.0995 0.2150 0.4953 0.1796 0.0582 0.0850 0.0012 0.0489 0.0126 0.0368 0.0040 0.2720 0.0036 0.0078 0.0551 0.2474 0.0395 0.2573 0.0399 09 09007 1 3 2 2 3 0 141503 105.8486 97.8787 119.8566 98.3316 99.0787 99.1309 1.0000 0.0156 0.5185 0.2449 0.2134 0.2398 0.0995 0.2150 0.4953 0.1796 0.0582 0.0850 0.0012 0.0489 0.0126 0.0368 0.0040 0.2720 0.0036 0.0078 0.0551 0.2474 0.0395 0.2573 0.0399 11 11020 1 3 1 2 3 0 138958 140.4057 105.1877 142.3702 107.8847 99.2487 99.3674 0.9686 0.0139 0.5121 0.2730 0.2171 0.1779 0.0539 0.2817 0.4281 0.1188 0.0621 0.0724 0.0043 0.4528 0.1018 0.0544 0.0132 0.3999 0.0082 0.0146 0.0336 0.1899 0.0661 0.2872 0.0301 08 08037 1 3 1 2 3 0 130852 151.0448 98.5664 167.8694 109.8990 101.1600 101.6602 0.9945 0.0115 0.4988 0.2582 0.2153 0.1970 0.0628 0.2606 0.4589 0.1451 0.0477 0.0544 0.0042 0.4465 0.0234 0.0224 0.0045 0.3587 0.0040 0.1036 0.2928 0.2212 0.0509 0.2409 0.0220 15 15033 1 3 1 2 3 0 129305 108.9309 97.9617 123.5294 98.7935 99.0817 99.1339 0.9990 0.0133 0.5202 0.2457 0.2088 0.2408 0.0839 0.2222 0.4900 0.1584 0.0632 0.0706 0.0039 0.2136 0.0808 0.0285 0.0134 0.3435 0.0088 0.0125 0.0569 0.2266 0.0395 0.2307 0.0447 08 08037 1 3 2 2 3 0 127546 151.0448 98.5664 167.8694 109.8990 101.1600 101.6602 0.9945 0.0115 0.4988 0.2582 0.2153 0.1970 0.0628 0.2606 0.4589 0.1451 0.0477 0.0544 0.0042 0.4465 0.0234 0.0224 0.0045 0.3587 0.0040 0.1036 0.2928 0.2212 0.0509 0.2409 0.0220 15 15033 1 3 2 2 3 0 124495 108.9309 97.9617 123.5294 98.7935 99.0817 99.1339 0.9990 0.0133 0.5202 0.2457 0.2088 0.2408 0.0839 0.2222 0.4900 0.1584 0.0632 0.0706 0.0039 0.2136 0.0808 0.0285 0.0134 0.3435 0.0088 0.0125 0.0569 0.2266 0.0395 0.2307 0.0447 Note que la información del censo esta agregada. 8.4.1 Distribución posterior. Para obtener una distribución posterior de cada observación se hace uso de la función posterior_epred de la siguiente forma. epred_mat &lt;- posterior_epred(fit, newdata = poststrat_df, type = &quot;response&quot;) Como el interés es realizar comparaciones entre los países de la región se presenta la estimación del ingreso medio en términos de lineas de pobreza. Para esto procedemos así: Obteniendo las lineas de pobreza por cada post-estrato (lp &lt;- encuesta_mrp %&gt;% distinct(area,lp,li)) %&gt;% tba() area lp li 1 2787 1342 0 2095 1101 Ingreso en términos de lineas de pobreza. lp &lt;- inner_join(poststrat_df,lp,by = &quot;area&quot;) %&gt;% select(lp) epred_mat &lt;- (exp(epred_mat)-1)/lp$lp "],["estimación-del-ingreso-medio-nacional.html", "8.5 Estimación del ingreso medio nacional", " 8.5 Estimación del ingreso medio nacional n_filtered &lt;- poststrat_df$n mrp_estimates &lt;- epred_mat %*% n_filtered / sum(n_filtered) (temp_ing &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 1.4858 0.0589 El resultado nos indica que el ingreso medio nacional es 1.49 lineas de pobreza 8.5.1 Estimación para el dam == “05”. Es importante siempre conservar el orden de la base, dado que relación entre la predicción y el censo en uno a uno. temp &lt;- poststrat_df %&gt;% mutate(Posi = 1:n()) temp &lt;- filter(temp, dam == &quot;05&quot;) %&gt;% select(n, Posi) n_filtered &lt;- temp$n temp_epred_mat &lt;- epred_mat[, temp$Posi] ## Estimando el CME mrp_estimates &lt;- temp_epred_mat %*% n_filtered / sum(n_filtered) (temp_dam05 &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 1.6972 0.0236 El resultado nos indica que el ingreso medio en el dam 05 es 1.7 lineas de pobreza 8.5.2 Estimación para la dam2 == “05001” temp &lt;- poststrat_df %&gt;% mutate(Posi = 1:n()) temp &lt;- filter(temp, dam2 == &quot;05001&quot;) %&gt;% select(n, Posi) n_filtered &lt;- temp$n temp_epred_mat &lt;- epred_mat[, temp$Posi] ## Estimando el CME mrp_estimates &lt;- temp_epred_mat %*% n_filtered / sum(n_filtered) (temp_dam2_05001 &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 1.1885 0.4653 El resultado nos indica que el ingreso medio en la dam2 05001 es 1.19 lineas de pobreza Después de comprender la forma en que se realiza la estimación de los dominios no observados procedemos el uso de la función Aux_Agregado que es desarrollada para este fin. (mrp_estimate_Ingresolp &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = NULL) ) %&gt;% tba() Nacional mrp_estimate mrp_estimate_se Nacional 1.4858 0.0589 El resultado nos indica que el ingreso medio nacional es 1 lineas de pobreza De forma similar es posible obtener los resultados para las divisiones administrativas. mrp_estimate_dam &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = &quot;dam&quot;) tba(mrp_estimate_dam %&gt;% head(10) ) dam mrp_estimate mrp_estimate_se 01 1.6481 0.0270 02 1.9452 0.0412 03 1.9317 0.0228 04 1.3795 0.0160 05 1.6972 0.0236 06 1.8084 0.0198 07 0.9211 0.1291 08 1.8629 0.0336 09 1.9366 0.0155 10 1.3266 0.0307 mrp_estimate_dam2 &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = &quot;dam2&quot;) tba(mrp_estimate_dam2 %&gt;% head(10) ) dam2 mrp_estimate mrp_estimate_se 01001 1.7740 0.0405 01002 1.2209 0.0363 01003 1.2377 0.0421 01004 1.3223 0.0659 01005 1.5968 0.0404 01006 1.4139 0.0408 01007 1.3518 0.0337 01008 1.7199 0.0877 01009 1.4134 0.0564 01010 1.0566 0.0600 El mapa resultante es el siguiente mrp_estimate_etnia_anoes &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = c(&quot;dam&quot;,&quot;anoest&quot;,&quot;etnia&quot;)) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Recursos/Día2/Sesion4/Shape/MEX_dam.shp&quot;) %&gt;% rename(depto = dam) p1 &lt;- Aux_Maps( Shape = ShapeSAE, dat_df = mrp_estimate_etnia_anoes%&gt;% rename(depto = dam), fnames = &quot;etnia&quot;, cnames = &quot;anoest&quot;, brks = brks_lp, outPaht = &quot;Recursos/Día2/Sesion4/0Recursos/Mosaico.png&quot; ) "],["día-3---sesión-1--estimación-de-la-pobreza-a-partir-del-ingreso.html", "Capítulo 9 Día 3 - Sesión 1- Estimación de la pobreza a partir del ingreso", " Capítulo 9 Día 3 - Sesión 1- Estimación de la pobreza a partir del ingreso Sea \\[ y_{ji}=\\begin{cases} 1 &amp; ingreso_{ji}\\le lp\\\\ 0 &amp; e.o.c. \\end{cases} \\] donde \\(ingreso_{ji}\\) representa el ingreso de la \\(i\\)-ésima persona en el \\(j\\)-ésimo post-estrato y \\(lp\\) es un valor limite, en particular la linea de pobreza. Ahora, Suponga que el ingreso de las personas se obtiene mediante el modelo de unidad estimado previamente. Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[ P_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) toma el valor de 1 cuando el ingreso de la persona es menor a la linea de pobreza 0 en caso contrario. Note que, \\[ \\begin{equation*} \\bar{Y}_d = P_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(P\\) esta dado por: \\[ \\hat{P} = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{P} = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] "],["proceso-de-estimación-y-predicción.html", "9.1 Proceso de estimación y predicción", " 9.1 Proceso de estimación y predicción source(&quot;Recursos/Día3/Sesion1/0Recursos/funciones_mrp.R&quot;) fit &lt;- readRDS(&quot;Recursos/Día3/Sesion1/Data/fit_ingresos.rds&quot;) La información auxiliar disponible ha sido extraída del censo e imágenes satelitales statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día3/Sesion1/Data/predictors_satelital_dam2.rds&quot;) Obtener el modelo es solo un paso más, ahora se debe realizar la predicción en el censo, el cual a sido previamente estandarizado y homologado con la encuesta. poststrat_df &lt;- readRDS(&quot;Recursos/Día3/Sesion1/Data/censo_mrp_dam2.rds&quot;) %&gt;% inner_join(statelevel_predictors_df) tba( poststrat_df %&gt;% arrange(desc(n)) %&gt;% head(10)) dam dam2 area etnia sexo edad anoest discapacidad n luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 02 02004 1 3 1 2 3 0 171484 140.2065 97.8905 167.4086 107.1455 99.3947 99.4554 0.9966 0.0268 0.4902 0.2696 0.2385 0.1970 0.0468 0.2458 0.4823 0.1274 0.0512 0.0597 0.0068 0.5761 0.0145 0.0386 0.0213 0.2865 0.0096 0.1836 0.2989 0.2037 0.0429 0.2514 0.0219 02 02004 1 3 2 2 3 0 157532 140.2065 97.8905 167.4086 107.1455 99.3947 99.4554 0.9966 0.0268 0.4902 0.2696 0.2385 0.1970 0.0468 0.2458 0.4823 0.1274 0.0512 0.0597 0.0068 0.5761 0.0145 0.0386 0.0213 0.2865 0.0096 0.1836 0.2989 0.2037 0.0429 0.2514 0.0219 11 11020 1 3 2 2 3 0 144757 140.4057 105.1877 142.3702 107.8847 99.2487 99.3674 0.9686 0.0139 0.5121 0.2730 0.2171 0.1779 0.0539 0.2817 0.4281 0.1188 0.0621 0.0724 0.0043 0.4528 0.1018 0.0544 0.0132 0.3999 0.0082 0.0146 0.0336 0.1899 0.0661 0.2872 0.0301 09 09007 1 3 1 2 3 0 143600 105.8486 97.8787 119.8566 98.3316 99.0787 99.1309 1.0000 0.0156 0.5185 0.2449 0.2134 0.2398 0.0995 0.2150 0.4953 0.1796 0.0582 0.0850 0.0012 0.0489 0.0126 0.0368 0.0040 0.2720 0.0036 0.0078 0.0551 0.2474 0.0395 0.2573 0.0399 09 09007 1 3 2 2 3 0 141503 105.8486 97.8787 119.8566 98.3316 99.0787 99.1309 1.0000 0.0156 0.5185 0.2449 0.2134 0.2398 0.0995 0.2150 0.4953 0.1796 0.0582 0.0850 0.0012 0.0489 0.0126 0.0368 0.0040 0.2720 0.0036 0.0078 0.0551 0.2474 0.0395 0.2573 0.0399 11 11020 1 3 1 2 3 0 138958 140.4057 105.1877 142.3702 107.8847 99.2487 99.3674 0.9686 0.0139 0.5121 0.2730 0.2171 0.1779 0.0539 0.2817 0.4281 0.1188 0.0621 0.0724 0.0043 0.4528 0.1018 0.0544 0.0132 0.3999 0.0082 0.0146 0.0336 0.1899 0.0661 0.2872 0.0301 08 08037 1 3 1 2 3 0 130852 151.0448 98.5664 167.8694 109.8990 101.1600 101.6602 0.9945 0.0115 0.4988 0.2582 0.2153 0.1970 0.0628 0.2606 0.4589 0.1451 0.0477 0.0544 0.0042 0.4465 0.0234 0.0224 0.0045 0.3587 0.0040 0.1036 0.2928 0.2212 0.0509 0.2409 0.0220 15 15033 1 3 1 2 3 0 129305 108.9309 97.9617 123.5294 98.7935 99.0817 99.1339 0.9990 0.0133 0.5202 0.2457 0.2088 0.2408 0.0839 0.2222 0.4900 0.1584 0.0632 0.0706 0.0039 0.2136 0.0808 0.0285 0.0134 0.3435 0.0088 0.0125 0.0569 0.2266 0.0395 0.2307 0.0447 08 08037 1 3 2 2 3 0 127546 151.0448 98.5664 167.8694 109.8990 101.1600 101.6602 0.9945 0.0115 0.4988 0.2582 0.2153 0.1970 0.0628 0.2606 0.4589 0.1451 0.0477 0.0544 0.0042 0.4465 0.0234 0.0224 0.0045 0.3587 0.0040 0.1036 0.2928 0.2212 0.0509 0.2409 0.0220 15 15033 1 3 2 2 3 0 124495 108.9309 97.9617 123.5294 98.7935 99.0817 99.1339 0.9990 0.0133 0.5202 0.2457 0.2088 0.2408 0.0839 0.2222 0.4900 0.1584 0.0632 0.0706 0.0039 0.2136 0.0808 0.0285 0.0134 0.3435 0.0088 0.0125 0.0569 0.2266 0.0395 0.2307 0.0447 Note que la información del censo esta agregada. 9.1.1 Distribución posterior. Para obtener una distribución posterior de cada observación se hace uso de la función posterior_epred de la siguiente forma. epred_mat &lt;- posterior_epred(fit, newdata = poststrat_df, type = &quot;response&quot;) dim(epred_mat) dim(poststrat_df) Como el interés es realizar comparaciones entre los países de la región se presenta la estimación del ingreso medio en términos de lineas de pobreza. Para esto procedemos así: Obteniendo las lineas de pobreza por cada post-estrato ( lp &lt;- readRDS(&quot;Recursos/Día3/Sesion1/Data/encuestaMEX20N1.rds&quot;) %&gt;% distinct(areageo2, lp, li) %&gt;% mutate( area = ifelse( haven::as_factor(areageo2, levels = &quot;values&quot;) == 1 , &quot;1&quot;, &quot;0&quot;), areageo2 = NULL ) ) %&gt;% tba() lp li area 2787 1342 1 2095 1101 0 Ingreso en términos de lineas de pobreza. lp &lt;- inner_join(poststrat_df,lp,by = &quot;area&quot;) %&gt;% select(lp) epred_mat_pobreza_lp &lt;- (exp(epred_mat)-1) &lt;= lp$lp # epred_mat_pobreza_li &lt;- (exp(epred_mat)-1) &lt;= lp$li "],["estimación-de-la-pobreza.html", "9.2 Estimación de la pobreza", " 9.2 Estimación de la pobreza n_filtered &lt;- poststrat_df$n mrp_estimates &lt;- epred_mat_pobreza_lp %*% n_filtered / sum(n_filtered) (temp_ing &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 0.2082 0.0455 El resultado nos indica que el ingreso medio nacional es 0.21 lineas de pobreza 9.2.1 Estimación para el dam == “05”. Es importante siempre conservar el orden de la base, dado que relación entre la predicción y el censo en uno a uno. temp &lt;- poststrat_df %&gt;% mutate(Posi = 1:n()) temp &lt;- filter(temp, dam == &quot;05&quot;) %&gt;% select(n, Posi) n_filtered &lt;- temp$n temp_epred_mat &lt;- epred_mat_pobreza_lp[, temp$Posi] ## Estimando el CME mrp_estimates &lt;- temp_epred_mat %*% n_filtered / sum(n_filtered) (temp_dam05 &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 0.051 0.0108 El resultado nos indica que el ingreso medio en el dam 05 es 0.05 lineas de pobreza 9.2.2 Estimación para la dam2 == “05001” temp &lt;- poststrat_df %&gt;% mutate(Posi = 1:n()) temp &lt;- filter(temp, dam2 == &quot;05001&quot;) %&gt;% select(n, Posi) n_filtered &lt;- temp$n temp_epred_mat &lt;- epred_mat_pobreza_lp[, temp$Posi] ## Estimando el CME mrp_estimates &lt;- temp_epred_mat %*% n_filtered / sum(n_filtered) (temp_dam2_05001 &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 0.4241 0.3616 El resultado nos indica que el ingreso medio en la dam2 05001 es 0.42 lineas de pobreza Después de comprender la forma en que se realiza la estimación de los dominios no observados procedemos el uso de la función Aux_Agregado que es desarrollada para este fin. (mrp_estimate_Ingresolp &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat_pobreza_lp, byMap = NULL) ) Nacional mrp_estimate mrp_estimate_se Nacional 0.2082 0.0455 De forma similar es posible obtener los resultados para las divisiones administrativas. mrp_estimate_dam &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat_pobreza_lp, byMap = &quot;dam&quot;) tba(mrp_estimate_dam %&gt;% head(10)) dam2 mrp_estimate mrp_estimate_se 01001 0.0000 0.0000 01002 0.1603 0.0530 01003 0.1457 0.0445 01004 0.1320 0.0409 01005 0.0000 0.0000 01006 0.1323 0.0549 01007 0.1405 0.0425 01008 0.0001 0.0023 01009 0.1133 0.0592 01010 0.4717 0.1202 mrp_estimate_dam2 &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat_pobreza_lp, byMap = &quot;dam2&quot;) tba(mrp_estimate_dam2 %&gt;% head(10) ) dam2 mrp_estimate mrp_estimate_se 01001 0.0000 0.0000 01002 0.1603 0.0530 01003 0.1457 0.0445 01004 0.1320 0.0409 01005 0.0000 0.0000 01006 0.1323 0.0549 01007 0.1405 0.0425 01008 0.0001 0.0023 01009 0.1133 0.0592 01010 0.4717 0.1202 El mapa resultante es el siguiente mrp_estimate_etnia_anoes &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = c(&quot;dam&quot;,&quot;anoest&quot;,&quot;etnia&quot;)) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Shape/MEX_dam.shp&quot;) %&gt;% rename(depto = dam) p1 &lt;- Aux_Maps( Shape = ShapeSAE, dat_df = mrp_estimate_etnia_anoes%&gt;% rename(depto = dam), fnames = &quot;etnia&quot;, cnames = &quot;anoest&quot;, brks = brks_lp, outPaht = &quot;Recursos/Día3/Sesion1/0Recursos/Mosaico.png&quot; ) "],["día-3---sesión-2--modelo-de-unidad-para-la-estimación-de-la-pobreza.html", "Capítulo 10 Día 3 - Sesión 2- Modelo de unidad para la estimación de la pobreza", " Capítulo 10 Día 3 - Sesión 2- Modelo de unidad para la estimación de la pobreza Lo primero a tener en cuenta, es que no se debe usar una regresión lineal cuando se tiene una variable de tipo binario como variable dependiente, ya que no es posible estimar la probabilidad del evento estudiado de manera directa, por esta razón se emplea una regresión logística, en la que para obtener las estimaciones de la probabilidad del evento estudiado se debe realizar una transformación (logit), lo cual consiste en tomar el logaritmo de la probabilidad de éxito entre la probabilidad de fracaso, de la siguiente manera: \\[ \\ln \\frac{\\theta}{1-\\theta} \\] donde \\(\\theta\\) representa la probabilidad de éxito del evento. "],["modelo-de-regresión-logistica..html", "10.1 Modelo de regresión logistica.", " 10.1 Modelo de regresión logistica. Sea \\[ y_{ji}=\\begin{cases} 1 &amp; ingreso_{ji}\\le lp\\\\ 0 &amp; e.o.c. \\end{cases} \\] donde \\(ingreso_{ji}\\) representa el ingreso de la \\(i\\)-ésima persona en el \\(j\\)-ésimo post-estrato y \\(lp\\) es un valor limite, en particular la linea de pobreza. Empleando un modelo de regresión logística de efecto aleatorios pretende establecer la relación entre la expectativa \\(\\theta_{ji}\\) de la variable dicotómica con las covariables de información auxiliar disponibles para ser incluidas. El procedimiento correspondiente a este proceso, modela el logaritmo del cociente entre la probabilidad de estar por debajo de la linea de pobreza a su complemento en relación al conjunto de covariables a nivel de unidad, \\(x_{ji}\\), y el efecto aleatorio \\(u_d\\). \\[ \\begin{eqnarray*} \\ln\\left(\\frac{\\theta_{ji}}{1-\\theta_{ji}}\\right)=\\boldsymbol{x}_{ji}^{T}\\boldsymbol{\\beta}+u_d \\end{eqnarray*} \\] Donde los coeficientes \\(\\boldsymbol{\\beta}\\) hacen referencia a los efectos fijos de las variables \\(x_{ji}^T\\) sobre las probabilidades de que la \\(i\\)-ésima persona este por debajo de la linea de pobreza; por otro lado, \\(u_d\\) son los efectos fijos aleatorios, donde \\(u_{d}\\sim N\\left(0,\\sigma^2_{u}\\right)\\). Para este caso se asumen las distribuciones previas \\[ \\begin{eqnarray*} \\beta_k &amp; \\sim &amp; N(0, 1000)\\\\ \\sigma^2_u &amp;\\sim &amp; IG(0.0001,0.0001) \\end{eqnarray*} \\] las cuales se toman no informativas. Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza, es decir, \\[ P_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_{di}\\) toma el valor de 1 cuando el ingreso de la persona es menor a la linea de pobreza 0 en caso contrario. Note que, \\[ \\begin{equation*} \\bar{Y}_d = P_d = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}y_{di}}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(P\\) esta dado por: \\[ \\hat{P} = \\frac{\\sum_{s_d}y_{di} + \\sum_{s^c_d}\\hat{y}_{di}}{N_d} \\] donde \\[\\hat{y}_{di}=E_{\\mathscr{M}}\\left(y_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\], donde \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{P} = \\frac{\\sum_{U_{d}}\\hat{y}_{di}}{N_d} \\] A continuación se muestra el proceso realizado para la obtención de la predicción de la tasa de pobreza. "],["proceso-de-estimación-en-r-1.html", "10.2 Proceso de estimación en R", " 10.2 Proceso de estimación en R Para desarrollar la metodología se hace uso de las siguientes librerías. # Interprete de STAN en R library(rstan) library(rstanarm) # Manejo de bases de datos. library(tidyverse) # Gráficas de los modelos. library(bayesplot) library(patchwork) # Organizar la presentación de las tablas library(kableExtra) library(printr) Un conjunto de funciones desarrolladas para realizar de forma simplificada los procesos están consignadas en la siguiente rutina. source(&quot;Recursos/Día3/Sesion2/0Recursos/funciones_mrp.R&quot;) Entre las funciones incluidas en el archivo encuentra plot_interaction: Esta crea un diagrama de lineas donde se estudia la interacción entre las variables, en el caso de presentar un traslape de las lineas se recomienda incluir el interacción en el modelo. Plot_Compare Puesto que es necesario realizar una homologar la información del censo y la encuesta es conveniente llevar a cabo una validación de las variables que han sido homologadas, por tanto, se espera que las proporciones resultantes del censo y la encuesta estén cercanas entre sí. Aux_Agregado: Esta es función permite obtener estimaciones a diferentes niveles de agregación, toma mucha relevancia cuando se realiza un proceso repetitivo. Las funciones están diseñada específicamente para este proceso 10.2.1 Encuesta de hogares Los datos empleados en esta ocasión corresponden a la ultima encuesta de hogares, la cual ha sido estandarizada por CEPAL y se encuentra disponible en BADEHOG encuesta &lt;- readRDS(&quot;Recursos/Día3/Sesion2/Data/encuestaMEX20N1.rds&quot;) encuesta_mrp &lt;- encuesta %&gt;% transmute( dam, dam2, ingreso = ingcorte,lp,li, pobreza = ifelse(ingcorte&lt;=lp,1,0), area = case_when(haven::as_factor(areageo2,&quot;value&quot;) == 1 ~ &quot;1&quot;, TRUE ~ &quot;0&quot;), sexo = case_when(sexo == &quot;Mujer&quot; ~ &quot;1&quot;, TRUE ~ &quot;2&quot;), anoest = case_when( edad &lt; 2 | anoest == -1 ~ &quot;98&quot; , #No aplica anoest == 99 ~ &quot;99&quot;, #NS/NR anoest == 0 ~ &quot;1&quot;, # Sin educacion anoest %in% c(1:6) ~ &quot;2&quot;, # 1 - 6 anoest %in% c(7:12) ~ &quot;3&quot;, # 7 - 12 anoest &gt; 12 ~ &quot;4&quot;, # mas de 12 TRUE ~ &quot;Error&quot; ), edad = case_when( edad &lt; 15 ~ &quot;1&quot;, edad &lt; 30 ~ &quot;2&quot;, edad &lt; 45 ~ &quot;3&quot;, edad &lt; 65 ~ &quot;4&quot;, TRUE ~ &quot;5&quot;), discapacidad, etnia = etnia_ee, fep ) tba(encuesta_mrp %&gt;% head(10)) dam dam2 ingreso lp li pobreza area sexo anoest edad discapacidad etnia fep 01 01001 1803.277 2787 1342 1 1 1 4 4 0 3 190 01 01001 1803.277 2787 1342 1 1 1 3 2 0 3 190 01 01001 1803.277 2787 1342 1 1 2 3 1 0 3 190 01 01001 2618.807 2787 1342 1 1 2 3 4 0 3 190 01 01001 2618.807 2787 1342 1 1 1 3 3 0 3 190 01 01001 2618.807 2787 1342 1 1 2 3 2 0 3 190 01 01001 2618.807 2787 1342 1 1 2 2 1 0 3 190 01 01001 5663.193 2787 1342 0 1 2 4 2 0 3 189 01 01001 5663.193 2787 1342 0 1 1 4 2 0 3 189 01 01001 11926.229 2787 1342 0 1 2 4 2 0 3 189 La base de datos de la encuesta tiene la siguientes columnas: dam: Corresponde al código asignado a la división administrativa mayor del país. dam2: Corresponde al código asignado a la segunda división administrativa del país. lp y li lineas de pobreza y pobreza extrema definidas por CEPAL. área división geográfica (Urbano y Rural). sexo Hombre y Mujer. etnia En estas variable se definen tres grupos: afrodescendientes, indígenas y Otros. Años de escolaridad (anoest) Rangos de edad (edad) Factor de expansión por persona (fep) Ahora, inspeccionamos el comportamiento de la variable de interés: tab &lt;- encuesta_mrp %&gt;% group_by(pobreza) %&gt;% tally() %&gt;% mutate(prop = round(n/sum(n),2), pobreza = ifelse(pobreza == 1, &quot;Si&quot;, &quot;No&quot;)) ggplot(data = tab, aes(x = pobreza, y = prop)) + geom_bar(stat = &quot;identity&quot;) + labs(y = &quot;&quot;, x = &quot;&quot;) + geom_text(aes(label = paste(prop*100,&quot;%&quot;)), nudge_y=0.05) + theme_bw(base_size = 20) + theme(axis.text.y = element_blank(), axis.ticks = element_blank()) Figura 10.1: Proporción de personas por debajo de la linea de pobreza La información auxiliar disponible ha sido extraída del censo e imágenes satelitales statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día3/Sesion2/Data/predictors_satelital_dam2.rds&quot;) tba(statelevel_predictors_df %&gt;% head(10)) dam dam2 luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 01 01001 127.0742 105.4925 128.6893 106.7313 99.2730 99.4086 0.9453 0.0116 0.5180 0.2656 0.2129 0.1962 0.0671 0.2101 0.4280 0.2258 0.0574 0.0595 0.0020 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.1690 0.0284 01 01002 103.2847 101.3865 101.7692 100.7418 99.2012 99.2972 0.4106 0.0030 0.5082 0.2690 0.1853 0.1703 0.0665 0.3188 0.4404 0.0666 0.0706 0.0318 0.0448 0.5895 0.4913 0.0650 0.1084 0.6548 0.0118 0.2598 0.1648 0.1003 0.0648 0.3137 0.0436 01 01003 102.5703 100.2905 100.6374 101.8864 99.4002 99.4182 0.5802 0.0039 0.5117 0.2438 0.1828 0.1920 0.0990 0.3647 0.4001 0.0665 0.0865 0.0387 0.0116 1.0000 0.0776 0.0909 0.0035 0.5350 0.0096 0.1028 0.0070 0.0996 0.0630 0.2553 0.0647 01 01004 99.2927 98.8613 98.8818 97.8499 99.1040 99.1727 0.5490 0.0042 0.5139 0.2730 0.2032 0.1587 0.0610 0.2666 0.4727 0.0785 0.0603 0.0585 0.0258 0.5185 0.1752 0.0544 0.0123 0.7144 0.0079 0.2572 0.0861 0.1261 0.0592 0.3578 0.0303 01 01005 107.6633 100.5877 104.8903 100.5953 99.1894 99.2677 0.7330 0.0255 0.5014 0.2524 0.2309 0.1629 0.0569 0.2359 0.3886 0.1761 0.0511 0.0525 0.0050 0.9308 0.2182 0.0302 0.0013 0.3709 0.0039 0.0329 0.0200 0.2858 0.0538 0.2565 0.0162 01 01006 101.2918 99.5395 99.7443 98.6083 99.0990 99.1562 0.7530 0.0111 0.5086 0.2595 0.1996 0.1806 0.0706 0.2497 0.4258 0.1650 0.0576 0.0210 0.0043 0.6154 0.0544 0.0303 0.0183 0.4093 0.0032 0.0464 0.0215 0.2584 0.0582 0.2585 0.0197 01 01007 102.6725 100.9469 100.5439 99.5256 99.1553 99.2049 0.7223 0.0028 0.5125 0.2738 0.1898 0.1566 0.0620 0.2593 0.4439 0.1062 0.0500 0.0373 0.0107 0.6809 0.0585 0.0444 0.0432 0.5551 0.0057 0.1440 0.0729 0.1765 0.0728 0.3470 0.0192 01 01008 99.7144 100.2844 98.6047 99.5866 99.6891 99.6227 0.4434 0.1588 0.5210 0.2681 0.1871 0.1591 0.0641 0.2909 0.4598 0.0736 0.0479 0.0382 0.0136 0.6316 0.1688 0.0853 0.0085 0.7140 0.0072 0.3095 0.0818 0.1224 0.0563 0.3412 0.0217 01 01009 100.4092 99.1593 99.8478 98.6020 99.1182 99.1724 0.4175 0.0026 0.5112 0.2794 0.1917 0.1578 0.0689 0.3231 0.4423 0.0661 0.0544 0.0264 0.0306 0.7288 0.3110 0.0362 0.0308 0.5720 0.0101 0.1213 0.0847 0.1039 0.0731 0.3582 0.0841 01 01010 101.4046 102.5622 99.7574 99.7674 99.1833 99.2439 0.2279 0.0128 0.5002 0.2671 0.1919 0.1627 0.0680 0.3292 0.4268 0.0533 0.0480 0.0353 0.0389 0.9180 0.1160 0.0563 0.0315 0.7420 0.0136 0.4060 0.3097 0.0851 0.0743 0.3758 0.0399 10.2.2 Niveles de agregación para colapsar la encuesta Después de realizar una investigación en la literatura especializada y realizar estudios de simulación fue posible evidenciar que las predicciones obtenidas con la muestra sin agregar y la muestra agregada convergen a la media del dominio. byAgrega &lt;- c(&quot;dam&quot; ,&quot;dam2&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;anoest&quot;, &quot;edad&quot;) 10.2.3 Creando base con la encuesta agregada El resultado de agregar la base de dato se muestra a continuación: encuesta_df_agg &lt;- encuesta_mrp %&gt;% # Encuesta group_by_at(all_of(byAgrega)) %&gt;% # Agrupar por el listado de variables summarise(n = n(), # Número de observaciones # conteo de personas con características similares. pobreza = sum(pobreza), no_pobreza = n-pobreza, .groups = &quot;drop&quot;) %&gt;% arrange(desc(pobreza)) # Ordenar la base. La tabla obtenida es la siguiente: dam dam2 area sexo anoest edad n pobreza no_pobreza 08 08037 1 1 3 2 491 142 349 01 01001 1 1 2 1 226 105 121 23 23005 1 1 3 2 272 105 167 23 23005 1 2 3 2 308 104 204 08 08037 1 2 2 1 244 102 142 01 01001 1 1 3 2 379 99 280 01 01001 1 2 3 2 379 98 281 12 12001 1 1 3 2 153 98 55 23 23005 1 2 2 1 163 96 67 10 10005 1 2 2 1 150 94 56 El paso a seguir es unificar las tablas creadas. encuesta_df_agg &lt;- inner_join(encuesta_df_agg, statelevel_predictors_df) 10.2.4 Definiendo el modelo multinivel. Después de haber ordenado la encuesta, podemos pasar a la definición del modelo. options(mc.cores = parallel::detectCores()) # Permite procesar en paralelo. fit &lt;- stan_glmer( cbind(pobreza, no_pobreza) ~ (1 | dam2) + # Efecto aleatorio (ud) edad + # Efecto fijo (Variables X) sexo + tasa_desocupacion + luces_nocturnas + suelo_cultivo + suelo_urbano + modificacion_humana , data = encuesta_df_agg, # Encuesta agregada verbose = TRUE, # Muestre el avance del proceso chains = 4, # Número de cadenas. iter = 1000, # Número de realizaciones de la cadena cores = 4, family = binomial(link = &quot;logit&quot;) ) saveRDS(fit, file = &quot;Recursos/Día3/Sesion2/Data/fit_pobreza2.rds&quot;) Después de esperar un tiempo prudente se obtiene el siguiente modelo. fit &lt;- readRDS(&quot;Recursos/Día3/Sesion2/Data/fit_pobreza2.rds&quot;) Validación del modelo library(posterior) library(bayesplot) encuesta_mrp2 &lt;- inner_join(encuesta_mrp %&gt;% filter(), statelevel_predictors_df) y_pred_B &lt;- posterior_epred(fit, newdata = encuesta_mrp2) rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(encuesta_mrp2$pobreza), y_pred2) # ggsave(plot = p1, # filename = &quot;Recursos/Día3/Sesion2/0Recursos/ppc_pobreza.PNG&quot;, # scale = 2) Figura 10.2: Tasa de pobreza por dam2 var_names &lt;- c(&quot;luces_nocturnas&quot;, &quot;suelo_cultivo&quot;,&quot;suelo_urbano&quot;, &quot;modificacion_humana&quot;, &quot;edad2&quot;,&quot;edad3&quot;,&quot;edad4&quot;,&quot;edad5&quot;) p1 &lt;- mcmc_areas(fit, pars = var_names) ggsave(plot = p1, filename = &quot;Recursos/Día3/Sesion2/0Recursos/pobreza1.PNG&quot;, scale = 2) p1 &lt;- mcmc_trace(fit,pars = var_names) ggsave(plot = p1, filename = &quot;Recursos/Día3/Sesion2/0Recursos/pobreza2.PNG&quot;, scale = 2) Los coeficientes del modelo para las primeras dam2 son: (Intercept) edad2 edad3 edad4 edad5 sexo2 tasa_desocupacion luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana 01001 4.4041 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01002 3.3811 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01003 3.8038 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01004 3.0597 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01005 3.6053 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01006 3.5408 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01007 3.6345 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01008 3.9105 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01009 2.8381 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 01010 3.8182 -0.6468 -0.5582 -1.0093 -1.2643 -0.0843 4.6382 -0.0291 -0.0143 -0.0117 0.0144 "],["proceso-de-estimación-y-predicción-1.html", "10.3 Proceso de estimación y predicción", " 10.3 Proceso de estimación y predicción Obtener el modelo es solo un paso más, ahora se debe realizar la predicción en el censo, el cual fue estandarizado y homologado con la encuesta previamente. poststrat_df &lt;- readRDS(&quot;Recursos/Día3/Sesion2/Data/censo_mrp_dam2.rds&quot;) %&gt;% inner_join(statelevel_predictors_df) tba( poststrat_df %&gt;% head(10)) dam dam2 area etnia sexo edad anoest discapacidad n luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 01 01001 0 1 1 1 1 0 300 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 1 2 0 151 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 2 2 0 38 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 2 3 0 224 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 2 4 0 38 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 3 2 0 112 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 3 3 0 151 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 3 3 1 38 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 4 1 0 38 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 01 01001 0 1 1 4 2 0 75 127.0742 105.4925 128.6893 106.7313 99.273 99.4086 0.9453 0.0116 0.518 0.2656 0.2129 0.1962 0.0671 0.2101 0.428 0.2258 0.0574 0.0595 0.002 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.169 0.0284 Note que la información del censo esta agregada. 10.3.1 Distribución posterior. Para obtener una distribución posterior de cada observación se hace uso de la función posterior_epred de la siguiente forma. epred_mat &lt;- posterior_epred(fit, newdata = poststrat_df, type = &quot;response&quot;) dim(epred_mat) dim(poststrat_df) 10.3.2 Estimación de la tasa de pobreza n_filtered &lt;- poststrat_df$n mrp_estimates &lt;- epred_mat %*% n_filtered / sum(n_filtered) (temp_ing &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 0.3685 0.0261 El resultado nos indica que el ingreso medio nacional es 0.37 lineas de pobreza 10.3.3 Estimación para el dam == “05”. Es importante siempre conservar el orden de la base, dado que relación entre la predicción y el censo en uno a uno. temp &lt;- poststrat_df %&gt;% mutate( Posi = 1:n()) temp &lt;- filter(temp, dam == &quot;05&quot;) %&gt;% select(n, Posi) n_filtered &lt;- temp$n temp_epred_mat &lt;- epred_mat[, temp$Posi] ## Estimando el CME mrp_estimates &lt;- temp_epred_mat %*% n_filtered / sum(n_filtered) (temp_dam05 &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 0.2787 0.0069 El resultado nos indica que la tasa de pobreza en la dam 05 es 0.28 10.3.4 Estimación para la dam2 == “05001” temp &lt;- poststrat_df %&gt;% mutate(Posi = 1:n()) temp &lt;- filter(temp, dam2 == &quot;05001&quot;) %&gt;% select(n, Posi) n_filtered &lt;- temp$n temp_epred_mat &lt;- epred_mat[, temp$Posi] ## Estimando el CME mrp_estimates &lt;- temp_epred_mat %*% n_filtered / sum(n_filtered) (temp_dam2_05001 &lt;- data.frame( mrp_estimate = mean(mrp_estimates), mrp_estimate_se = sd(mrp_estimates) ) ) %&gt;% tba() mrp_estimate mrp_estimate_se 0.4451 0.2022 El resultado nos indica que la tasa de pobreza en la dam2 05001 es 0.45 Después de comprender la forma en que se realiza la estimación de los dominios no observados procedemos el uso de la función Aux_Agregado que es desarrollada para este fin. (mrp_estimate_Ingresolp &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = NULL) ) %&gt;% tba() Nacional mrp_estimate mrp_estimate_se Nacional 0.3685 0.0261 De forma similar es posible obtener los resultados para las divisiones administrativas del país. mrp_estimate_dam &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = &quot;dam&quot;) tba(mrp_estimate_dam %&gt;% head(10)) dam mrp_estimate mrp_estimate_se 01 0.2502 0.0044 02 0.1772 0.0072 03 0.1943 0.0042 04 0.4289 0.0057 05 0.2787 0.0069 06 0.2178 0.0041 07 0.6073 0.0613 08 0.2555 0.0049 09 0.2596 0.0050 10 0.3837 0.0146 mrp_estimate_dam2 &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = &quot;dam2&quot;) tba(mrp_estimate_dam2 %&gt;% head(10) ) dam2 mrp_estimate mrp_estimate_se 01001 0.2383 0.0057 01002 0.2510 0.0194 01003 0.3606 0.0195 01004 0.2125 0.0395 01005 0.2404 0.0125 01006 0.2703 0.0174 01007 0.2834 0.0155 01008 0.3687 0.0491 01009 0.2075 0.0296 01010 0.3459 0.0516 El mapa resultante es el siguiente Figura 10.3: Tasa de pobreza por dam2 mrp_estimate_etnia_anoes &lt;- Aux_Agregado(poststrat = poststrat_df, epredmat = epred_mat, byMap = c(&quot;dam&quot;,&quot;anoest&quot;,&quot;etnia&quot;)) ## Leer Shapefile del país ShapeSAE &lt;- read_sf(&quot;Recursos/Día3/Sesion2/Shape/MEX_dam.shp&quot;) %&gt;% rename(depto = dam) p1 &lt;- Aux_Maps( Shape = ShapeSAE, dat_df = mrp_estimate_etnia_anoes%&gt;% rename(depto = dam), fnames = &quot;etnia&quot;, cnames = &quot;anoest&quot;, brks = brks_lp, outPaht = &quot;Recursos/Día3/Sesion2/0Recursos/Mosaico.png&quot; ) "],["día-3---sesión-3--estimación-del-índice-de-privación-multidimensional-ipm.html", "Capítulo 11 Día 3 - Sesión 3- Estimación del Índice de Privación Multidimensional (IPM)", " Capítulo 11 Día 3 - Sesión 3- Estimación del Índice de Privación Multidimensional (IPM) La pobreza es, y ha sido, uno de los temas principales en las agendas nacionales e internacionales de los países durante décadas. Un ejemplo reciente es el primer objetivo de la agenda 2030 para el Desarrollo Sostenible (ODS): “Poner fin a la pobreza en todas sus formas en todo el mundo”, así como su indicador 1.2.2 que mide “la proporción de hombres, mujeres y niños de todas las edades que viven en pobreza en todas sus dimensiones según las definiciones nacionales” Tradicionalmente los organismos nacionales e internacionales exigen la medida de pobreza unidimensional basada en ingresos y/o gastos. La pobreza es un fenómeno complejo que debe ser analizado considerando un conjunto de factores y no solo el monetario. En está ocasión se aborda el problema multidimensional de la pobreza utilizando métodos de áreas pequeñas proporcionando una estimación del índice de privación multidimensional (IPM) en Colombia. "],["índice-de-pobreza-multidimensional.html", "11.1 Índice de Pobreza Multidimensional", " 11.1 Índice de Pobreza Multidimensional El Índice de Pobreza Multidimensional (IPM) es una medida que captura la pobreza desde múltiples dimensiones. Se calcula utilizando ponderaciones y umbrales en función de diferentes variables o indicadores que reflejan aspectos diversos de la calidad de vida. Ahora el IPM es un caso particular de la metodología de la tasa de pobreza ajustada FGT (Foster, Greer y Thorbecke, 1984) de medidas de pobreza unidimensionales. Al igual que cada medida FGT se puede ver como la media de un vector apropiado construido a partir de los datos originales y censurado usando la línea de pobreza, la tasa de pobreza ajustada es la media del vector de puntuación de privación censurado. \\[ IPM = \\frac{1}{N}\\sum_{i=1}^{N}c_i(z) \\] donde, \\(N\\) es el número de individuos u hogares en la población y \\(c_i(z)\\) es el puntaje de privación censurado de la observación \\(i\\) que esta dado como: \\[ c_{i}\\left(z\\right)=\\begin{cases} q_i &amp; \\text{si } q_i\\ge z\\\\ 0 &amp; \\text{si } q &lt; z \\end{cases} \\] con \\[ q_i = \\sum_{k=1}^{K} w_k \\cdot y_{i}^{k} \\] Donde: \\(K\\) es el número de dimensiones o indicadores de la privación. \\(w_k\\) es el ponderador asociado a la dimensión \\(k\\). \\(y_{i}^{k}\\) es una variable binaria que toma el valor \\(1\\) si el individuo \\(i\\) esta privado en la dimensión \\(k\\) y \\(0\\) en el caso contrario. \\(z\\) es el umbral para considerar a alguien con multiples privaciones. Una segunda forma de ver es en términos de índices parciales, es decir, medidas que proporcionan información básica sobre un solo aspecto de la pobreza. A continuación vemos cada uno de estos componentes: Headcount Ratio (H) Este componente mide la proporción de personas que están privadas en al menos una de las dimensiones consideradas. Matemáticamente, \\(H\\) se calcula como la proporción entre el número de personas privadas y la población total: \\[ H = \\frac{1}{N} \\sum_{i=1}^{N} I\\left( q_{i} \\ge z \\right)= \\frac{N\\left(z\\right)}{N} \\] donde \\(N\\left(z\\right) = \\sum_{i=1}^{N} I\\left( q_{i} \\ge z \\right)\\) Intensity of Deprivation (A) Este componente mide la intensidad o gravedad promedio de la privación entre aquellos que están privados. Matemáticamente, \\(A\\) se calcula como el promedio de los indicadoras \\(y_{i}^{k}\\) para aquellos hogares o personas que están privados: \\[ A=\\sum_{i=1}^{N}\\frac{c_{i}\\left(z\\right)}{N\\left(z\\right)} \\] Luego, el Índice de Pobreza Multidimensional (IPM) se expresa como: \\[ IPM = H \\times A \\] reemplazando las \\(H\\) y \\(A\\) por sus respectivas ecuaciones se tiene que: \\[ IPM=\\frac{N\\left(z\\right)}{N}\\times\\sum_{i=1}^{N}\\frac{c_{i}\\left(z\\right)}{N\\left(z\\right)}=\\frac{1}{N}\\sum_{i=1}^{N}c_{i}\\left(z\\right) \\] "],["estimación-del-modelo-de-unidad-para-variables-binarias.html", "11.2 Estimación del modelo de unidad para variables Binarias", " 11.2 Estimación del modelo de unidad para variables Binarias En muchas aplicaciones, la variable de interés en áreas pequeñas puede ser binaria, esto es \\(y_{dj} = 0\\) o \\(1\\) que representa la ausencia (o no) de una característica específica. Para este caso, la estimación objetivo en cada dominio \\(d = 1,\\cdots , D\\) es la proporción \\(\\bar{Y}_d = \\pi_d =\\frac{1}{N_d}\\sum_{i=1}^{N_d}y_{di}\\) de la población que tiene esta característica, donde \\(N=\\sum_{d=1}^{D}N_d\\) y siendo \\(\\pi_{di}\\) la probabilidad de que una determinada unidad \\(i\\) en el dominio \\(d\\) obtenga el valor \\(1\\). Bajo este escenario, el \\(\\theta_{di}\\) con una función de enlace logit se define como: \\[ logit(\\pi_{di}) = \\log \\left(\\frac{\\pi_{di}}{1-\\pi_{di}}\\right) = \\eta_{di} = \\boldsymbol{x}_{di}^{T}\\boldsymbol{\\beta} + u_{d} \\] con \\(i=1,\\cdots,N_d\\), \\(d=1,\\cdots,D\\), \\(\\boldsymbol{\\beta}\\) un vector de parámetros de efecto fijo, y \\(u_d\\) el efecto aleatorio especifico del área para el dominio \\(d\\) con \\(u_d \\sim N\\left(0,\\sigma^2_u \\right)\\). \\(u_d\\) son independiente y \\(y_{di}\\mid u_d \\sim Bernoulli(\\pi_{di})\\) con \\(E(y_{di}\\mid u_d)=\\pi_{di}\\) y \\(Var(y_{di}\\mid u_d)=\\sigma_{di}^2=\\pi_{di}(1-\\pi_{di})\\). Además, \\(\\boldsymbol{x}_{di}^T\\) representa el vector \\(p\\times 1\\) de valores de \\(p\\) variables auxiliares. Entonces, \\(\\pi_{di}\\) se puede escribir como \\[ \\pi_{di} = \\frac{\\exp(\\boldsymbol{x}_{di}^T\\boldsymbol{\\beta} + u_{d})}{1+ \\exp(\\boldsymbol{x}_{di}^T\\boldsymbol{\\beta} + u_{d})} \\] De está forma podemos definir distribuciones previas \\[ \\begin{eqnarray*} \\beta_k &amp; \\sim &amp; N(0, 10000)\\\\ \\sigma^2_u &amp;\\sim &amp; IG(0.0001,0.0001) \\end{eqnarray*} \\] El modelo se debe estimar para cada una de las dimensiones. Obejtivo Estimar la proporción de personas que presentan la \\(k-\\)ésima carencia, es decir, \\[ P_d = \\frac{\\sum_{U_d}c_{di}(z)}{N_d} \\] Note que, \\[ \\begin{equation*} \\bar{Y}_d = P_d = \\frac{\\sum_{s_d}c_{di}(z) + \\sum_{s^c_d}c_{di}(z)}{N_d} \\end{equation*} \\] Ahora, el estimador de \\(P\\) esta dado por: \\[ \\hat{P}_d = \\frac{\\sum_{s_d}c_{di}(z) + \\sum_{s^c_d}\\hat{c}_{di}(z)}{N_d} \\] donde \\[ \\hat{c}_{di}\\left(z\\right)=\\begin{cases} \\hat{q}_{di} &amp; \\text{si } \\hat{q}_{di}\\ge z\\\\ 0 &amp; \\text{si } \\hat{q}_{di} &lt; z \\end{cases} \\] con \\[ \\hat{q}_{di} = \\sum_{k=1}^{K} w_k \\cdot \\hat{y}_{di}^{k} \\] \\[ \\hat{y}_{di}^{k} = E_{\\mathscr{M}}\\left(y_{di}^{k}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right) \\], con \\(\\mathscr{M}\\) la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, \\[ \\hat{P}_d = \\frac{\\sum_{U_{d}}\\hat{q}_{di}}{N_d} \\] "],["estimación-de-pik_di.html", "11.3 Estimación de \\(\\pi^{k}_{di}\\)", " 11.3 Estimación de \\(\\pi^{k}_{di}\\) La estimación de \\(\\pi^{k}_{di}\\) refleja la probabilidad de que una unidad específica \\(i\\) en el dominio \\(d\\) obtenga el valor 1 en la dimensión \\(k\\). Para llevar a cabo esta estimación, seguimos el siguiente procedimiento: \\[ \\bar{Y}^{k}_d = \\pi^{k}_d = \\frac{1}{N_d} \\sum_{i=1}^{N_d} y^{k}_{di} \\] Aquí, \\(y^{k}_{di}\\) puede tomar los valores 0 ó 1, representando la ausencia (o no) de una característica específica. Ahora, podemos dividir la suma en dos partes: \\(s_d\\), que representa las unidades observadas en una muestra, y \\(s_d^c\\), que son las unidades no observados. Por lo tanto, \\[ \\begin{equation*} \\bar{Y}^{k}_d = \\pi^{k}_d = \\frac{1}{N_d}\\left(\\sum_{s_d}y^{k}_{di} + \\sum_{s^c_d}y^{k}_{di} \\right) \\end{equation*} \\] Ahora, suponga que mediante un modelo de unidad es posible realizar la predicción de \\(y^{k}_{di}\\) para las unidades no observadas. De esta manera, el estimador de \\(\\pi^{k}_d\\) se expresa como: \\[ \\hat{\\pi}^{k}_d = \\frac{1}{N_d}\\left( \\sum_{s_d}y^{k}_{di} + \\sum_{s^c_d}\\hat{y}^{k}_{di} \\right) \\] Donde, \\[\\hat{y}^{k}_{di}=E_{\\mathscr{M}}\\left(y^{k}_{di}\\mid\\boldsymbol{x}_{d},\\boldsymbol{\\beta}\\right)\\] Aquí, \\(\\mathscr{M}\\) hace referencia a la medida de probabilidad inducida por el modelo. Sin embargo, en la práctica, individualizar a las unidades observadas y no observadas en una encuesta de hogares puede ser difícil. Por lo tanto, una alternativa es realizar la predicción \\(\\hat{y}^{k}_{di}\\) para todas las observaciones en el universo. De esta manera, la estimación \\(\\hat{\\pi}^{k}_d\\) se simplifica a: \\[ \\hat{\\pi}^{k}_d = \\frac{1}{N_d}\\sum_{i=1}^{N_d}\\hat{y}^{k}_{di} \\] Este enfoque permite estimar la probabilidad \\(\\pi^{k}_d\\) en el dominio \\(d\\) en la dimensión \\(k\\) utilizando predicciones y datos disponibles en lugar de contar con información individual detallada para todos los casos. "],["pedicción-de-los-hard-estimates.html", "11.4 Pedicción de los Hard estimates", " 11.4 Pedicción de los Hard estimates Hobza y Morales (2016) definen los “hard estimates” como valores binarios (0 o 1) que indican de manera precisa si un individuo tiene o no una característica específica en relación con cada indicador de pobreza multidimensional. Estas estimaciones reflejan la naturaleza binaria de la información, facilitando el cálculo de indicadores y tasas de incidencia de pobreza. Estas estimaciones desempeñan un papel clave en la determinación de la incidencia de la pobreza multidimensional, ya que indican la presencia o ausencia de privaciones en indicadores específicos para cada individuo. Esto plantea un desafío en la estimación, ya que no se trata solo de obtener valores finales, sino de precisar si las características están presentes o no en indicadores faltantes. Con la definición de los hard estimates, y sabiendo que la estimación de \\(\\pi^{k}_{di}\\) refleja la probabilidad de que una unidad específica \\(i\\) en el dominio \\(d\\) obtenga el valor 1 en la dimensión \\(k\\) se define \\(\\hat{y}^{k}_{di} \\sim Bernoulli(\\hat{\\pi}^{k}_{di})\\) "],["estimación-puntual-del-índice-de-pobreza-multidimensional-ipm.html", "11.5 Estimación Puntual del Índice de Pobreza Multidimensional (IPM)", " 11.5 Estimación Puntual del Índice de Pobreza Multidimensional (IPM) Supongamos que el Índice de Pobreza Multidimensional está compuesto por \\(K\\) dimensiones o indicadores para cada individuo \\(i\\) en el censo. El procedimiento propuesto para estimar el IPM es el siguiente: Utilice los datos de la muestra para ajustar un modelo logit Bernoulli a nivel de unidad para cada indicador. Esto se logra mediante el uso del algoritmo de Markov Chain Monte Carlo (MCMC) con \\(L\\) iteraciones. Para cada dimensión \\(k\\) a la cual se le para ajustó un modelo logit Bernoulli a nivel de unidad con \\(L\\) iteraciones, realice la predicción de los valores \\(\\hat{y}^{k}_{di}\\) para cada individuo en el censo. Esto generará \\(L\\) realizaciones aleatorias de \\(\\hat{y}^{k}_{di}\\). Denotemos como \\(\\hat{y}_{di}^{kl}\\) a la \\(l\\)-ésima realización aleatoria de la dimensión \\(k\\) para el individuo \\(i\\) en el dominio \\(d\\). Calculamos \\(q_{di}^{l} = \\sum_{k=1}^{K} w_k \\cdot y_{di}^{kl}\\). Luego, podemos calcular \\(H_d^{l}\\), \\(A_d^{l}\\) y \\(IPM_{d}^{l}\\) utilizando las ecuaciones: \\[ IPM_{d}^{l} = \\frac{1}{N_d}\\sum_{i=1}^{N_{d}}c_{di}^{l}\\left(z\\right) \\] \\[ H_d^{l}=\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}I\\left(q_{di}^{l}\\ge z\\right)=\\frac{N_{d}^{l}\\left(z\\right)}{N_{d}} \\] y \\[ A_{d}^{l}=\\sum_{i=1}^{N_{d}}\\frac{c_{di}^{l}\\left(z\\right)}{N^{l}_{d}\\left(z\\right)} \\] La estimación puntual de \\(H_d\\), \\(A_{d}\\) y \\(IPM_{d}\\) en cada área pequeña \\(d\\) se calcula tomando el promedio sobre cada una de las \\(L\\) iteraciones: \\[ \\hat{H}_d = \\frac{1}{L}\\sum_{l=1}^{L}H_d^l, \\] \\[ \\hat{A}_d = \\frac{1}{L}\\sum_{l=1}^{L}A_d^l \\] y \\[ \\widehat{IPM}_d = \\frac{1}{L}\\sum_{l=1}^{L}IPM_d^l \\] Dada que el modelo se estimó usando el algoritmo MCMC, es posible tener la estimación del error de estimación, de esta forma: \\[ \\widehat{Var}(\\hat{H}_d) = \\frac{1}{L}\\sum_{l=1}^{L}\\left( H^{l}_{d} -\\hat{H}_d \\right)^2, \\] \\[ \\widehat{Var}(\\hat{A}_d) = \\frac{1}{L}\\sum_{l=1}^{L}\\left( A^{l}_{d} -\\hat{A}_d \\right)^2 \\] y \\[ \\widehat{Var}(\\widehat{IPM}_d) = \\frac{1}{L}\\sum_{l=1}^{L}\\left( IPM_d^{l} -\\widehat{IPM}_d \\right)^2 \\] "],["aplicación-índice-de-pobreza-multidimensional-en-méxico..html", "11.6 Aplicación: Índice de Pobreza Multidimensional en México.", " 11.6 Aplicación: Índice de Pobreza Multidimensional en México. Nos centramos en la incidencia de la pobreza multidimensional descrito previamente. En este caso, requerimos \\(K = 9\\) indicadores que se miden como privaciones: \\(y_{di}^{k} = 1\\) si la persona tiene la privación y \\(y_{di}^{k} = 0\\) si la persona no ha tenido la privación. El índice requiere información para cada individuo \\(i = 1, \\ldots, N_d\\) en los dominios \\(d = 1, \\ldots, D\\), donde \\(N_d\\) denota el tamaño de la población del dominio \\(d\\). La función indicadora \\(I(\\cdot)\\) es igual a 1 cuando se cumple la condición \\(q_{di} \\ge z\\). Para este estudio, utilizamos el valor de 0.4 para \\(z\\), es decir, \\(I(\\cdot)\\) es igual a 1 cuando \\(q_{di} \\ge 0.4\\). \\(q_{di}\\) es una cantidad ponderada que considera los \\(K = 9\\) indicadores que conforman el índice. El valor de \\(q_{di}\\) el dominio \\(d\\) se calcula como: \\[ q_{di} = \\frac{1}{9}(y_{di}^{1} + y_{di}^{2} + y_{di}^{3} + y_{di}^{4} + y_{di}^{5} + y_{di}^{6} + y_{di}^{7} + y_{di}^{8} + y_{di}^{9}) \\] Donde: \\(y_{di}^{1}\\) = Privación en material de construcción de la vivienda \\(y_{di}^{2}\\) = Hacinamiento en el hogar. \\(y_{di}^{3}\\) = Privación en material de pisos. \\(y_{di}^{3}\\) = Privación en material de techos. \\(y_{di}^{3}\\) = Privación en el combustible. \\(y_{di}^{4}\\) = Privación en el servicio energía eléctrica. \\(y_{di}^{5}\\) = Privación en saneamiento. \\(y_{di}^{6}\\) = Privación de acceso al agua potable. \\(y_{di}^{8}\\) = Privación de la educación. \\(y_{di}^{9}\\) = Privación en paredes. 11.6.1 Procesamiento del modelo en R. El proceso inicia con el cargue de las librerías. library(patchwork) library(lme4) library(tidyverse) library(rstan) library(rstanarm) library(magrittr) library(bayesplot) library(posterior) Los datos de la encuesta y el censo han sido preparados previamente, la información sobre la cual realizaremos la predicción corresponde a Colombia en el 2019 encuesta_ipm &lt;- readRDS(&quot;Recursos/Día3/Sesion3/Data/encuestaMEX20_nbi.Rds&quot;) statelevel_predictors_df &lt;- readRDS(&quot;Recursos/Día3/Sesion3/Data/predictors_satelital_dam.rds&quot;) byAgrega &lt;- c(&quot;dam&quot;, &quot;area&quot;, &quot;sexo&quot;, &quot;etnia&quot;, &quot;anoest&quot;, &quot;edad&quot; ) names_ipm &lt;- grep(pattern = &quot;nbi&quot;, names(encuesta_ipm),value = TRUE) encuesta_df &lt;- map(setNames(names_ipm,names_ipm), function(y){ encuesta_ipm$temp &lt;- encuesta_ipm[[y]] encuesta_ipm %&gt;% group_by_at(all_of(byAgrega)) %&gt;% summarise(n = n(), yno = sum(temp), ysi = n - yno, .groups = &quot;drop&quot;) %&gt;% inner_join(statelevel_predictors_df) }) Privación en material de construcción de la vivienda Tabla 11.1: nbi_matviv_ee dam area sexo etnia anoest edad n yno ysi luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 05 1 1 3 3 2 825 2 823 100.0359 96.2598 101.1301 98.0690 126.5907 123.6785 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 0.0056 0.5310 0.0474 0.0326 0.0523 0.4265 0.0058 0.0948 0.0993 0.2523 0.0472 0.2140 0.0421 05 1 2 3 3 2 819 9 810 100.0359 96.2598 101.1301 98.0690 126.5907 123.6785 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 0.0056 0.5310 0.0474 0.0326 0.0523 0.4265 0.0058 0.0948 0.0993 0.2523 0.0472 0.2140 0.0421 08 1 2 3 3 2 787 6 781 104.0508 107.8007 108.6009 112.2464 130.4471 133.5435 0.8727 0.0097 0.5057 0.2494 0.2098 0.2059 0.0748 0.2632 0.4235 0.1646 0.0501 0.0971 0.0153 0.8879 0.0852 0.0772 0.0555 0.4189 0.0171 0.1695 0.3581 0.2461 0.0579 0.2010 0.0323 08 1 1 3 3 2 697 9 688 104.0508 107.8007 108.6009 112.2464 130.4471 133.5435 0.8727 0.0097 0.5057 0.2494 0.2098 0.2059 0.0748 0.2632 0.4235 0.1646 0.0501 0.0971 0.0153 0.8879 0.0852 0.0772 0.0555 0.4189 0.0171 0.1695 0.3581 0.2461 0.0579 0.2010 0.0323 05 1 2 3 3 4 627 5 622 100.0359 96.2598 101.1301 98.0690 126.5907 123.6785 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 0.0056 0.5310 0.0474 0.0326 0.0523 0.4265 0.0058 0.0948 0.0993 0.2523 0.0472 0.2140 0.0421 25 1 1 3 3 2 622 9 613 100.8792 108.9958 99.5980 103.6310 97.6080 98.0501 0.7769 0.0099 0.5092 0.2489 0.2026 0.2094 0.0916 0.2618 0.3947 0.1984 0.0575 0.0861 0.0208 0.7730 0.0721 0.0885 0.1084 0.4526 0.0204 0.0328 0.0477 0.2889 0.0571 0.2744 0.0300 Hacinamiento dam area sexo etnia anoest edad n yno ysi luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 05 1 1 3 3 2 825 311 514 100.0359 96.2598 101.1301 98.0690 126.5907 123.6785 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 0.0056 0.5310 0.0474 0.0326 0.0523 0.4265 0.0058 0.0948 0.0993 0.2523 0.0472 0.2140 0.0421 05 1 2 3 3 2 819 357 462 100.0359 96.2598 101.1301 98.0690 126.5907 123.6785 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 0.0056 0.5310 0.0474 0.0326 0.0523 0.4265 0.0058 0.0948 0.0993 0.2523 0.0472 0.2140 0.0421 08 1 2 3 3 2 787 340 447 104.0508 107.8007 108.6009 112.2464 130.4471 133.5435 0.8727 0.0097 0.5057 0.2494 0.2098 0.2059 0.0748 0.2632 0.4235 0.1646 0.0501 0.0971 0.0153 0.8879 0.0852 0.0772 0.0555 0.4189 0.0171 0.1695 0.3581 0.2461 0.0579 0.2010 0.0323 08 1 1 3 3 2 697 270 427 104.0508 107.8007 108.6009 112.2464 130.4471 133.5435 0.8727 0.0097 0.5057 0.2494 0.2098 0.2059 0.0748 0.2632 0.4235 0.1646 0.0501 0.0971 0.0153 0.8879 0.0852 0.0772 0.0555 0.4189 0.0171 0.1695 0.3581 0.2461 0.0579 0.2010 0.0323 05 1 2 3 3 4 627 155 472 100.0359 96.2598 101.1301 98.0690 126.5907 123.6785 0.9182 0.0085 0.5018 0.2467 0.2079 0.2061 0.0764 0.2274 0.4522 0.1677 0.0467 0.0189 0.0056 0.5310 0.0474 0.0326 0.0523 0.4265 0.0058 0.0948 0.0993 0.2523 0.0472 0.2140 0.0421 25 1 1 3 3 2 622 330 292 100.8792 108.9958 99.5980 103.6310 97.6080 98.0501 0.7769 0.0099 0.5092 0.2489 0.2026 0.2094 0.0916 0.2618 0.3947 0.1984 0.0575 0.0861 0.0208 0.7730 0.0721 0.0885 0.1084 0.4526 0.0204 0.0328 0.0477 0.2889 0.0571 0.2744 0.0300 11.6.2 Definiendo el modelo multinivel. Para cada dimensión que compone el IPM se ajusta el siguiente modelo mostrado en el script. En este código se incluye el uso de la función future_map que permite procesar en paralelo cada modelo O puede compilar cada por separado. library(rstanarm) plan(multisession, workers = 4) formula_mod &lt;- formula( cbind(yno, ysi) ~ (1 | dam) + (1 | etnia) + (1 | edad) + (1 | anoest) + sexo + area + tasa_desocupacion + luces_nocturnas + modificacion_humana ) for(xdat in names(encuesta_df)) { fit &lt;- stan_glmer( formula = formula_mod, family = binomial(link = &quot;logit&quot;), data = encuesta_df[[xdat]], cores = 4, chains = 4, iter = 1000 ) saveRDS(object = fit, paste0(&quot;/Recursos/Día3/Sesion3/Data/&quot;, xdat, &quot;.rds&quot;)) } Terminado la compilación de los modelos después de realizar validaciones sobre esto, pasamos hacer las predicciones en el censo. 11.6.3 Proceso de estimación y predicción Los modelos fueron compilados de manera separada, por tanto, disponemos de un objeto .rds por cada dimensión del IPM fit_agua &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_agua_ee.rds&quot;) fit_combustible &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_combus_ee.rds&quot;) fit_techo &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_techo_ee.rds&quot;) fit_energia &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_elect_ee.rds&quot;) fit_hacinamiento &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_hacina_ee.rds&quot;) fit_paredes &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_pared_ee.rds&quot;) fit_material &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_matviv_ee.rds&quot;) fit_saneamiento &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_saneamiento_ee.rds&quot;) fit_piso &lt;- readRDS(file = &quot;Recursos/Día3/Sesion3/Data/nbi_piso_ee.rds&quot;) Ahora, debemos leer la información del censo y crear los post-estrato censo_ipm &lt;- readRDS(&quot;Recursos/Día3/Sesion3/Data/censo_mrp_dam.rds&quot;) poststrat_df &lt;- censo_ipm %&gt;% group_by_at(byAgrega) %&gt;% summarise(n = sum(n), .groups = &quot;drop&quot;) %&gt;% arrange(desc(n)) tba(head(poststrat_df)) dam area sexo etnia anoest edad n 15 1 1 3 3 2 1140651 15 1 2 3 3 2 1118896 15 1 2 3 3 3 890981 15 1 1 3 3 3 816829 15 1 2 3 3 4 748706 15 1 1 3 3 4 715064 Para realizar la predicción en el censo debemos incluir la información auxiliar poststrat_df &lt;- inner_join(poststrat_df, statelevel_predictors_df) dim(poststrat_df) ## [1] 8755 38 Privación de acceso al agua potable. temp &lt;- poststrat_df epred_mat_agua &lt;- posterior_epred( fit_agua, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Privación de acceso al combustible para cocinar. epred_mat_combustible &lt;- posterior_epred( fit_combustible, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Privación en material de los techo. epred_mat_techo &lt;- posterior_epred( fit_techo, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Acceso al servicio energía eléctrica. epred_mat_energia &lt;- posterior_epred( fit_energia, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Hacinamiento en el hogar. epred_mat_hacinamiento &lt;- posterior_epred( fit_hacinamiento, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Privación el material de las paredes. epred_mat_paredes &lt;- posterior_epred( fit_paredes, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Privación en material de construcción de la vivienda epred_mat_material &lt;- posterior_epred( fit_material, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Privación en piso epred_mat_piso &lt;- posterior_epred( fit_piso, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Privación en saneamiento. epred_mat_saneamiento &lt;- posterior_epred( fit_saneamiento, newdata = temp, type = &quot;response&quot;, allow.new.levels = TRUE ) Los resultados anteriores se deben procesarse en términos de carencia (1) y no carencia (0) para la \\(k-esima\\) dimensión . Privación de acceso al agua potable. epred_mat_agua_dummy &lt;- rbinom(n = nrow(epred_mat_agua) * ncol(epred_mat_agua) , 1, epred_mat_agua) epred_mat_agua_dummy &lt;- matrix( epred_mat_agua_dummy, nrow = nrow(epred_mat_agua), ncol = ncol(epred_mat_agua) ) Privación de acceso al combustible para cocinar. epred_mat_combustible_dummy &lt;- rbinom(n = nrow(epred_mat_combustible) * ncol(epred_mat_combustible) , 1, epred_mat_combustible) epred_mat_combustible_dummy &lt;- matrix( epred_mat_combustible_dummy, nrow = nrow(epred_mat_combustible), ncol = ncol(epred_mat_combustible) ) Acceso al servicio energía eléctrica epred_mat_energia_dummy &lt;- rbinom(n = nrow(epred_mat_energia) * ncol(epred_mat_energia) , 1, epred_mat_energia) epred_mat_energia_dummy &lt;- matrix( epred_mat_energia_dummy, nrow = nrow(epred_mat_energia), ncol = ncol(epred_mat_energia) ) Hacinamiento en el hogar. epred_mat_hacinamiento_dummy &lt;- rbinom( n = nrow(epred_mat_hacinamiento) * ncol(epred_mat_hacinamiento) , 1, epred_mat_hacinamiento ) epred_mat_hacinamiento_dummy &lt;- matrix( epred_mat_hacinamiento_dummy, nrow = nrow(epred_mat_hacinamiento), ncol = ncol(epred_mat_hacinamiento) ) Privación el material de las paredes. epred_mat_paredes_dummy &lt;- rbinom(n = nrow(epred_mat_paredes) * ncol(epred_mat_paredes) , 1, epred_mat_paredes) epred_mat_paredes_dummy &lt;- matrix( epred_mat_paredes_dummy, nrow = nrow(epred_mat_paredes), ncol = ncol(epred_mat_paredes) ) Privación en material de construcción de la vivienda epred_mat_material_dummy &lt;- rbinom(n = nrow(epred_mat_material) * ncol(epred_mat_material) , 1, epred_mat_material) epred_mat_material_dummy &lt;- matrix( epred_mat_material_dummy, nrow = nrow(epred_mat_material), ncol = ncol(epred_mat_material) ) Privación en saneamiento. epred_mat_saneamiento_dummy &lt;- rbinom(n = nrow(epred_mat_saneamiento) * ncol(epred_mat_saneamiento) , 1, epred_mat_saneamiento) epred_mat_saneamiento_dummy &lt;- matrix( epred_mat_saneamiento_dummy, nrow = nrow(epred_mat_saneamiento), ncol = ncol(epred_mat_saneamiento) ) Privación en material de los techo. epred_mat_techo_dummy &lt;- rbinom(n = nrow(epred_mat_techo) * ncol(epred_mat_techo) , 1, epred_mat_techo) epred_mat_techo_dummy &lt;- matrix( epred_mat_techo_dummy, nrow = nrow(epred_mat_techo), ncol = ncol(epred_mat_techo) ) Privación en el material de piso epred_mat_piso_dummy &lt;- rbinom(n = nrow(epred_mat_piso) * ncol(epred_mat_piso) , 1, epred_mat_piso) epred_mat_piso_dummy &lt;- matrix( epred_mat_techo_dummy, nrow = nrow(epred_mat_piso), ncol = ncol(epred_mat_piso) ) Con las variables dummy creadas es posible estimar el IPM chain_q &lt;- (1 / 9) * ( epred_mat_material_dummy + epred_mat_combustible_dummy + epred_mat_techo_dummy + epred_mat_piso_dummy + epred_mat_paredes_dummy + epred_mat_saneamiento_dummy + epred_mat_energia_dummy + epred_mat_agua_dummy + epred_mat_hacinamiento_dummy) saveRDS(chain_q, &quot;Recursos/Día3/Sesion3/Data/chain_q.rds&quot;) Ahora, es posible tener el calculo de \\(I\\left( q_{di}^{l} \\ge z \\right)\\), tomando como umbral \\(z=0.4\\). chain_q &lt;- readRDS(&quot;Recursos/Día3/Sesion3/Data/chain_q.rds&quot;) chain_Ind &lt;- chain_q chain_Ind[chain_Ind &lt; 0.4] &lt;- 0 chain_Ind[chain_Ind != 0] &lt;- 1 seguidamente calculamos \\(c_{di}^{l}\\left(z\\right)\\) chain_ci &lt;- matrix(0,nrow = nrow(chain_q), ncol = ncol(chain_q)) chain_ci[chain_Ind == 1] &lt;- chain_q[chain_Ind == 1] datos&lt;-data.frame(t(chain_q[1:5,1:10]), t(chain_Ind[1:5,1:10]), t(chain_ci[1:5,1:10]), N = censo_ipm$n[1:10] ) colnames(datos) &lt;- c(paste0(&quot;q&quot;,1:5), paste0(&quot;Ind&quot;,1:5),paste0(&quot;c&quot;,1:5),&quot;N&quot;) tba(datos, &quot;Cadenas obtenidas&quot;) Tabla 11.2: Cadenas obtenidas q1 q2 q3 q4 q5 Ind1 Ind2 Ind3 Ind4 Ind5 c1 c2 c3 c4 c5 N 0.0000 0.0000 0.1111 0.0000 0.0000 0 0 0 0 0 0 0 0 0 0 590 0.0000 0.1111 0.2222 0.2222 0.0000 0 0 0 0 0 0 0 0 0 0 10 0.1111 0.1111 0.1111 0.1111 0.2222 0 0 0 0 0 0 0 0 0 0 442 0.0000 0.1111 0.2222 0.0000 0.1111 0 0 0 0 0 0 0 0 0 0 11 0.0000 0.0000 0.2222 0.0000 0.0000 0 0 0 0 0 0 0 0 0 0 65 0.0000 0.1111 0.0000 0.0000 0.0000 0 0 0 0 0 0 0 0 0 0 3 0.1111 0.1111 0.2222 0.1111 0.1111 0 0 0 0 0 0 0 0 0 0 68 0.0000 0.1111 0.2222 0.1111 0.1111 0 0 0 0 0 0 0 0 0 0 45 0.0000 0.1111 0.2222 0.0000 0.0000 0 0 0 0 0 0 0 0 0 0 559 0.1111 0.0000 0.2222 0.1111 0.1111 0 0 0 0 0 0 0 0 0 0 10 "],["estimación-de-h-a-e-ipm.html", "11.7 Estimación de \\(H\\), \\(A\\) e \\(IPM\\)", " 11.7 Estimación de \\(H\\), \\(A\\) e \\(IPM\\) Para este proceso debemos realizar sumas ponderadas, dado que cada registro de la base de datos representa un grupo de observaciones con las mismas características. numIPM &lt;- t(chain_ci) %&gt;% as.data.frame() %&gt;% mutate_all(~ . * poststrat_df$n) %&gt;% as.matrix() chain_N &lt;- t(chain_Ind) %&gt;% as.data.frame() %&gt;% mutate_all(~ . * poststrat_df$n) %&gt;% as.matrix() IPM_l &lt;- colSums(numIPM)/sum(censo_ipm$n) Nz_l &lt;- colSums(chain_N) H_l &lt;- Nz_l/sum(censo_ipm$n) A_l &lt;- colSums(numIPM)/Nz_l Tabla 11.3: l-iteraciones IPM_l H_l A_l HA_l l = 1 0.0067 0.0147 0.4526 0.0067 l = 2 0.0038 0.0083 0.4523 0.0038 l = 3 0.0063 0.0136 0.4675 0.0063 l = 4 0.0041 0.0086 0.4733 0.0041 l = 5 0.0057 0.0122 0.4638 0.0057 l = 6 0.0075 0.0162 0.4651 0.0075 l = 7 0.0086 0.0190 0.4523 0.0086 l = 8 0.0050 0.0106 0.4744 0.0050 l = 9 0.0057 0.0125 0.4587 0.0057 l = 10 0.0074 0.0161 0.4610 0.0074 Por último se realiza las estimaciones puntuales y varianza para \\(H\\), \\(A\\) y \\(IPM\\), esto es: estimacion &lt;- data.frame(H = mean(H_l), H_sd = sd(H_l), A = mean(A_l), A_sd = sd(A_l), IPM = mean(IPM_l), IPM_sd = sd(IPM_l)) Tabla 11.4: Estimaciones Nacionales H H_sd A A_sd IPM IPM_sd 0.0128 0.0023 0.4639 0.0082 0.006 0.0011 "],["estimaciones-desagregadas-del-ipm.html", "11.8 Estimaciones desagregadas del IPM", " 11.8 Estimaciones desagregadas del IPM Para realizar las estimaciones desagregadas se desarrollo una función que facilita el calculo, la estructura general el proceso es repetir el proceso anterior por subgrupos, por ejemplo, departamento (dam) source(&quot;Recursos/Día3/Sesion3/0Recursos//Estimar_ipm.R&quot;) ipm_dam &lt;- estime_IPM( poststrat = poststrat_df, chain_ci = chain_ci, chain_ind = chain_ind, byMap = &quot;dam&quot; ) %&gt;% data.frame() Tabla 11.5: Estimaciones por estados dam H H_sd A A_sd IPM IPM_sd 01 0.0002 0.0020 0.4533 0.0290 0.0001 0.0009 02 0.0004 0.0017 0.4562 0.0325 0.0002 0.0008 03 0.0024 0.0049 0.4646 0.0366 0.0011 0.0022 04 0.0207 0.0118 0.4719 0.0304 0.0097 0.0055 05 0.0003 0.0016 0.4606 0.0386 0.0001 0.0007 06 0.0019 0.0044 0.4613 0.0356 0.0009 0.0020 07 0.0646 0.0268 0.4628 0.0210 0.0299 0.0124 08 0.0017 0.0022 0.4572 0.0326 0.0008 0.0010 09 0.0003 0.0014 0.4523 0.0246 0.0001 0.0006 10 0.0017 0.0036 0.4604 0.0356 0.0008 0.0016 11 0.0020 0.0046 0.4567 0.0324 0.0009 0.0021 12 0.0637 0.0217 0.4596 0.0137 0.0293 0.0100 13 0.0057 0.0068 0.4559 0.0297 0.0026 0.0031 14 0.0006 0.0020 0.4558 0.0320 0.0003 0.0009 15 0.0031 0.0045 0.4590 0.0301 0.0014 0.0020 16 0.0068 0.0068 0.4564 0.0280 0.0031 0.0031 17 0.0048 0.0050 0.4550 0.0259 0.0022 0.0023 18 0.0032 0.0045 0.4599 0.0365 0.0015 0.0020 19 0.0005 0.0032 0.4579 0.0329 0.0002 0.0014 20 0.0700 0.0298 0.4674 0.0236 0.0327 0.0140 21 0.0131 0.0094 0.4530 0.0203 0.0059 0.0042 22 0.0013 0.0024 0.4533 0.0277 0.0006 0.0011 23 0.0269 0.0152 0.4792 0.0252 0.0128 0.0071 24 0.0418 0.0181 0.4835 0.0295 0.0202 0.0087 25 0.0011 0.0028 0.4583 0.0356 0.0005 0.0013 26 0.0020 0.0036 0.4627 0.0375 0.0009 0.0016 27 0.0100 0.0099 0.4545 0.0258 0.0046 0.0045 28 0.0009 0.0029 0.4607 0.0377 0.0004 0.0013 29 0.0007 0.0021 0.4505 0.0230 0.0003 0.0009 30 0.0299 0.0141 0.4640 0.0232 0.0139 0.0065 31 0.0142 0.0117 0.4719 0.0330 0.0067 0.0055 32 0.0007 0.0032 0.4567 0.0335 0.0003 0.0014 Otra estimación desagregada que es posible obtener es la combinación por departamento y sexo, para ellos se usa la sintaxis. ipm_dam_sexo &lt;- estime_IPM( poststrat = poststrat_df, chain_ci = chain_ci, chain_ind = chain_ind, byMap = c(&quot;dam&quot;, &quot;sexo&quot;) ) %&gt;% data.frame() Tabla 11.6: Estimaciones por estado y sexo dam sexo H H_sd A A_sd IPM IPM_sd 01 1 0.0001 0.0008 0.4540 0.0304 0.0000 0.0004 01 2 0.0002 0.0039 0.4556 0.0341 0.0001 0.0017 02 1 0.0005 0.0029 0.4578 0.0366 0.0002 0.0013 02 2 0.0004 0.0019 0.4560 0.0344 0.0002 0.0010 03 1 0.0023 0.0056 0.4667 0.0438 0.0011 0.0025 03 2 0.0024 0.0080 0.4647 0.0416 0.0011 0.0036 04 1 0.0220 0.0176 0.4717 0.0379 0.0104 0.0083 04 2 0.0194 0.0159 0.4723 0.0397 0.0091 0.0075 05 1 0.0003 0.0026 0.4622 0.0429 0.0001 0.0012 05 2 0.0002 0.0018 0.4599 0.0388 0.0001 0.0008 06 1 0.0022 0.0069 0.4628 0.0410 0.0010 0.0031 06 2 0.0016 0.0056 0.4621 0.0403 0.0007 0.0025 07 1 0.0651 0.0389 0.4631 0.0295 0.0301 0.0180 07 2 0.0641 0.0366 0.4630 0.0297 0.0297 0.0170 08 1 0.0018 0.0034 0.4578 0.0355 0.0008 0.0015 08 2 0.0016 0.0028 0.4580 0.0371 0.0007 0.0013 09 1 0.0003 0.0020 0.4537 0.0292 0.0001 0.0009 09 2 0.0003 0.0019 0.4534 0.0273 0.0001 0.0009 10 1 0.0018 0.0054 0.4620 0.0403 0.0008 0.0024 10 2 0.0016 0.0044 0.4616 0.0390 0.0007 0.0020 11 1 0.0022 0.0064 0.4564 0.0331 0.0010 0.0029 11 2 0.0018 0.0065 0.4568 0.0338 0.0008 0.0029 12 1 0.0657 0.0306 0.4601 0.0191 0.0302 0.0141 12 2 0.0617 0.0281 0.4595 0.0192 0.0283 0.0129 13 1 0.0057 0.0098 0.4563 0.0319 0.0026 0.0045 13 2 0.0057 0.0095 0.4557 0.0328 0.0026 0.0043 14 1 0.0007 0.0035 0.4572 0.0349 0.0003 0.0016 14 2 0.0005 0.0020 0.4564 0.0329 0.0002 0.0009 15 1 0.0033 0.0066 0.4603 0.0363 0.0015 0.0030 15 2 0.0029 0.0062 0.4600 0.0366 0.0013 0.0028 16 1 0.0076 0.0101 0.4573 0.0323 0.0034 0.0046 16 2 0.0061 0.0091 0.4577 0.0328 0.0028 0.0041 17 1 0.0052 0.0075 0.4553 0.0308 0.0023 0.0034 17 2 0.0044 0.0066 0.4564 0.0304 0.0020 0.0030 18 1 0.0034 0.0070 0.4609 0.0406 0.0016 0.0031 18 2 0.0030 0.0058 0.4601 0.0396 0.0014 0.0027 19 1 0.0004 0.0027 0.4583 0.0346 0.0002 0.0012 19 2 0.0006 0.0058 0.4582 0.0367 0.0003 0.0026 20 1 0.0724 0.0434 0.4678 0.0317 0.0339 0.0204 20 2 0.0677 0.0406 0.4675 0.0320 0.0317 0.0192 21 1 0.0138 0.0136 0.4534 0.0244 0.0062 0.0062 21 2 0.0125 0.0127 0.4532 0.0245 0.0056 0.0057 22 1 0.0015 0.0037 0.4552 0.0317 0.0007 0.0017 22 2 0.0012 0.0030 0.4535 0.0295 0.0005 0.0013 23 1 0.0278 0.0218 0.4804 0.0334 0.0133 0.0101 23 2 0.0260 0.0212 0.4804 0.0355 0.0124 0.0099 24 1 0.0441 0.0268 0.4852 0.0401 0.0213 0.0129 24 2 0.0396 0.0254 0.4834 0.0412 0.0190 0.0121 25 1 0.0012 0.0035 0.4595 0.0391 0.0005 0.0016 25 2 0.0011 0.0044 0.4597 0.0403 0.0005 0.0020 26 1 0.0022 0.0049 0.4633 0.0403 0.0010 0.0022 26 2 0.0017 0.0051 0.4634 0.0424 0.0008 0.0023 27 1 0.0102 0.0141 0.4547 0.0278 0.0046 0.0064 27 2 0.0099 0.0137 0.4559 0.0318 0.0045 0.0063 28 1 0.0010 0.0048 0.4604 0.0394 0.0004 0.0022 28 2 0.0009 0.0034 0.4619 0.0414 0.0004 0.0016 29 1 0.0007 0.0031 0.4500 0.0226 0.0003 0.0014 29 2 0.0006 0.0028 0.4522 0.0279 0.0003 0.0013 30 1 0.0317 0.0215 0.4649 0.0324 0.0147 0.0099 30 2 0.0282 0.0190 0.4650 0.0334 0.0131 0.0088 31 1 0.0147 0.0168 0.4727 0.0408 0.0069 0.0079 31 2 0.0138 0.0158 0.4714 0.0402 0.0065 0.0075 32 1 0.0007 0.0041 0.4585 0.0361 0.0003 0.0019 32 2 0.0007 0.0048 0.4564 0.0346 0.0003 0.0021 "],["estimaciones-por-dimension-del-ipm.html", "11.9 Estimaciones por dimension del IPM", " 11.9 Estimaciones por dimension del IPM Dado que el Índice de Pobreza Multidimensional (IPM) está compuesto por diversas dimensiones, resulta fundamental analizar cada una de estas dimensiones de manera individual. Esto permite comprender la naturaleza compleja y multifacética de la pobreza, lo cual a su vez posibilita diseñar estrategias de reducción efectivas. Esta aproximación garantiza una toma de decisiones fundamentada, la distribución eficiente de recursos y un impacto más profundo en la mejora de las condiciones de vida de las personas vulnerables. En este contexto, los “hard estimates” previamente obtenidos para cada dimensión resultan esenciales para obtener las estimaciones correspondientes a cada una de ellas. El proceso de cálculo se basa en una media ponderada y se aplica a la dimensión de Hacinamiento, siguiendo una lógica similar para las demás dimensiones del IPM. Con el objetivo de agilizar el proceso de calculo se define crea la función agregado_dim_ipm que hace los cálculos. La forma de uso es la siguiente. source(&quot;Recursos/Día3/Sesion3/0Recursos/agregado_dim_ipm.r&quot;) epred_mat_hacinamiento_dummy &lt;- readRDS( &quot;Recursos/Día3/Sesion3/Data/epred_mat_hacinamiento_dummy.rds&quot;) datos_dam_haci &lt;- agregado_dim_ipm(poststrat = poststrat_df, epredmat = epred_mat_hacinamiento_dummy, byMap = &quot;dam&quot;) Tabla 11.7: Estimaciones por departamento para Hacinamiento dam estimate estimate_se 01 0.3502 0.0809 02 0.3810 0.0900 03 0.4902 0.0835 04 0.5150 0.0551 05 0.3243 0.0887 06 0.4429 0.0816 07 0.5795 0.0555 08 0.3132 0.0797 09 0.3925 0.0868 10 0.3920 0.0760 11 0.4842 0.0759 12 0.6018 0.0514 13 0.4216 0.0552 14 0.4072 0.0808 15 0.4301 0.0811 16 0.4502 0.0638 17 0.4554 0.0684 18 0.3933 0.0680 19 0.3629 0.0898 20 0.5435 0.0528 21 0.4974 0.0592 22 0.4149 0.0719 23 0.5770 0.0753 24 0.4086 0.0665 25 0.4516 0.0733 26 0.4289 0.0816 27 0.4282 0.0602 28 0.4135 0.0843 29 0.5095 0.0741 30 0.4739 0.0559 31 0.5015 0.0624 32 0.3717 0.0717 El resultado por dam y para todas las dimensiones es calculando usando el siguiente código. epred_mat_dim &lt;- list( Material = epred_mat_material_dummy, Hacinamiento = epred_mat_hacinamiento_dummy , Agua = epred_mat_agua_dummy, Saneamiento = epred_mat_saneamiento_dummy, Energia = epred_mat_energia_dummy , Techo = epred_mat_techo_dummy, Pisos = epred_mat_piso_dummy , Paredes = epred_mat_paredes_dummy, Combustible = epred_mat_combustible_dummy ) estimacion_dam_dim &lt;- aux_agregado(epred_mat_dim, byx = &quot;dam&quot;, censo = poststrat_df) %&gt;% data.frame() saveRDS(estimacion_dam_dim,&quot;Recursos/Día3/Sesion3/0Recursos/estimacion_dam_dim.Rds&quot;) Tabla 11.8: Estimacion puntual por estado y dimension dam Agua Combustible Energia Hacinamiento Material Paredes Pisos Saneamiento Techo 01 0.0085 0.0093 0.0022 0.3502 0.0070 0.0026 0.0010 0.0957 0.0010 02 0.0312 0.0032 0.0049 0.3810 0.0130 0.0035 0.0019 0.2294 0.0019 03 0.0428 0.0235 0.0033 0.4902 0.0285 0.0026 0.0069 0.2098 0.0069 04 0.0314 0.2971 0.0045 0.5150 0.0371 0.0014 0.0206 0.3337 0.0206 05 0.0154 0.0121 0.0050 0.3243 0.0099 0.0018 0.0025 0.1425 0.0025 06 0.0045 0.0694 0.0031 0.4429 0.0223 0.0049 0.0043 0.1818 0.0043 07 0.1256 0.5679 0.0090 0.5795 0.0958 0.0056 0.0076 0.3970 0.0076 08 0.0350 0.0557 0.0113 0.3132 0.0223 0.0014 0.0005 0.1520 0.0005 09 0.0983 0.0031 0.0056 0.3925 0.0063 0.0010 0.0004 0.1915 0.0004 10 0.0121 0.0632 0.0066 0.3920 0.0186 0.0039 0.0040 0.1787 0.0040 11 0.0414 0.0662 0.0038 0.4842 0.0155 0.0008 0.0016 0.2377 0.0016 12 0.1240 0.4642 0.0067 0.6018 0.0946 0.0082 0.0025 0.5064 0.0025 13 0.0356 0.2135 0.0110 0.4216 0.0288 0.0048 0.0015 0.2277 0.0015 14 0.0101 0.0407 0.0030 0.4072 0.0149 0.0016 0.0014 0.1534 0.0014 15 0.0394 0.0667 0.0042 0.4301 0.0157 0.0031 0.0021 0.2868 0.0021 16 0.0313 0.1837 0.0044 0.4502 0.0467 0.0039 0.0024 0.2689 0.0024 17 0.0746 0.0901 0.0018 0.4554 0.0214 0.0016 0.0012 0.2894 0.0012 18 0.0242 0.0895 0.0110 0.3933 0.0305 0.0037 0.0031 0.1833 0.0031 19 0.0202 0.0226 0.0022 0.3629 0.0154 0.0016 0.0015 0.1582 0.0015 20 0.1127 0.5029 0.0079 0.5435 0.1315 0.0091 0.0153 0.4595 0.0153 21 0.0654 0.2427 0.0036 0.4974 0.0304 0.0021 0.0004 0.3689 0.0004 22 0.0284 0.0657 0.0048 0.4149 0.0107 0.0009 0.0007 0.1787 0.0007 23 0.0143 0.1856 0.0047 0.5770 0.0641 0.0029 0.0509 0.2139 0.0509 24 0.0862 0.2215 0.0083 0.4086 0.0792 0.0356 0.0380 0.3499 0.0380 25 0.0129 0.0622 0.0036 0.4516 0.0187 0.0026 0.0029 0.1254 0.0029 26 0.0159 0.0482 0.0056 0.4289 0.0218 0.0044 0.0031 0.2273 0.0031 27 0.0694 0.3314 0.0029 0.4282 0.0153 0.0035 0.0022 0.2868 0.0022 28 0.0214 0.0432 0.0030 0.4135 0.0132 0.0016 0.0024 0.1926 0.0024 29 0.0059 0.0703 0.0026 0.5095 0.0106 0.0015 0.0003 0.2803 0.0003 30 0.1123 0.2999 0.0060 0.4739 0.0558 0.0062 0.0087 0.3649 0.0087 31 0.0057 0.3546 0.0063 0.5015 0.0314 0.0020 0.0203 0.2487 0.0203 32 0.0227 0.0515 0.0045 0.3717 0.0104 0.0006 0.0023 0.1606 0.0023 Tabla 11.9: Error de estimacion por estado y dimension dam Agua_se Combustible_se Energia_se Hacinamiento_se Material_se Paredes_se Pisos_se Saneamiento_se Techo_se 01 0.0135 0.0125 0.0058 0.0809 0.0127 0.0076 0.0048 0.0505 0.0048 02 0.0304 0.0085 0.0114 0.0900 0.0187 0.0096 0.0060 0.0768 0.0060 03 0.0311 0.0195 0.0070 0.0835 0.0258 0.0080 0.0113 0.0700 0.0113 04 0.0171 0.0434 0.0064 0.0551 0.0187 0.0042 0.0132 0.0532 0.0132 05 0.0217 0.0162 0.0117 0.0887 0.0176 0.0076 0.0082 0.0680 0.0082 06 0.0092 0.0309 0.0075 0.0816 0.0212 0.0102 0.0085 0.0619 0.0085 07 0.0359 0.0454 0.0104 0.0555 0.0330 0.0083 0.0098 0.0556 0.0098 08 0.0261 0.0261 0.0133 0.0797 0.0210 0.0055 0.0028 0.0597 0.0028 09 0.0548 0.0090 0.0123 0.0868 0.0133 0.0048 0.0029 0.0720 0.0029 10 0.0145 0.0264 0.0095 0.0760 0.0174 0.0093 0.0077 0.0612 0.0077 11 0.0279 0.0294 0.0078 0.0759 0.0164 0.0037 0.0049 0.0678 0.0049 12 0.0330 0.0467 0.0096 0.0514 0.0298 0.0126 0.0055 0.0551 0.0055 13 0.0185 0.0370 0.0104 0.0552 0.0171 0.0071 0.0036 0.0469 0.0036 14 0.0148 0.0244 0.0079 0.0808 0.0179 0.0060 0.0050 0.0616 0.0050 15 0.0270 0.0293 0.0083 0.0811 0.0170 0.0076 0.0051 0.0740 0.0051 16 0.0196 0.0381 0.0067 0.0638 0.0235 0.0066 0.0048 0.0543 0.0048 17 0.0319 0.0286 0.0050 0.0684 0.0164 0.0045 0.0038 0.0609 0.0038 18 0.0173 0.0282 0.0109 0.0680 0.0203 0.0067 0.0059 0.0522 0.0059 19 0.0249 0.0238 0.0073 0.0898 0.0218 0.0067 0.0065 0.0688 0.0065 20 0.0364 0.0448 0.0104 0.0528 0.0384 0.0115 0.0147 0.0545 0.0147 21 0.0251 0.0401 0.0056 0.0592 0.0174 0.0050 0.0018 0.0572 0.0018 22 0.0203 0.0248 0.0084 0.0719 0.0129 0.0036 0.0027 0.0576 0.0027 23 0.0153 0.0503 0.0076 0.0753 0.0313 0.0076 0.0274 0.0624 0.0274 24 0.0309 0.0403 0.0097 0.0665 0.0304 0.0206 0.0196 0.0654 0.0196 25 0.0139 0.0255 0.0073 0.0733 0.0168 0.0063 0.0060 0.0484 0.0060 26 0.0179 0.0252 0.0088 0.0816 0.0217 0.0093 0.0072 0.0702 0.0072 27 0.0283 0.0486 0.0057 0.0602 0.0136 0.0066 0.0052 0.0557 0.0052 28 0.0229 0.0280 0.0083 0.0843 0.0178 0.0056 0.0069 0.0683 0.0069 29 0.0101 0.0288 0.0066 0.0741 0.0136 0.0050 0.0016 0.0660 0.0016 30 0.0316 0.0406 0.0071 0.0559 0.0231 0.0082 0.0090 0.0549 0.0090 31 0.0087 0.0570 0.0090 0.0624 0.0212 0.0057 0.0153 0.0554 0.0153 32 0.0207 0.0271 0.0087 0.0717 0.0135 0.0030 0.0064 0.0567 0.0064 El siguiente paso es realizar el mapa de los resultados library(sf) library(tmap) ShapeSAE &lt;- read_sf(&quot;Recursos/Día3/Sesion3/Shape/MEX_dam.shp&quot;) Los resultados nacionales son mostrados en el mapa. maps3 &lt;- tm_shape(ShapeSAE %&gt;% left_join(ipm_dam, by = &quot;dam&quot;)) tema_map &lt;- tm_layout(legend.only = FALSE, legend.height = -0.5, legend.width = -0.3, asp = 1.5, legend.text.size = 5, legend.title.size = 4) Mapa_H &lt;- maps3 + tm_polygons( &quot;H&quot;, title = &quot;H&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tema_map Mapa_A &lt;- maps3 + tm_polygons( &quot;A&quot;, title = &quot;A&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tema_map Mapa_ipm &lt;- maps3 + tm_polygons( &quot;IPM&quot;, title = &quot;IPM&quot;, palette = &quot;YlOrRd&quot;, colorNA = &quot;white&quot; ) + tema_map Mapas &lt;- tmap_arrange(Mapa_H, Mapa_A, Mapa_ipm) tmap_save( Mapas, &quot;Recursos/Día3/Sesion3/0Recursos/MEX_IPM.jpeg&quot;, width = 6920, height = 4080, asp = 0 ) Los resultado para cada componente puede ser mapeado de forma similar. Para obtener el resultado por municipio procedemos así: "],["día-3---sesión-4--modelo-de-área-para-estadísticas-del-mercado-de-trabajo.html", "Capítulo 12 Día 3 - Sesión 4- Modelo de área para estadísticas del mercado de trabajo", " Capítulo 12 Día 3 - Sesión 4- Modelo de área para estadísticas del mercado de trabajo La Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH) 2020 es una encuesta representativa a nivel nacional que recopila información sobre los ingresos, gastos, características sociodemográficas y acceso a servicios de los hogares mexicanos. La encuesta se realizó del 27 de octubre al 30 de diciembre de 2020 y contó con una muestra de aproximadamente 100,000 viviendas. La ENIGH recopila una amplia variedad de datos relacionados con el empleo en los hogares mexicanos. Algunos de los resultados y análisis asociados al empleo que se pueden derivar de la ENIGH incluyen: Tasa de empleo: Proporciona una estimación de la proporción de la población económicamente activa que está empleada. Ingresos laborales: La encuesta recopila información sobre los ingresos derivados del trabajo, incluidos salarios, sueldos, bonificaciones y otras compensaciones laborales. Desempleo: La ENIGH también puede proporcionar datos sobre la tasa de desempleo, que mide la proporción de la población económicamente activa que no tiene empleo y está buscando activamente trabajo. "],["definición-del-modelo-multinomial.html", "12.1 Definición del modelo multinomial", " 12.1 Definición del modelo multinomial Sea \\(K\\) el número de categorías de la variable de interés \\(\\sim multinimial\\left(\\boldsymbol{\\theta}\\right)\\), con \\(\\boldsymbol{\\theta}=\\left(p_{1},p_{2},\\dots ,p_{k}\\right)\\) y \\(\\sum_{k=1}^{K}p_{k}=1\\). Sea \\(N_i\\) el número de elementos en el i-ésiamo dominio y \\(N_{ik}\\) el número de elementos que tienen la k-ésima categoría, note que \\(\\sum_{k=1}^{K}N_{ik}=N_{i}\\) y \\(p_{ik}=\\frac{N_{ik}}{N_{i}}\\). Sea \\(\\hat{p}_{ik}\\) la estimación directa de \\(p_{ik}\\) y \\(v_{ik}=Var\\left(\\hat{p}_{ik}\\right)\\) y denote el estimador de la varianza por \\(\\hat{v}_{ik}=\\widehat{Var}\\left(\\hat{p}_{ik}\\right)\\) Note que el efecto diseño cambia entre categoría, por tanto, lo primero será definir el tamaño de muestra efectivo por categoría. Esto es: La estimación de \\(\\tilde{n}\\) esta dado por \\(\\tilde{n}_{ik} = \\frac{(\\tilde{p}_{ik}\\times(1-\\tilde{p}_{ik}))}{\\hat{v}_{ik}},\\) \\(\\tilde{y}_{ik}=\\tilde{n}_{ik}\\times\\hat{p}_{ik}\\) luego, \\(\\hat{n}_{i} = \\sum_{k=1}^{K}\\tilde{y}_{ik}\\) de donde se sigue que \\(\\hat{y}_{ik} = \\hat{n}_i\\times \\hat{p}_{ik}\\) Sea \\(\\boldsymbol{\\theta}=\\left(p_{1},p_{2}, p_{3}\\right)^{T}=\\left(\\frac{N_{i1}}{N_{i}},\\frac{N_{i2}}{N_{i}}\\frac{N_{i3}}{N_{i}}\\right)^{T}\\), entonces el modelo multinomial para el i-ésimo dominio estaría dado por: \\[ \\left(\\tilde{y}_{i1},\\tilde{y}_{i2},\\tilde{y}_{i3}\\right)\\mid\\hat{n}_{i},\\boldsymbol{\\theta}_{i}\\sim multinomial\\left(\\hat{n}_{i},\\boldsymbol{\\theta}_{i}\\right) \\] Ahora, puede escribir \\(p_{ik}\\) como : \\(\\ln\\left(\\frac{p_{i2}}{p_{i1}}\\right)=\\boldsymbol{X}_{i}^{T}\\beta_{2} + u_{i2}\\) y \\(\\ln\\left(\\frac{p_{i3}}{p_{i1}}\\right)=\\boldsymbol{X}_{i}^{T}\\beta_{3}+ u_{i3}\\) Dada la restricción \\(1 = p_{i1} + p_{i2} + p_{i3}\\) entonces \\[p_{i1} + p_{i1}(e^{\\boldsymbol{X}_{i}^{T}\\boldsymbol{\\beta_{2}}}+ u_{i2})+p_{i1}(e^{\\boldsymbol{X}_{i}^{T}\\boldsymbol{\\beta}_{3}} + u_{i3})\\] de donde se sigue que \\[ p_{i1}=\\frac{1}{1+e^{\\boldsymbol{X}_{i}^{T}\\boldsymbol{\\beta_{2}}}+ u_{i2}+e^{\\boldsymbol{X_{i}}^{T}\\boldsymbol{\\beta_{3}}}+ u_{i3}} \\] Las expresiones para \\(p_{i2}\\) y \\(p_{i3}\\) estarían dadas por: \\[ p_{i2}=\\frac{e^{\\boldsymbol{X}_{i}^{T}\\boldsymbol{\\beta}_{2}} + u_{i2}}{1+e^{\\boldsymbol{X}_{i}^{T}\\boldsymbol{\\beta_{2}}}+ u_{i2}+e^{\\boldsymbol{X_{i}}^{T}\\boldsymbol{\\beta_{3}}}+ u_{i3}} \\] \\[ p_{i3}=\\frac{e^{\\boldsymbol{X}_{i}^{T}\\boldsymbol{\\beta}_{3}}+ u_{i3}}{1+e^{\\boldsymbol{X}_{i}^{T}\\boldsymbol{\\beta_{2}}}+ u_{i2}+e^{\\boldsymbol{X_{i}}^{T}\\boldsymbol{\\beta_{3}}}+ u_{i3}} \\] "],["lectura-de-librerías..html", "12.2 Lectura de librerías.", " 12.2 Lectura de librerías. La librería survey es una herramienta de análisis estadístico en R que permite trabajar con datos de encuestas complejas, como las encuestas estratificadas, multietápicas o con pesos de muestreo. Ofrece funciones para estimación de parámetros, diseño de muestras, análisis de varianza y regresión, y cálculo de errores estándar. La librería tidyverse es un conjunto de paquetes de R que se utilizan para la manipulación y visualización de datos. Incluye las librerías dplyr, ggplot2, tidyr y otras, y se caracteriza por su enfoque en la programación tidy o ordenada, que facilita la exploración y análisis de datos. La librería srvyr es una extensión de la librería survey que permite integrar las funciones de survey con la sintaxis de dplyr, lo que facilita la manipulación de datos de encuestas complejas. Incluye funciones para agrupar, filtrar y resumir datos de encuestas utilizando la sintaxis tidy. La librería TeachingSampling es una herramienta de R que se utiliza para la enseñanza de métodos de muestreo estadístico. Incluye funciones para simular diferentes tipos de muestras, estimar parámetros, calcular errores estándar y construir intervalos de confianza, entre otras. La librería haven es una herramienta de R que permite importar y exportar datos en diferentes formatos, incluyendo SPSS, Stata y SAS. Permite trabajar con archivos de datos de encuestas, y ofrece funciones para etiquetar variables, codificar datos faltantes y convertir datos de diferentes formatos. La librería bayesplot es una herramienta de R que se utiliza para la visualización y diagnóstico de modelos Bayesianos. Incluye funciones para graficar distribuciones posteriores, diagnósticos de convergencia, gráficos de diagnóstico de residuos, y otros tipos de gráficos relacionados con el análisis Bayesianos. La librería patchwork es una herramienta de R que permite unir gráficos de manera sencilla y flexible. Esta librería facilita la creación de gráficos complejos al permitir la combinación de múltiples gráficos en una sola visualización, lo que resulta especialmente útil en análisis de datos y modelización. La librería stringr es una herramienta de R que se utiliza para la manipulación de cadenas de texto. Incluye funciones para la extracción, manipulación y modificación de cadenas de texto, lo que resulta especialmente útil en la limpieza y preparación de datos antes de su análisis. La librería rstan es una herramienta de R que se utiliza para la estimación de modelos Bayesianos mediante el método de cadenas de Markov Monte Carlo (MCMC). Esta librería permite la especificación y estimación de modelos complejos mediante un lenguaje sencillo y flexible, y ofrece diversas herramientas para el diagnóstico y visualización de resultados. library(survey) library(tidyverse) library(srvyr) library(TeachingSampling) library(haven) library(bayesplot) library(patchwork) library(stringr) library(rstan) "],["lectura-de-la-encuesta-y-estimaciones-directas.html", "12.3 Lectura de la encuesta y estimaciones directas", " 12.3 Lectura de la encuesta y estimaciones directas En la primera línea se carga la encuesta desde un archivo RDS y se guarda en un objeto llamado encuesta. La segunda línea utiliza la función transmute() de la librería dplyr para seleccionar las variables de interés en la encuesta y crear nuevas variables a partir de ellas. Luego, se utiliza la variable id_dominio para identificar el dominio de estudio. En conjunto, estos pasos son fundamentales para preparar los datos de la encuesta para su posterior estimación del parámetro. encuesta &lt;- readRDS(&#39;Recursos/Día3/Sesion4/Data/encuesta_empleo.rds&#39;) ## length_upm &lt;- max(nchar(encuesta[[&quot;upm&quot;]])) length_estrato &lt;- max(nchar(encuesta[[&quot;estrato&quot;]])) encuesta &lt;- encuesta %&gt;% transmute( dam, dam2, upm = str_pad(string = `upm`, width = length_upm, pad = &quot;0&quot;), estrato = str_pad(string = `estrato`, width = length_estrato , pad = &quot;0&quot;), fep = `fep`, empleo ) id_dominio &lt;- &quot;dam2&quot; El código presentado define el diseño muestral para el análisis de la encuesta “encuesta” en R. La primera línea establece una opción para el tratamiento de las PSU (unidades primarias de muestreo) solitarias, lo que indica que se deben aplicar ajustes en el cálculo de los errores estándar. La segunda línea utiliza la función “as_survey_design” de la librería “survey” para definir el diseño muestral. La función toma como argumentos la variable “encuesta” y los siguientes parámetros: strata: la variable que define las estratas de muestreo en la encuesta, en este caso la variable “estrato”. ids: la variable que identifica las PSU en la encuesta, en este caso la variable “upm”. weights: la variable que indica los pesos muestrales de cada observación, en este caso la variable “fep”. nest: un parámetro lógico que indica si los datos de la encuesta están anidados o no. En este caso, se establece en “TRUE” porque los datos están anidados por dominio. En conjunto, estos pasos permiten definir un diseño muestral que tenga en cuenta las características del muestreo y los pesos asignados a cada observación en la encuesta, lo que es necesario para obtener estimaciones precisas y representativas de los parámetros de interés. options(survey.lonely.psu= &#39;adjust&#39; ) diseno &lt;- encuesta %&gt;% as_survey_design( strata = estrato, ids = upm, weights = fep, nest=T ) El código presentado es una operación que se realiza en el diseño muestral definido en el código anterior, con el objetivo de obtener un indicador del empleo por dominio. La primera línea define un objeto llamado “indicador_dam”. En la segunda línea, se agrupa el diseño muestral según el dominio especificado en la variable “id_dominio”. La tercera línea filtra los datos para quedarse con los individuos que tienen empleo (empleo igual a 1), están desempleados (empleo igual a 2) o son inactivos (empleo igual a 3). A partir de la cuarta línea, se utilizan las funciones “summarise” y “survey_mean” para calcular las estadísticas descriptivas de interés. En particular, se calculan el número de personas ocupadas, desocupadas e inactivas en cada dominio, y la proporción de personas en cada una de estas categorías. La función “survey_mean” se utiliza para calcular la proporción de personas en cada una de estas categorías con sus respectivos errores estándar y efecto de diseño. indicador_dam &lt;- diseno %&gt;% group_by_at(id_dominio) %&gt;% filter(empleo %in% c(1:3)) %&gt;% summarise( n_ocupado = unweighted(sum(empleo == 1)), n_desocupado = unweighted(sum(empleo == 2)), n_inactivo = unweighted(sum(empleo == 3)), Ocupado = survey_mean(empleo == 1, vartype = c(&quot;se&quot;, &quot;var&quot;), deff = T ), Desocupado = survey_mean(empleo == 2, vartype = c(&quot;se&quot;, &quot;var&quot;), deff = T ), Inactivo = survey_mean(empleo == 3, vartype = c(&quot;se&quot;, &quot;var&quot;), deff = T ) ) "],["selección-de-dominios.html", "12.4 Selección de dominios", " 12.4 Selección de dominios En la sección anterior, se llevó a cabo una estimación directa para cada categoría individualmente en cada municipio (dominio) presente en la muestra. Ahora, para evaluar la calidad de los resultados obtenidos. Se emplean varias medidas de calidad, entre ellas, se cuenta el número de dominios que tienen tres o más unidades primarias de muestreo (UPM), así como el efecto de diseño mayor a 0.7 el alguna de las categorías y las varianzas mayores a 0. Estas medidas nos permitirán determinar la fiabilidad de nuestros resultados y tomar decisiones informadas en función de ellos. Después de realizar las validaciones anteriores se establece como regla incluir en el estudio los dominios que posean Tres o más UPM por dominio. Varianzas estimadas mayor a cero. Contar con un resultado en el efecto de diseño mayor a 0.7 en por lo menos una categoría. indicador_dam &lt;- encuesta %&gt;% select(id_dominio, upm) %&gt;% distinct() %&gt;% group_by_at(id_dominio) %&gt;% tally(name = &quot;n_upm&quot;) %&gt;% inner_join(indicador_dam, by = id_dominio) saveRDS(indicador_dam, &quot;Recursos/Día3/Sesion4/Data/indicador_dam.rds&quot;) indicador_dam1 &lt;- indicador_dam %&gt;% filter( n_upm &gt;= 3, Desocupado_var &gt; 0, Ocupado_var &gt; 0, Inactivo_var &gt; 0, Desocupado_deff &gt; 0.7 | Ocupado_deff &gt; 0.7 | Inactivo_deff &gt; 0.7 ) %&gt;% mutate(id_orden = 1:n()) saveRDS(object = indicador_dam1, &quot;Recursos/Día3/Sesion4/Data/base_modelo.Rds&quot;) dam2 n_upm n_ocupado n_desocupado n_inactivo Ocupado Ocupado_se Ocupado_var Ocupado_deff Desocupado Desocupado_se Desocupado_var Desocupado_deff Inactivo Inactivo_se Inactivo_var Inactivo_deff id_orden 01001 274 2560 151 1609 0.5927 0.0082 1e-04 1.2035 0.0361 0.0032 0e+00 1.3006 0.3712 0.0079 0.0001 1.1630 1 01003 8 238 18 194 0.5399 0.0158 2e-04 0.4575 0.0362 0.0075 1e-04 0.7328 0.4239 0.0138 0.0002 0.3554 2 01005 27 619 25 298 0.6365 0.0182 3e-04 1.3637 0.0352 0.0077 1e-04 1.6567 0.3283 0.0157 0.0002 1.0603 3 01006 7 273 14 194 0.5801 0.0200 4e-04 0.7957 0.0264 0.0056 0e+00 0.5836 0.3935 0.0180 0.0003 0.6536 4 01007 8 334 9 225 0.6132 0.0312 1e-03 2.3457 0.0155 0.0059 0e+00 1.3093 0.3714 0.0300 0.0009 2.2054 5 01011 8 324 17 177 0.6359 0.0290 8e-04 1.9003 0.0298 0.0074 1e-04 0.9842 0.3343 0.0319 0.0010 2.3912 6 02001 73 1359 55 979 0.5488 0.0190 4e-04 3.5128 0.0230 0.0050 0e+00 2.6845 0.4282 0.0181 0.0003 3.2195 7 02002 141 2014 93 1644 0.5537 0.0113 1e-04 1.9590 0.0221 0.0039 0e+00 2.6821 0.4242 0.0110 0.0001 1.8694 8 02003 16 519 19 379 0.5640 0.0293 9e-04 3.2058 0.0188 0.0072 1e-04 2.5963 0.4172 0.0361 0.0013 4.9275 9 02004 197 2077 116 1531 0.5577 0.0101 1e-04 1.5357 0.0327 0.0041 0e+00 1.9601 0.4096 0.0098 0.0001 1.4950 10 "],["modelo-programando-en-stan.html", "12.5 Modelo programando en STAN", " 12.5 Modelo programando en STAN El código presenta la implementación de un modelo multinomial logístico de área de respuesta utilizando el lenguaje de programación STAN. En este modelo, se asume que la variable de respuesta en cada dominio sigue una distribución multinomial. Se asume que los parámetros que rigen la relación entre las variables predictoras y la variable de respuesta son diferentes en cada dominio y se modelan como efectos aleatorios. La sección de functions define una función auxiliar llamada pred_theta(), que se utiliza para predecir los valores de la variable de respuesta en los dominios no observados. La sección de data contiene las variables de entrada del modelo, incluyendo el número de dominios, el número de categorías de la variable de respuesta, las estimaciones directas de la variable de respuesta en cada dominio, las covariables observadas en cada dominio y las covariables correspondientes a los dominios no observados. La sección de parameters define los parámetros desconocidos del modelo, incluyendo la matriz de parámetros beta, que contiene los coeficientes que relacionan las covariables con la variable de respuesta en cada categoría. También se incluyen los desviaciones estándar de los efectos aleatorios. En la sección de transformed parameters se define el vector de parámetros theta, que contiene las probabilidades de pertenencia a cada categoría de la variable de respuesta en cada dominio. Se utilizan los efectos aleatorios para ajustar los valores de theta en cada dominio. En la sección de model se define la estructura del modelo y se incluyen las distribuciones a priori para los parámetros desconocidos. En particular, se utiliza una distribución normal para los coeficientes de la matriz beta. Finalmente, se calcula la función de verosimilitud de la distribución multinomial para las estimaciones directas de la variable de respuesta en cada dominio. La sección de generated quantities se utiliza para calcular las predicciones de la variable de respuesta en los dominios no observados utilizando la función auxiliar definida previamente. functions { matrix pred_theta(matrix Xp, int p, matrix beta){ int D1 = rows(Xp); real num1[D1, p]; real den1[D1]; matrix[D1,p] theta_p; for(d in 1:D1){ num1[d, 1] = 1; num1[d, 2] = exp(Xp[d, ] * beta[1, ]&#39; ) ; num1[d, 3] = exp(Xp[d, ] * beta[2, ]&#39; ) ; den1[d] = sum(num1[d, ]); } for(d in 1:D1){ for(i in 2:p){ theta_p[d, i] = num1[d, i]/den1[d]; } theta_p[d, 1] = 1/den1[d]; } return theta_p ; } } data { int&lt;lower=1&gt; D; // número de dominios int&lt;lower=1&gt; P; // categorías int&lt;lower=1&gt; K; // cantidad de regresores int y_tilde[D, P]; // matriz de datos matrix[D, K] X_obs; // matriz de covariables int&lt;lower=1&gt; D1; // número de dominios matrix[D1, K] X_pred; // matriz de covariables } parameters { matrix[P-1, K] beta;// matriz de parámetros real&lt;lower=0&gt; sigma2_u1; // random effects standard deviations real&lt;lower=0&gt; sigma2_u2; // random effects standard deviations vector[D] u1; vector[D] u2; // declare L_u to be the Choleski factor of a 2x2 correlation matrix } transformed parameters { simplex[P] theta[D];// vector de parámetros; real num[D, P]; real den[D]; real&lt;lower=0&gt; sigma_u1; // random effects standard deviations real&lt;lower=0&gt; sigma_u2; // random effects standard deviations sigma_u1 = sqrt(sigma2_u1); sigma_u2 = sqrt(sigma2_u2); for(d in 1:D){ num[d, 1] = 1; num[d, 2] = exp(X_obs[d, ] * beta[1, ]&#39; + u1[d]) ; num[d, 3] = exp(X_obs[d, ] * beta[2, ]&#39; + u2[d]) ; den[d] = sum(num[d, ]); } for(d in 1:D){ for(p in 2:P){ theta[d, p] = num[d, p]/den[d]; } theta[d, 1] = 1/den[d]; } } model { u1 ~ normal(0, sigma_u1); u2 ~ normal(0, sigma_u2); sigma2_u1 ~ inv_gamma(0.0001, 0.0001); sigma2_u2 ~ inv_gamma(0.0001, 0.0001); for(p in 2:P){ for(k in 1:K){ beta[p-1, k] ~ normal(0, 10000); } } for(d in 1:D){ target += multinomial_lpmf(y_tilde[d, ] | theta[d, ]); } } generated quantities { matrix[D1,P] theta_pred; theta_pred = pred_theta(X_pred, P, beta); } "],["preparando-insumos-para-stan.html", "12.6 Preparando insumos para STAN", " 12.6 Preparando insumos para STAN Lectura y adecuación de covariables statelevel_predictors_df &lt;- readRDS(&#39;Recursos/Día3/Sesion4/Data/predictors_satelital_dam2.rds&#39;) head(statelevel_predictors_df,10) %&gt;% tba() dam dam2 luces_nocturnas suelo_cultivo suelo_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado area1 etnia2 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 discapacidad1 etnia1 tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 01 01001 127.0742 105.4925 128.6893 106.7313 99.2730 99.4086 0.9453 0.0116 0.5180 0.2656 0.2129 0.1962 0.0671 0.2101 0.4280 0.2258 0.0574 0.0595 0.0020 0.3925 0.0303 0.0301 0.0058 0.2946 0.0022 0.0211 0.0098 0.3419 0.0381 0.1690 0.0284 01 01002 103.2847 101.3865 101.7692 100.7418 99.2012 99.2972 0.4106 0.0030 0.5082 0.2690 0.1853 0.1703 0.0665 0.3188 0.4404 0.0666 0.0706 0.0318 0.0448 0.5895 0.4913 0.0650 0.1084 0.6548 0.0118 0.2598 0.1648 0.1003 0.0648 0.3137 0.0436 01 01003 102.5703 100.2905 100.6374 101.8864 99.4002 99.4182 0.5802 0.0039 0.5117 0.2438 0.1828 0.1920 0.0990 0.3647 0.4001 0.0665 0.0865 0.0387 0.0116 1.0000 0.0776 0.0909 0.0035 0.5350 0.0096 0.1028 0.0070 0.0996 0.0630 0.2553 0.0647 01 01004 99.2927 98.8613 98.8818 97.8499 99.1040 99.1727 0.5490 0.0042 0.5139 0.2730 0.2032 0.1587 0.0610 0.2666 0.4727 0.0785 0.0603 0.0585 0.0258 0.5185 0.1752 0.0544 0.0123 0.7144 0.0079 0.2572 0.0861 0.1261 0.0592 0.3578 0.0303 01 01005 107.6633 100.5877 104.8903 100.5953 99.1894 99.2677 0.7330 0.0255 0.5014 0.2524 0.2309 0.1629 0.0569 0.2359 0.3886 0.1761 0.0511 0.0525 0.0050 0.9308 0.2182 0.0302 0.0013 0.3709 0.0039 0.0329 0.0200 0.2858 0.0538 0.2565 0.0162 01 01006 101.2918 99.5395 99.7443 98.6083 99.0990 99.1562 0.7530 0.0111 0.5086 0.2595 0.1996 0.1806 0.0706 0.2497 0.4258 0.1650 0.0576 0.0210 0.0043 0.6154 0.0544 0.0303 0.0183 0.4093 0.0032 0.0464 0.0215 0.2584 0.0582 0.2585 0.0197 01 01007 102.6725 100.9469 100.5439 99.5256 99.1553 99.2049 0.7223 0.0028 0.5125 0.2738 0.1898 0.1566 0.0620 0.2593 0.4439 0.1062 0.0500 0.0373 0.0107 0.6809 0.0585 0.0444 0.0432 0.5551 0.0057 0.1440 0.0729 0.1765 0.0728 0.3470 0.0192 01 01008 99.7144 100.2844 98.6047 99.5866 99.6891 99.6227 0.4434 0.1588 0.5210 0.2681 0.1871 0.1591 0.0641 0.2909 0.4598 0.0736 0.0479 0.0382 0.0136 0.6316 0.1688 0.0853 0.0085 0.7140 0.0072 0.3095 0.0818 0.1224 0.0563 0.3412 0.0217 01 01009 100.4092 99.1593 99.8478 98.6020 99.1182 99.1724 0.4175 0.0026 0.5112 0.2794 0.1917 0.1578 0.0689 0.3231 0.4423 0.0661 0.0544 0.0264 0.0306 0.7288 0.3110 0.0362 0.0308 0.5720 0.0101 0.1213 0.0847 0.1039 0.0731 0.3582 0.0841 01 01010 101.4046 102.5622 99.7574 99.7674 99.1833 99.2439 0.2279 0.0128 0.5002 0.2671 0.1919 0.1627 0.0680 0.3292 0.4268 0.0533 0.0480 0.0353 0.0389 0.9180 0.1160 0.0563 0.0315 0.7420 0.0136 0.4060 0.3097 0.0851 0.0743 0.3758 0.0399 Seleccionar las variables del modelo y crear matriz de covariables. names_cov &lt;- c( &quot;dam2&quot;, &quot;tasa_desocupacion&quot;, &quot;material_paredes&quot;, &quot;piso_tierra&quot;, &quot;luces_nocturnas&quot;, &quot;suelo_cultivo&quot;, &quot;modificacion_humana&quot; ) X_pred &lt;- anti_join(statelevel_predictors_df %&gt;% select(all_of(names_cov)), indicador_dam1 %&gt;% select(dam2)) En el bloque de código se identifican que dominios serán los predichos. X_pred %&gt;% select(dam2) %&gt;% saveRDS(file = &quot;Recursos/Día3/Sesion4/Data/dam_pred.rds&quot;) Creando la matriz de covariables para los dominios no observados (X_pred) y los observados (X_obs) ## Obteniendo la matrix X_pred %&lt;&gt;% data.frame() %&gt;% select(-dam2) %&gt;% as.matrix() ## Identificando los dominios para realizar estimación del modelo X_obs &lt;- inner_join(indicador_dam1 %&gt;% select(dam2, id_orden), statelevel_predictors_df %&gt;% select(all_of(names_cov))) %&gt;% arrange(id_orden) %&gt;% data.frame() %&gt;% select(-dam2, -id_orden) %&gt;% as.matrix() Calculando el n_efectivo y el \\(\\tilde{y}\\) D &lt;- nrow(indicador_dam1) P &lt;- 3 # Ocupado, desocupado, inactivo. Y_tilde &lt;- matrix(NA, D, P) n_tilde &lt;- matrix(NA, D, P) Y_hat &lt;- matrix(NA, D, P) # n efectivos ocupado n_tilde[,1] &lt;- (indicador_dam1$Ocupado*(1 - indicador_dam1$Ocupado))/indicador_dam1$Ocupado_var Y_tilde[,1] &lt;- n_tilde[,1]* indicador_dam1$Ocupado # n efectivos desocupado n_tilde[,2] &lt;- (indicador_dam1$Desocupado*(1 - indicador_dam1$Desocupado))/indicador_dam1$Desocupado_var Y_tilde[,2] &lt;- n_tilde[,2]* indicador_dam1$Desocupado # n efectivos Inactivo n_tilde[,3] &lt;- (indicador_dam1$Inactivo*(1 - indicador_dam1$Inactivo))/indicador_dam1$Inactivo_var Y_tilde[,3] &lt;- n_tilde[,3]* indicador_dam1$Inactivo Ahora, validamos la coherencia de los cálculos realizados ni_hat = rowSums(Y_tilde) Y_hat[,1] &lt;- ni_hat* indicador_dam1$Ocupado Y_hat[,2] &lt;- ni_hat* indicador_dam1$Desocupado Y_hat[,3] &lt;- ni_hat* indicador_dam1$Inactivo Y_hat &lt;- round(Y_hat) hat_p &lt;- Y_hat/rowSums(Y_hat) par(mfrow = c(1,3)) plot(hat_p[,1],indicador_dam1$Ocupado) abline(a = 0,b=1,col = &quot;red&quot;) plot(hat_p[,2],indicador_dam1$Desocupado) abline(a = 0,b=1,col = &quot;red&quot;) plot(hat_p[,3],indicador_dam1$Inactivo) abline(a = 0,b=1,col = &quot;red&quot;) Compilando el modelo X1_obs &lt;- cbind(matrix(1,nrow = D,ncol = 1),X_obs) K = ncol(X1_obs) D1 &lt;- nrow(X_pred) X1_pred &lt;- cbind(matrix(1,nrow = D1,ncol = 1),X_pred) sample_data &lt;- list(D = D, P = P, K = K, y_tilde = Y_hat, X_obs = X1_obs, X_pred = X1_pred, D1 = D1) library(rstan) fit_mcmc2 &lt;- stan( file = &quot;Recursos/Día3/Sesion4/Data/modelosStan/00 Multinomial_simple_no_cor.stan&quot;, # Stan program data = sample_data, # named list of data verbose = TRUE, warmup = 1000, # number of warmup iterations per chain iter = 2000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(fit_mcmc2, &quot;Recursos/Día3/Sesion4/Data/fit_multinomial_no_cor.Rds&quot;) "],["validación-del-modelo.html", "12.7 Validación del modelo", " 12.7 Validación del modelo La validación de un modelo es esencial para evaluar su capacidad para predecir de manera precisa y confiable los resultados futuros. En el caso de un modelo de área con respuesta multinomial, la validación se enfoca en medir la precisión del modelo para predecir las diferentes categorías de respuesta. El objetivo principal de la validación es determinar si el modelo es capaz de generalizar bien a datos no vistos y proporcionar predicciones precisas. Esto implica comparar las predicciones del modelo con los datos observados y utilizar métricas de evaluación para medir el rendimiento del modelo. La validación del modelo es esencial para garantizar la calidad de las predicciones y la confiabilidad del modelo para su uso en aplicaciones futuras. infile &lt;- paste0(&quot;Recursos/Día3/Sesion4/Data/fit_multinomial_no_cor.Rds&quot;) fit &lt;- readRDS(infile) theta_dir &lt;- indicador_dam1 %&gt;% transmute(dam2, n = n_desocupado + n_ocupado + n_inactivo, Ocupado, Desocupado, Inactivo) color_scheme_set(&quot;brightblue&quot;) theme_set(theme_bw(base_size = 15)) y_pred_B &lt;- as.array(fit, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) theta_1&lt;- grep(pattern = &quot;1]&quot;,x = colnames(y_pred_B),value = TRUE) theta_2&lt;- grep(pattern = &quot;2]&quot;,x = colnames(y_pred_B),value = TRUE) theta_3&lt;- grep(pattern = &quot;3]&quot;,x = colnames(y_pred_B),value = TRUE) y_pred1 &lt;- y_pred_B[rowsrandom,theta_1 ] y_pred2 &lt;- y_pred_B[rowsrandom,theta_2 ] y_pred3 &lt;- y_pred_B[rowsrandom,theta_3 ] p1 &lt;- ppc_dens_overlay(y = as.numeric(theta_dir$Ocupado), y_pred1)/ ppc_dens_overlay(y = as.numeric(theta_dir$Desocupado), y_pred2)/ ppc_dens_overlay(y = as.numeric(theta_dir$Inactivo), y_pred3) # ggsave(plot = p1, # filename = &quot;Recursos/Día3/Sesion4/0Recursos/ppc.png&quot;, # scale = 2) "],["estimación-de-los-parámetros..html", "12.8 Estimación de los parámetros.", " 12.8 Estimación de los parámetros. El código crea dos matrices, theta_obs_ordenado y theta_pred_ordenado, que contienen las estimaciones medias de los parámetros del modelo de respuesta multinomial con covariables para los datos de observación y predicción, respectivamente. La función matrix() se utiliza para dar formato a los datos con una matriz nrow x ncol, y se asignan nombres de columna apropiados a la matriz resultante utilizando colnames(). Luego se convierten las matrices en marcos de datos (as.data.frame()) y se unen mediante full_join() para crear una única tabla que contenga todas las estimaciones de los parámetros para los datos de observación y predicción, junto con la información del indicador de área (theta_dir). El resultado final es un marco de datos llamado estimaciones_obs. dam_pred &lt;- readRDS(&quot;Recursos/Día3/Sesion4/Data/dam_pred.rds&quot;) P &lt;- 3 D &lt;- nrow(indicador_dam1) D1 &lt;- nrow(dam_pred) ## Estimación del modelo. theta_obs &lt;- summary(fit, pars = &quot;theta&quot;)$summary[, &quot;mean&quot;] theta_pred &lt;- summary(fit, pars = &quot;theta_pred&quot;)$summary[, &quot;mean&quot;] ## Ordenando la matrix de theta theta_obs_ordenado &lt;- matrix(theta_obs, nrow = D, ncol = P,byrow = TRUE) colnames(theta_obs_ordenado) &lt;- c(&quot;Ocupado_mod&quot;, &quot;Desocupado_mod&quot;, &quot;Inactivo_mod&quot;) theta_obs_ordenado%&lt;&gt;% as.data.frame() theta_obs_ordenado &lt;- cbind(dam2 = indicador_dam1$dam2, theta_obs_ordenado) theta_pred_ordenado &lt;- matrix(theta_pred, nrow = D1, ncol = P,byrow = TRUE) colnames(theta_pred_ordenado) &lt;- c(&quot;Ocupado_mod&quot;, &quot;Desocupado_mod&quot;, &quot;Inactivo_mod&quot;) theta_pred_ordenado%&lt;&gt;% as.data.frame() theta_pred_ordenado &lt;- cbind(dam2 = dam_pred$dam2, theta_pred_ordenado) "],["estimación-de-la-desviación-estárdar-y-el-coeficiente-de-valiación.html", "12.9 Estimación de la desviación estárdar y el coeficiente de valiación", " 12.9 Estimación de la desviación estárdar y el coeficiente de valiación Este bloque de código corresponde al cálculo de las desviaciones estándar (sd) y coeficientes de variación (cv) de los parámetros theta para los datos observados y predichos. En primer lugar, se utiliza la función summary() del paquete rstan para extraer los valores de sd de los parámetros theta observados y predichos, respectivamente, a partir del modelo (fit) que contiene la información de la estimación de los parámetros de la distribución Bayesiana. Luego, se organizan los valores de sd en una matriz ordenada por dam2 y se les asignan los nombres correspondientes. Con esta matriz, se calcula otra matriz que contiene los coeficientes de variación para los parámetros theta observados (theta_obs_ordenado_cv). De manera similar, se construyen matrices ordenadas por dam2 para los valores de sd y cv de los parámetros theta predichos (theta_pred_ordenado_sd y theta_pred_ordenado_cv, respectivamente). theta_obs_sd &lt;- summary(fit, pars = &quot;theta&quot;)$summary[, &quot;sd&quot;] theta_pred_sd &lt;- summary(fit, pars = &quot;theta_pred&quot;)$summary[, &quot;sd&quot;] theta_obs_ordenado_sd &lt;- matrix(theta_obs_sd, nrow = D, ncol = P,byrow = TRUE) colnames(theta_obs_ordenado_sd) &lt;- c(&quot;Ocupado_mod_sd&quot;, &quot;Desocupado_mod_sd&quot;, &quot;Inactivo_mod_sd&quot;) theta_obs_ordenado_sd%&lt;&gt;% as.data.frame() theta_obs_ordenado_sd &lt;- cbind(dam2 = indicador_dam1$dam2, theta_obs_ordenado_sd) theta_obs_ordenado_cv &lt;- theta_obs_ordenado_sd[,-1]/theta_obs_ordenado[,-1] colnames(theta_obs_ordenado_cv) &lt;- c(&quot;Ocupado_mod_cv&quot;, &quot;Desocupado_mod_cv&quot;, &quot;Inactivo_mod_cv&quot;) theta_obs_ordenado_cv &lt;- cbind(dam2 = indicador_dam1$dam2, theta_obs_ordenado_cv) theta_pred_ordenado_sd &lt;- matrix(theta_pred_sd, nrow = D1, ncol = P,byrow = TRUE) colnames(theta_pred_ordenado_sd) &lt;- c(&quot;Ocupado_mod_sd&quot;, &quot;Desocupado_mod_sd&quot;, &quot;Inactivo_mod_sd&quot;) theta_pred_ordenado_sd%&lt;&gt;% as.data.frame() theta_pred_ordenado_sd &lt;- cbind(dam2 = dam_pred$dam2, theta_pred_ordenado_sd) theta_pred_ordenado_cv &lt;- theta_pred_ordenado_sd[,-1]/theta_pred_ordenado[,-1] colnames(theta_pred_ordenado_cv) &lt;- c(&quot;Ocupado_mod_cv&quot;, &quot;Desocupado_mod_cv&quot;, &quot;Inactivo_mod_cv&quot;) theta_pred_ordenado_cv &lt;- cbind(dam2 = dam_pred$dam2, theta_pred_ordenado_cv) El último paso es realizar la consolidación de la bases obtenidas para la estimación puntual, desviación estándar y coeficiente de variación. theta_obs_ordenado &lt;- full_join(theta_obs_ordenado,theta_obs_ordenado_sd) %&gt;% full_join(theta_obs_ordenado_cv) theta_pred_ordenado &lt;- full_join(theta_pred_ordenado,theta_pred_ordenado_sd) %&gt;% full_join(theta_pred_ordenado_cv) estimaciones &lt;- full_join(indicador_dam1, bind_rows(theta_obs_ordenado, theta_pred_ordenado)) saveRDS(object = estimaciones, file = &quot;Recursos/Día3/Sesion4/Data/estimaciones.rds&quot;) tba(head(estimaciones,10)) dam2 n_upm n_ocupado n_desocupado n_inactivo Ocupado Ocupado_se Ocupado_var Ocupado_deff Desocupado Desocupado_se Desocupado_var Desocupado_deff Inactivo Inactivo_se Inactivo_var Inactivo_deff id_orden Ocupado_mod Desocupado_mod Inactivo_mod Ocupado_mod_sd Desocupado_mod_sd Inactivo_mod_sd Ocupado_mod_cv Desocupado_mod_cv Inactivo_mod_cv 01001 274 2560 151 1609 0.5927 0.0082 1e-04 1.2035 0.0361 0.0032 0e+00 1.3006 0.3712 0.0079 0.0001 1.1630 1 0.5915 0.0362 0.3723 0.0078 0.0030 0.0079 0.0133 0.0835 0.0212 01003 8 238 18 194 0.5399 0.0158 2e-04 0.4575 0.0362 0.0075 1e-04 0.7328 0.4239 0.0138 0.0002 0.3554 2 0.5428 0.0353 0.4219 0.0136 0.0052 0.0138 0.0251 0.1462 0.0328 01005 27 619 25 298 0.6365 0.0182 3e-04 1.3637 0.0352 0.0077 1e-04 1.6567 0.3283 0.0157 0.0002 1.0603 3 0.6272 0.0353 0.3375 0.0160 0.0062 0.0158 0.0255 0.1749 0.0469 01006 7 273 14 194 0.5801 0.0200 4e-04 0.7957 0.0264 0.0056 0e+00 0.5836 0.3935 0.0180 0.0003 0.6536 4 0.5786 0.0275 0.3939 0.0175 0.0057 0.0176 0.0302 0.2052 0.0446 01007 8 334 9 225 0.6132 0.0312 1e-03 2.3457 0.0155 0.0059 0e+00 1.3093 0.3714 0.0300 0.0009 2.2054 5 0.5974 0.0215 0.3812 0.0246 0.0067 0.0247 0.0412 0.3099 0.0648 01011 8 324 17 177 0.6359 0.0290 8e-04 1.9003 0.0298 0.0074 1e-04 0.9842 0.3343 0.0319 0.0010 2.3912 6 0.6151 0.0315 0.3533 0.0256 0.0087 0.0248 0.0416 0.2769 0.0703 02001 73 1359 55 979 0.5488 0.0190 4e-04 3.5128 0.0230 0.0050 0e+00 2.6845 0.4282 0.0181 0.0003 3.2195 7 0.5553 0.0225 0.4222 0.0178 0.0051 0.0177 0.0320 0.2284 0.0419 02002 141 2014 93 1644 0.5537 0.0113 1e-04 1.9590 0.0221 0.0039 0e+00 2.6821 0.4242 0.0110 0.0001 1.8694 8 0.5511 0.0234 0.4255 0.0112 0.0032 0.0112 0.0204 0.1388 0.0263 02003 16 519 19 379 0.5640 0.0293 9e-04 3.2058 0.0188 0.0072 1e-04 2.5963 0.4172 0.0361 0.0013 4.9275 9 0.5669 0.0228 0.4103 0.0261 0.0071 0.0261 0.0460 0.3097 0.0636 02004 197 2077 116 1531 0.5577 0.0101 1e-04 1.5357 0.0327 0.0041 0e+00 1.9601 0.4096 0.0098 0.0001 1.4950 10 0.5576 0.0331 0.4093 0.0099 0.0036 0.0098 0.0177 0.1074 0.0239 "],["metodología-de-benchmarking.html", "12.10 Metodología de Benchmarking", " 12.10 Metodología de Benchmarking Conteos de personas agregados por dam2, personas mayores de 15 años de edad. conteo_pp_dam &lt;- readRDS(&quot;Recursos/Día3/Sesion4/Data/censo_mrp_dam2.rds&quot;) %&gt;% filter(edad &gt; 1) %&gt;% group_by(dam , dam2) %&gt;% summarise(pp_dam2 = sum(n),.groups = &quot;drop&quot;) %&gt;% group_by(dam) %&gt;% mutate(pp_dam = sum(pp_dam2)) head(conteo_pp_dam) %&gt;% tba() dam dam2 pp_dam2 pp_dam 01 01001 701329 1031756 01 01002 35575 1031756 01 01003 41731 1031756 01 01004 11826 1031756 01 01005 91311 1031756 01 01006 33782 1031756 Estimación del parámetro theta al nivel que la encuesta sea representativa. indicador_agregado &lt;- diseno %&gt;% group_by_at(&quot;dam&quot;) %&gt;% filter(empleo %in% c(1:3)) %&gt;% summarise( Ocupado = survey_ratio(numerator = (empleo == 1), denominator = 1 ), Desocupado = survey_ratio(numerator =( empleo == 2),denominator = 1 ), Inactivo = survey_ratio(numerator = (empleo == 3), denominator = 1 ) ) %&gt;% select(dam,Ocupado,Desocupado, Inactivo) tba(indicador_agregado) dam Ocupado Desocupado Inactivo 01 0.5974 0.0336 0.3690 02 0.5552 0.0276 0.4172 03 0.5824 0.0297 0.3879 04 0.6429 0.0205 0.3365 05 0.5331 0.0352 0.4316 06 0.6115 0.0239 0.3645 07 0.6799 0.0158 0.3043 08 0.5465 0.0316 0.4219 09 0.5142 0.0601 0.4257 10 0.5668 0.0303 0.4030 11 0.5666 0.0334 0.4000 12 0.6310 0.0195 0.3495 13 0.6044 0.0216 0.3740 14 0.5697 0.0356 0.3946 15 0.5290 0.0428 0.4282 16 0.6337 0.0185 0.3478 17 0.5972 0.0285 0.3743 18 0.6331 0.0204 0.3466 19 0.5429 0.0366 0.4205 20 0.6686 0.0130 0.3184 21 0.6505 0.0261 0.3234 22 0.5746 0.0338 0.3916 23 0.5785 0.0433 0.3782 24 0.5796 0.0272 0.3932 25 0.5385 0.0310 0.4305 26 0.5615 0.0324 0.4061 27 0.5990 0.0301 0.3709 28 0.5416 0.0295 0.4289 29 0.5928 0.0263 0.3809 30 0.5971 0.0200 0.3829 31 0.6536 0.0193 0.3271 32 0.5875 0.0132 0.3993 Organizando la salida como un vector. temp &lt;- gather(indicador_agregado, key = &quot;agregado&quot;, value = &quot;estimacion&quot;, -dam) %&gt;% mutate(nombre = paste0(&quot;dam_&quot;, dam,&quot;_&quot;, agregado)) Razon_empleo &lt;- setNames(temp$estimacion, temp$nombre) Definir los pesos por dominios. names_cov &lt;- &quot;dam&quot; estimaciones_mod &lt;- estimaciones %&gt;% transmute( dam = substr(dam2,1,2), dam2,Ocupado_mod,Desocupado_mod,Inactivo_mod) %&gt;% inner_join(conteo_pp_dam ) %&gt;% mutate(wi = pp_dam2/pp_dam) Crear variables dummys estimaciones_mod %&lt;&gt;% fastDummies::dummy_cols(select_columns = names_cov, remove_selected_columns = FALSE) Xdummy &lt;- estimaciones_mod %&gt;% select(matches(&quot;dam_&quot;)) %&gt;% mutate_at(vars(matches(&quot;_\\\\d&quot;)) , list(Ocupado = function(x) x*estimaciones_mod$Ocupado_mod, Desocupado = function(x) x*estimaciones_mod$Desocupado_mod, Inactivo = function(x) x*estimaciones_mod$Inactivo_mod)) %&gt;% select((matches(&quot;Ocupado|Desocupado|Inactivo&quot;))) # head(Xdummy) %&gt;% tba() Algunas validaciones realizadas colnames(Xdummy) == names(Razon_empleo) data.frame(Modelo = colSums(Xdummy*estimaciones_mod$wi), Estimacion_encuesta = Razon_empleo) Calcular el ponderador para cada nivel de la variable. Ocupado library(sampling) names_ocupado &lt;- grep(pattern = &quot;_O&quot;, x = colnames(Xdummy),value = TRUE) gk_ocupado &lt;- calib(Xs = Xdummy[,names_ocupado], d = estimaciones_mod$wi, total = Razon_empleo[names_ocupado], method=&quot;logit&quot;,max_iter = 5000,) checkcalibration(Xs = Xdummy[,names_ocupado], d =estimaciones_mod$wi, total = Razon_empleo[names_ocupado], g = gk_ocupado) Desocupado names_descupados &lt;- grep(pattern = &quot;_D&quot;, x = colnames(Xdummy),value = TRUE) gk_desocupado &lt;- calib(Xs = Xdummy[,names_descupados], d = estimaciones_mod$wi, total = Razon_empleo[names_descupados], method=&quot;logit&quot;,max_iter = 5000,) checkcalibration(Xs = Xdummy[,names_descupados], d =estimaciones_mod$wi, total = Razon_empleo[names_descupados], g = gk_desocupado) Inactivo names_inactivo &lt;- grep(pattern = &quot;_I&quot;, x = colnames(Xdummy),value = TRUE) gk_Inactivo &lt;- calib(Xs = Xdummy[,names_inactivo], d = estimaciones_mod$wi, total = Razon_empleo[names_inactivo], method=&quot;logit&quot;,max_iter = 5000,) checkcalibration(Xs = Xdummy[,names_inactivo], d =estimaciones_mod$wi, total = Razon_empleo[names_inactivo], g = gk_Inactivo) Validar los resultados obtenidos. par(mfrow = c(1,3)) hist(gk_ocupado) hist(gk_desocupado) hist(gk_Inactivo) Estimaciones ajustadas por el ponderador estimacionesBench &lt;- estimaciones_mod %&gt;% mutate(gk_ocupado, gk_desocupado, gk_Inactivo) %&gt;% transmute( dam, dam2, wi,gk_ocupado, gk_desocupado, gk_Inactivo, Ocupado_Bench = Ocupado_mod*gk_ocupado, Desocupado_Bench = Desocupado_mod*gk_desocupado, Inactivo_Bench = Inactivo_mod*gk_Inactivo ) Validación de resultados. tabla_validar &lt;- estimacionesBench %&gt;% group_by(dam) %&gt;% summarise(Ocupado_Bench = sum(wi*Ocupado_Bench), Desocupado_Bench = sum(wi*Desocupado_Bench), Inactivo_Bench = sum(wi*Inactivo_Bench)) %&gt;% inner_join(indicador_agregado) tabla_validar %&gt;% tba() dam Ocupado_Bench Desocupado_Bench Inactivo_Bench Ocupado Desocupado Inactivo 01 0.5974 0.0336 0.3690 0.5974 0.0336 0.3690 02 0.5552 0.0276 0.4172 0.5552 0.0276 0.4172 03 0.5824 0.0297 0.3879 0.5824 0.0297 0.3879 04 0.6429 0.0205 0.3365 0.6429 0.0205 0.3365 05 0.5331 0.0352 0.4316 0.5331 0.0352 0.4316 06 0.6115 0.0239 0.3645 0.6115 0.0239 0.3645 07 0.6799 0.0158 0.3043 0.6799 0.0158 0.3043 08 0.5465 0.0316 0.4219 0.5465 0.0316 0.4219 09 0.5142 0.0601 0.4257 0.5142 0.0601 0.4257 10 0.5668 0.0303 0.4030 0.5668 0.0303 0.4030 11 0.5666 0.0334 0.4000 0.5666 0.0334 0.4000 12 0.6310 0.0195 0.3495 0.6310 0.0195 0.3495 13 0.6044 0.0216 0.3740 0.6044 0.0216 0.3740 14 0.5697 0.0356 0.3946 0.5697 0.0356 0.3946 15 0.5290 0.0428 0.4282 0.5290 0.0428 0.4282 16 0.6337 0.0185 0.3478 0.6337 0.0185 0.3478 17 0.5972 0.0285 0.3743 0.5972 0.0285 0.3743 18 0.6331 0.0204 0.3466 0.6331 0.0204 0.3466 19 0.5429 0.0366 0.4205 0.5429 0.0366 0.4205 20 0.6686 0.0130 0.3184 0.6686 0.0130 0.3184 21 0.6505 0.0261 0.3234 0.6505 0.0261 0.3234 22 0.5746 0.0338 0.3916 0.5746 0.0338 0.3916 23 0.5785 0.0433 0.3782 0.5785 0.0433 0.3782 24 0.5796 0.0272 0.3932 0.5796 0.0272 0.3932 25 0.5385 0.0310 0.4305 0.5385 0.0310 0.4305 26 0.5615 0.0324 0.4061 0.5615 0.0324 0.4061 27 0.5990 0.0301 0.3709 0.5990 0.0301 0.3709 28 0.5416 0.0295 0.4289 0.5416 0.0295 0.4289 29 0.5928 0.0263 0.3809 0.5928 0.0263 0.3809 30 0.5971 0.0200 0.3829 0.5971 0.0200 0.3829 31 0.6536 0.0193 0.3271 0.6536 0.0193 0.3271 32 0.5875 0.0132 0.3993 0.5875 0.0132 0.3993 Guardar resultados estimaciones &lt;- inner_join(estimaciones,estimacionesBench) saveRDS(object = estimaciones, file = &quot;Recursos/Día3/Sesion4/Data/estimaciones_Bench.rds&quot;) "],["mapas-del-mercado-de-trabajo..html", "12.11 Mapas del mercado de trabajo.", " 12.11 Mapas del mercado de trabajo. El código carga las librerías sf y tmap. Luego, se lee un archivo shapefile con información geográfica y se utiliza la función ‘inner_join’ para unirlo con las estimaciones de la encuesta previamente calculadas. Posteriormente, se definen los puntos de corte para la generación de los intervalos de clase en los mapas de cada variable de interés (ocupados, desocupados e inactivos) y se asignan a las variables ‘brks_ocupado’, ‘brks_desocupado’ y ‘brks_inactivo’, respectivamente. library(sf) library(tmap) ShapeSAE &lt;- read_sf(&quot;Shape/MEX_dam2.shp&quot;) P1_empleo &lt;- tm_shape(ShapeSAE %&gt;% inner_join(estimaciones)) brks_ocupado &lt;- seq(0.2,0.8,0.1) brks_desocupado &lt;- seq(0,0.2,0.05) brks_inactivo &lt;- seq(0.17,0.62, 0.09) Ocupado Este código está creando un mapa de la variable “Ocupado” utilizando la función tm_fill() de la librería tmap. Los valores de la variable se clasifican en diferentes categorías utilizando la función breaks, y se establece un título para la leyenda del mapa con el argumento title. Se utiliza una paleta de colores llamada “-Blues” para representar las diferentes categorías de la variable en el mapa. La función tm_layout se utiliza para establecer algunas características del diseño del mapa, como el tamaño de la leyenda, el tamaño de la fuente, y la relación de aspecto del mapa. Finalmente, el mapa se guarda en la variable Mapa_ocupado. Mapa_ocupado &lt;- P1_empleo + tm_fill(&quot;Ocupado_Bench&quot;, breaks = brks_ocupado, title = &quot;Ocupado&quot;, palette = &quot;-Blues&quot;) + tm_layout( legend.only = FALSE, legend.height = -0.3, legend.width = -0.2, asp = 1.5, legend.text.size = 3, legend.title.size = 3 ) tmap_save( Mapa_ocupado, filename = &quot;Recursos/Día3/Sesion4/0Recursos/Ocupados.png&quot;, width = 3500, height = 2500, asp = 0 ) Mapa_ocupado Desocupado Este código utiliza la función tm_fill() de la librería tmap para crear un mapa temático del indicador de “desocupado” a nivel de las áreas geográficas definidas en el archivo de polígonos ShapeSAE. La paleta de colores utilizada para representar los valores del indicador es la “YlOrRd”. Se especifican los mismos parámetros de tm_layout() que en el mapa anterior para definir el diseño general del mapa. Mapa_desocupado &lt;- P1_empleo + tm_fill( &quot;Desocupado_Bench&quot;, breaks = brks_desocupado, title = &quot;Desocupado&quot;, palette = &quot;YlOrRd&quot; ) + tm_layout( legend.only = FALSE, legend.height = -0.3, legend.width = -0.2, asp = 1.5, legend.text.size = 3, legend.title.size = 3 ) tmap_save( Mapa_desocupado, filename = &quot;Recursos/Día3/Sesion4/0Recursos/Desocupados.png&quot;, width = 3500, height = 2500, asp = 0 ) Mapa_desocupado Inactivo Este código genera un mapa temático de la variable “Inactivo” utilizando la librería tmap. Primero se carga el archivo de shapefile y se hace una unión con las estimaciones previamente calculadas. Luego se utiliza la función tm_fill() para especificar que se desea utilizar el valor de la variable “Inactivo” para el relleno del mapa. Se definen los intervalos de la paleta de colores con la variable “brks_inactivo” y se especifica el título del mapa con la opción “title”. Finalmente, se configura el diseño del mapa con la función tm_layout(), donde se ajustan parámetros como el tamaño del texto y de la leyenda, y se establece el aspecto del mapa en 1.5 para una mejor visualización. Mapa_Inactivo &lt;- P1_empleo + tm_fill( &quot;Inactivo_Bench&quot;, title = &quot;Inactivo&quot;, breaks = brks_inactivo, palette = &quot;YlGn&quot; ) + tm_layout( legend.only = FALSE, legend.height = -0.3, legend.width = -0.2, asp = 1.5, legend.text.size = 3, legend.title.size = 3 ) tmap_save( Mapa_Inactivo, filename = &quot;Recursos/Día3/Sesion4/0Recursos/Inactivo.png&quot;, width = 3500, height = 2500, asp = 0 ) Mapa_Inactivo "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
